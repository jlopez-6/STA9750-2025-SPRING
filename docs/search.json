[
  {
    "objectID": "week2/houston.html#history-of-house-sales",
    "href": "week2/houston.html#history-of-house-sales",
    "title": "Houston’s Housing Market",
    "section": "History of House Sales",
    "text": "History of House Sales\n\nIn the last 15 years, home sales peaked in 2006 and 2014.\nThey are currently in a steady decline."
  },
  {
    "objectID": "week2/houston.html#average-sale-price",
    "href": "week2/houston.html#average-sale-price",
    "title": "Houston’s Housing Market",
    "section": "Average Sale Price",
    "text": "Average Sale Price"
  },
  {
    "objectID": "week2/houston.html#how-does-houston-rank",
    "href": "week2/houston.html#how-does-houston-rank",
    "title": "Houston’s Housing Market",
    "section": "How does Houston rank?",
    "text": "How does Houston rank?\n\nNot great"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to STA9750 - Spring 2025",
    "section": "",
    "text": "Welcome to the course submissions page for the 2025 Spring Semester of STA9750!\n\n\n\nMini Project 01\n\n\nWelcome to the Commission to Analyze Taxpayer Spending (CATS)\n\n\n\n\n Mini Project 02\n\n\nIdentifying Environmentally Responsible US Public Transit Systems\n\n\n\n\nMini Project 03\n\n\nCreating the Ultimate Playlist\n\n\n\n\nMini Project 04\n\n\nExploring Recent US Political Shifts"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "",
    "text": "CATS logo1."
  },
  {
    "objectID": "mp01.html#executive-summary",
    "href": "mp01.html#executive-summary",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "1.1 Executive Summary",
    "text": "1.1 Executive Summary\nThis report analyzes New York City personnel expenditure data collected between 2014 and 2024, with the goal of analyzing policies that effectively reduce personnel spending. The Commission proposes 3 policy options:\n\nPolicy I: Capping Salaries at Mayoral Level\nPolicy II: Increasing Staffing to Reduce Overtime Expenses\nPolicy III: Reduce Salary Increases for Top Earners\n\nThe data suggest that Policy II offers the largest amount of savings, although its implementation will be significantly difficult. Policy I would result in the least savings, but is the easiest to implement. Policy III strikes a balance between the two, offering moderate savings and ease of implementation.\nThe Commission most strongly recommends that the City implement Policy III, where raises for high earners are tied to demonstrated productivity improvements, ensuring sustainable wage growth.\n\n1.1.1 Quick Facts\n\nThe job title with the highest base rate of pay is Chairman with a rate of $567 per hour.\n\n\n\nCode\nlibrary(readr)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(DT)\nlibrary(ggplot2)\n\nnyc_payroll &lt;- read_csv(\"data/mp01/nyc_payroll_export.csv\") |&gt; \n  mutate(agency_name = str_to_title(agency_name),\n         last_name = str_to_title(last_name),\n         first_name = str_to_title(first_name),\n         work_location_borough = str_to_title(work_location_borough),\n         title_description = str_to_title(title_description),\n         leave_status_as_of_june_30 = str_to_title(leave_status_as_of_june_30))\n\n# Constants\nWORK_HOURS_IN_DAY = 7.5\nFT_ANNUAL_HOURS = 2000\nN_ITEMS = 20\nPAY_RAISE_CAP = 0.005\n\nHOURLY_RATE_MAX = nyc_payroll |&gt; \n  filter(pay_basis == \"per Hour\") |&gt;\n  arrange(base_salary) |&gt;\n  mutate(jump = base_salary - lag(base_salary)) |&gt;\n  select(jump, base_salary, agency_name) |&gt;\n  arrange(jump) |&gt;\n  slice_max(jump,n=1) |&gt;\n  pull(base_salary)\n\n# Adding total_pay and base_rt_hr columns.\nnyc_payroll &lt;-\n  nyc_payroll |&gt;\n  mutate(total_pay =\n           case_when(\n             (pay_basis == \"per Day\" & regular_hours &gt;= 0 & ot_hours &lt; 0) ~ base_salary * floor(regular_hours / WORK_HOURS_IN_DAY),\n             (pay_basis == \"per Day\" & regular_hours &gt;= 0 & ot_hours &gt;= 0) ~ base_salary * floor(regular_hours / WORK_HOURS_IN_DAY) + ((base_salary/WORK_HOURS_IN_DAY) * ot_hours * 1.5),\n             (pay_basis == \"per Hour\" & regular_hours &gt;= 0 & ot_hours &gt;= 0) ~ base_salary * (regular_hours + ot_hours * 1.5),\n             (pay_basis == \"per Hour\" & regular_hours &gt;= 0 & ot_hours &lt; 0) ~ base_salary * regular_hours,\n             (pay_basis == \"per Hour\" & regular_hours &lt; 0) ~ NA,\n             .default = base_salary + (base_salary/FT_ANNUAL_HOURS * 1.5 * ot_hours),\n             ),\n         base_rate_hr = case_when(\n           pay_basis == \"per Day\" & (base_salary/WORK_HOURS_IN_DAY) &lt; HOURLY_RATE_MAX ~ base_salary / WORK_HOURS_IN_DAY,\n           pay_basis == \"per Day\" & (base_salary/WORK_HOURS_IN_DAY) &gt;= HOURLY_RATE_MAX ~ base_salary / FT_ANNUAL_HOURS, # I assume they meant pay basis is per Annum\n           pay_basis == \"per Hour\" & base_salary &lt; HOURLY_RATE_MAX ~ base_salary,\n           pay_basis == \"per Hour\" & base_salary &gt;= HOURLY_RATE_MAX ~ base_salary / FT_ANNUAL_HOURS, # I assume they meant per Annum\n           TRUE ~ base_salary / FT_ANNUAL_HOURS\n      )\n    )\n\nhighest_base_rate &lt;-\n  nyc_payroll |&gt;\n  select(fiscal_year,\n         last_name,mid_init,first_name,\n         title_description,\n         agency_name,\n         base_rate_hr) |&gt;\n  slice_max(base_rate_hr, n=1)\n\n\n\nThe individual with the single highest city total payroll was Chief Fire Marshal at the Fire Department of New York (FDNY), Daniel E. Flynn. He earned $511,821 in 2021.\n\n\n\nCode\n# Which individual & in what year had the single highest city total payroll (regular and overtime combined)?\nmost_paid &lt;-\n  nyc_payroll |&gt;\n  filter(total_pay == max(total_pay,na.rm = TRUE)) |&gt;\n  select(fiscal_year,last_name,first_name,mid_init, agency_name,title_description, total_pay)\n\n\n\nThe individual who worked the most overtime hours in a single year was James Internicola, Correction Officer at the Department Of Correction. He logged 3,693 hours of overtime in 2022.\n\n\n\nCode\n# Which individual worked the most overtime hours in this data set?\nmost_OT &lt;-\n  nyc_payroll |&gt;\n  filter(ot_hours == max(ot_hours)) |&gt;\n  select(fiscal_year, last_name, first_name, mid_init,agency_name,title_description,ot_hours)\n\n\n\nThe Department of Education has the highest average total annual payroll at $10,083,191,004 a year.\n\n\n\nCode\nhigh_total_annual_payroll &lt;-\n  nyc_payroll |&gt;\n  group_by(agency_name, fiscal_year) |&gt;\n  summarize(\n    sum_total_payroll = sum(total_pay,na.rm=TRUE),\n    n_emp = n()\n  ) |&gt;\n  group_by(agency_name) |&gt;\n  summarize(mean_total_annual_payroll = mean(sum_total_payroll, na.rm = TRUE)) |&gt;\n  slice_max(mean_total_annual_payroll, n=1)\n\n\n\nThe Department of Education has the most employees on payroll in each year.\n\n\n\nCode\nmost_emps &lt;-\n  nyc_payroll |&gt;\n  group_by(fiscal_year, agency_name) |&gt;\n  summarize(\n    num_employees = n(), .groups = \"drop\"\n  ) |&gt;\n  slice_max(num_employees, n = 1, by = fiscal_year)\n\n\n\nThe Board of Elections has the highest overtime (OT) usage, where employees log approximately 1 hour of OT for every 5 regular hours.\n\n\n\nCode\nhighest_OT_usage &lt;-\n  nyc_payroll |&gt;\n  group_by(agency_name) |&gt;\n  summarize(\n    sum_reg = sum(regular_hours,na.rm=TRUE),\n    sum_ot = sum(ot_hours,na.rm=TRUE),\n    ot_ratio = sum_ot/sum_reg\n  ) |&gt;\n  slice_max(ot_ratio, n = 1)\n\n\n\nThe average salary of employees who work outside of the five boroughs is $97,125.\n\n\n\nCode\nsalary_out_nyc &lt;-\n  nyc_payroll |&gt;\n  filter(\n    !is.na(work_location_borough),\n    !work_location_borough %in% c(\"Bronx\",\"Brooklyn\",\"Queens\",\"Manhattan\",\"Staten Island\")\n    ) |&gt;\n  summarize(average = mean(total_pay,na.rm=TRUE))\n\n\n\nFrom 2014 to 2024, the city’s aggregate payroll grew 45%, from $22,452,904,026 to $32,635,103,990.\n\n\n\nCode\nactual_payroll &lt;-\n  nyc_payroll |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(\n    aggregate_pay = sum(total_pay,na.rm=TRUE)\n    ) |&gt;\n  mutate(percent_change = ((aggregate_pay - lag(aggregate_pay)) / lag(aggregate_pay)) * 100)"
  },
  {
    "objectID": "mp01.html#background",
    "href": "mp01.html#background",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "1.2 Background",
    "text": "1.2 Background\nNew York City’s population of 8.26 million requires a sizable host of people to maintain a livable city for all (York, 2024). In 2024 alone, the City employed 562,898 people to put our fires out, teach our children, and clean up our streets, among other things (City, 2024). It follows then, that a large portion of the City’s budget is dedicated to personnel costs. While the efforts of these individuals is highly appreciated, we must also strive for optimization in personnel costs to lower the cost of living so that New Yorkers can thrive in our city."
  },
  {
    "objectID": "mp01.html#problem-description",
    "href": "mp01.html#problem-description",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "1.3 Problem Description",
    "text": "1.3 Problem Description\nIn fiscal year 2024, ~31% ($32,998,000) of the City’s $107,115,000 million budget was set aside to pay for the salaries and wages of government employees. However, in 2025, the city government estimates that it will see an 3.8% increase in personnel spending. Likewise, the City predicts that 2026 will see a 2.6% rise in salaries, even though total expenditures will be down 1.1% that year. Indeed, rising personnel costs is and will continue to be a pervasive problem for New York City, with the Comptroller estimating a 3.4% annual growth in payroll expenditure for 2026 and beyond (Figure 1) (Comptroller, 2024).\n\nCode\nlast_yr = tail(actual_payroll,n=1) |&gt; pull(fiscal_year)\nlast_ap = tail(actual_payroll,n=1) |&gt; pull(aggregate_pay)\n\npredicted_payroll &lt;- tibble(\n  fiscal_year = c(last_yr,2025, 2026, 2027, 2028),  \n  aggregate_pay = c(last_ap,last_ap*1.038, last_ap*1.038*1.026, last_ap*1.038*1.026*1.038, last_ap*1.038*1.026*(1.038^2)) \n)\n\ncombined_data &lt;- bind_rows(\n  predicted_payroll |&gt; mutate(type = \"Predicted\"),\n  actual_payroll |&gt; mutate(type = \"Actual\")\n)|&gt;\n  ggplot(aes(x = fiscal_year, y = aggregate_pay, color = type, linetype = type)) +\n  geom_line(linewidth = 1.2) +  \n  geom_point(size = 3) + \n  scale_color_manual(values = c(\"Actual\" = \"#78c2ad\", \"Predicted\" = \"gray80\")) +  \n  labs(\n    title = \"NYC Payroll Expenditure\",\n    x = \"Fiscal Year\",\n    y = \"Aggregate Payroll ($)\",\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 1: Payroll Expenditure per Year\n\n\nFigure 1\n\n\n\nTo address this problem, Mayor Eric L. Adams has tasked the Commission to Analyze Taxpayer Spending (CATS) with investigating payroll expenditures, formulating policies aimed at reducing the City’s payroll costs, and offering data based recommendations. This report presents our findings."
  },
  {
    "objectID": "mp01.html#data-acquisition-and-preparation",
    "href": "mp01.html#data-acquisition-and-preparation",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "1.4 Data Acquisition and Preparation",
    "text": "1.4 Data Acquisition and Preparation\nThe Commission acquired Citywide Payroll Data from NYC Open Data, wherein New York City agencies publish data that they collect and use. The data set contains various points of information on every city employee as inputted into the Personnel Management System by user Agencies. As an example of the datapoints in the set, table 1 presents an employee salary table for Mayor Eric L. Adams from 2014 to 2024.\n\nTable 1: Salary Table for Mayor Eric L. Adams\n\n\nCode\neric_adams_payroll &lt;- \n  nyc_payroll |&gt;\n  filter(\n    first_name == \"Eric\",\n    mid_init == \"L\",\n    last_name == \"Adams\"\n    ) |&gt;\n  group_by(fiscal_year ) |&gt;\n  summarize(\n    title_description = str_c(unique(title_description), collapse = \", \"),\n    agency_name = str_c(unique(agency_name), collapse = \", \"),\n    total_salary = sum(base_salary, na.rm = TRUE),\n    .groups = \"drop\"\n    )|&gt;\n  rename(\"Fiscal Year\" = fiscal_year,\n         \"Total Salary\" = total_salary,\n         \"Agency Name\" = agency_name,\n         \"Position\" = title_description\n  )|&gt;\n  datatable(options = list(pageLength = 10), \n            rownames = FALSE\n  )\n\n\n\nThe analysis was performed using R and several R packages, including (Team, 2024), (Wickham & Hester, 2024), (Wickham, 2016), and (Wickham et al., 2024). The Commission notes that superficial changes were made to several text columns in the dataframe to prepare them for presentation. Furthermore, the Commission creates two new columns:\n\ntotal_pay\nbase_rate_hr\n\nThe total_pay column contains the total gross pay of each employee based on their base pay, their pay basis (per Hour, per Day, or assumed annual salary), and the number of hours–both regular and OT–reported. Several cases in the data set contained negative counts for hours. The Commision addressed this by setting total_pay to NA for these cases. The base_rate_hr column represents an employee’s estimated hourly wage, calculated based on their pay basis, while ensuring reasonable hourly rate assumptions of less than $67,440 per hour. This assumption was developed to address pay basis and base salary mismatch errors."
  },
  {
    "objectID": "mp01.html#policy-analysis",
    "href": "mp01.html#policy-analysis",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "1.5 Policy Analysis",
    "text": "1.5 Policy Analysis\nThe Commission proposes the following policies to address mounting payroll expenditures.\n\n1.5.1 Policy I: Capping Salaries at Mayoral Level\nOne approach to reducing personnel expenditures is to cap the salaries of subordinate employees, so that the mayor of the City earns the highest salary. Table 2 presents the salaries of New York City’s Mayor from 2014 to 2024.\n\nTable 2: Salaries of New York City’s Mayor (2014-2024)\n\n\nCode\nmayor_pay &lt;- nyc_payroll |&gt;\n  filter(title_description == \"Mayor\" &\n           leave_status_as_of_june_30 == \"Active\") |&gt;\n  select(fiscal_year, mayoral_cap = total_pay)\n\nkable(mayor_pay, col.names = c(\"Fiscal Year\", \"Mayoral Cap ($)\"))\n\n\n\n\n\nFiscal Year\nMayoral Cap ($)\n\n\n\n\n2024\n258750\n\n\n2023\n258750\n\n\n2022\n258750\n\n\n2021\n258750\n\n\n2020\n258750\n\n\n2019\n258750\n\n\n2018\n258750\n\n\n2017\n225000\n\n\n2016\n225000\n\n\n2015\n225000\n\n\n2014\n225000\n\n\n\n\n\n\nIn total, 1,566 job titles earned more money than the mayor did during the the same fiscal year. A distribution of the agencies with employees exceeding the mayoral salary is shown in figure 2. The vast majority of people who earned more money than the mayor work for the Department of Corrections. This is followed by the FDNY and the NYPD. These agencies would see the largest savings if personnel salaries are capped at the mayoral level.\n\nCode\nexceeding_agencies &lt;- nyc_payroll |&gt; \n  left_join(mayor_pay, join_by(fiscal_year == fiscal_year)) |&gt;\n  filter(total_pay &gt; mayoral_cap) |&gt;\n  select(fiscal_year, last_name, mid_init, first_name, agency_name, title_description, total_pay) |&gt;\n  group_by(agency_name) |&gt;\n  summarize(n = n())\n\nremaining_agency_count &lt;- max(nrow(exceeding_agencies) - N_ITEMS, 0)\n\nremaining_sum &lt;- exceeding_agencies |&gt; \n  slice_min(n, n = remaining_agency_count) |&gt; \n  summarize(sum_n = sum(n)) |&gt; \n  pull(sum_n)\n\nremaining_agency_count\n\n\nCode\nremaining_sum\n\n\nCode\nggplot(exceeding_agencies |&gt; slice_max(n, n=N_ITEMS), aes(x = reorder(agency_name, n), y = n)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +\n  labs(\n    title = paste(\"Top\",N_ITEMS,\"Agencies Exceeding Mayoral Cap\"),\n    x = \"Agency\",\n    y = \"Number of Entries Exceeding Cap \\n(FY2014-FY2024)\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 2: Number of Employees in Agencies Exceeding Mayoral Cap\n\n\n[1] 47\n\n\n[1] 119\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\nThe Commission identifies agency title groupings that have significant numbers of payroll entries over the mayoral cap. These are presented in table 3. The top two rows suggest that Captains at the Department of Corrections and the Fire Department would be most affected by this policy.\n\nTable 3: Agency-Title Pairs Exceeding Mayoral Cap\n\n\nCode\nagency_titles_over_mayoral_cap &lt;-\n  nyc_payroll |&gt; \n  left_join(mayor_pay, join_by(fiscal_year == fiscal_year)) |&gt;\n  mutate(is_over_cap = total_pay &gt; mayoral_cap) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(n = n(), n_over = sum(is_over_cap, na.rm = TRUE), freq = mean(is_over_cap, na.rm = TRUE)) \n\nagency_titles_over_mayoral_cap |&gt;\n  filter(n_over &gt; 0) |&gt;\n  select(agency_name,title_description,n_over) |&gt;\n  arrange(desc(n_over)) |&gt;\n  rename(\"Agency Name\" = agency_name,\n         \"Title Description\" = title_description,\n         \"Entries Over Mayoral Cap\" = n_over) |&gt;\n  datatable()\n\n\n\n\n\n\n\nThe total savings per fiscal year that could have resulted from enactment of the proposed policy are presented in table 4. The Commission notes that, had the proposed policy been enacted in 2014, New York City would have seen total savings of $39,074,243.\n\nTable 4: Savings per Annum from Policy I\n\n\nCode\n# Determine total savings if these employees’ compensation were capped at the mayor’s salary.\nsavings_per_year &lt;-\n  nyc_payroll |&gt; \n  left_join(mayor_pay, join_by(fiscal_year == fiscal_year)) |&gt;\n  filter(total_pay &gt; mayoral_cap) |&gt;\n  mutate(savings = total_pay - mayoral_cap) |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(total_savings = round(sum(savings,na.rm=TRUE),0), n_over = n())\n\ntotal_savings &lt;- savings_per_year |&gt; summarize(sum(total_savings, na.rm=TRUE))\n\ndatatable(savings_per_year |&gt;\n            rename(\"Fiscal Year\" = fiscal_year,\n                   \"# of Entries Exceeding Mayoral Cap\" = n_over,\n                   \"Savings ($)\" = total_savings))\n\n\n\n\n\n\n\nThe Commission does not recommend a blanket capping subordinate salaries at the mayoral level. A majority of employees earning more than the mayor hold positions that involve significant risks. Given the nature of these high-risk jobs, the Commission believes that these employees should be fairly compensated for the additional responsibilities and dangers they face in their roles. However, the Commission is not against capping salaries on a title-by-title basis.\n\n\n1.5.2 Policy II: Increasing Staffing to Reduce Overtime Expenses\nA second approach to curtailing personnel expenditure is increasing staffing to reduce OT expenses. For this analysis, the Commission assumes that a full-time position is 2000 hours a year. Figure 3 presents OT hours converted to the number of new full-time employees required to fulfill the same amount of work hours. The total number of new employees needed to address the current OT load is 200,412. This represents the workforce that would need to be hired if OT hours were eliminated through additional staffing. The Commission notes that this number is approximately 50% of the last fiscal year’s workforce 1.\n\nCode\nnyc_payroll |&gt;\n  filter(ot_hours &gt; 0) |&gt;\n  group_by(agency_name, title_description, fiscal_year) |&gt;\n  summarize(total_ot = sum(ot_hours), .groups = \"drop\") |&gt;\n  mutate(n_fte_yearly = total_ot / 2000) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(total_n_fte = sum(n_fte_yearly, na.rm=TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(total_n_fte)) |&gt;\n  slice_max(total_n_fte, n = N_ITEMS) |&gt;  # Show top 20 only\n  ggplot(aes(x = reorder(paste(agency_name, title_description, sep=\" | \"), total_n_fte), y = total_n_fte)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    title = \"OT Hours Converted to FTE\",\n    x = \"Agency | Job Title\",\n    y = \"Number of Full-Time Equivalent Positions\",\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 3: Full Time Personnel Equivalent, by Agency and Title \n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\nThe total potential savings from this proposed policy amount to $15,025,441,508 (Table 5). The commission notes that the potential savings from this policy are almost 400x that of Policy I.\nEnacting this policy entails recruiting:\n\nover 30,000 police officers,\nover 20,000 firefighters,\nand over 15,000 corrections officers.\n\nThese positions are difficult to fill, as they are high-risk in nature and require specialized training. Therefore, the Commission recommends a more feasible, scaled-down version of this policy. Under this modified approach, the City would subsidize the training costs for these critical positions, and the recruitment rate would be regularly assessed to ensure that sufficient progress is being made in filling these roles.\nTable 5 presents total savings by agency.\n\nTable 5: Total Savings by Agency\n\n\nCode\nnyc_payroll |&gt;\n  filter(ot_hours &gt; 0) |&gt;\n  mutate(savings = base_rate_hr * ot_hours) |&gt;\n  group_by(agency_name) |&gt;\n  summarize(total_savings = round(sum(savings, na.rm=TRUE),0)) |&gt;\n  arrange(desc(total_savings)) |&gt;\n  rename(\"Agency Name\" = agency_name,\n         \"Total Savings ($)\" = total_savings) |&gt;\n  datatable()\n\n\n\n\n\n\n\n1.5.3 Policy III: Reduce Salary Increases for Top Earners\nThe third policy the Commission proposes to reduce personnel spending is the reduction of salary increases for the top 10% of earners in their respective agencies to 1.05% of their base hourly rate. The Commission found that on average, high earners get a raise every 1.85 years, and presents the distribution of the frequency of raises per year in figure @ref(fig:pay_raise_freq).\n\nCode\nhigh_earners &lt;- \n  nyc_payroll |&gt;\n  group_by(agency_name) |&gt;\n  mutate(high_earner_threshold = quantile(total_pay, 0.9, na.rm = TRUE)) |&gt;  \n  ungroup() |&gt;  \n  filter(!is.na(total_pay),\n         !is.na(first_name),\n         !is.na(last_name),\n         !is.na(base_rate_hr),\n         !is.na(fiscal_year),\n         total_pay &gt; high_earner_threshold)\n\n# Calculate the number of raises per year for each employee\nhigh_earners_with_raises &lt;- \n  high_earners |&gt;\n  group_by(agency_name, title_description, first_name, mid_init, last_name) |&gt;\n  arrange(fiscal_year) |&gt;\n  mutate(\n    base_rate_change = case_when( is.na(lag(base_rate_hr)) | is.na(lag(fiscal_year)) ~ 0,\n                                  TRUE ~ (base_rate_hr - lag(base_rate_hr))/(fiscal_year - lag(fiscal_year))),\n    years_worked = max(fiscal_year,na.rm=TRUE) - min(fiscal_year,na.rm=TRUE),  # Calculate years worked\n    n_raises = sum(base_rate_change &gt; 0, na.rm = TRUE),\n    raises_per_year = n_raises / years_worked,\n    avg_rate_increase_per_person = mean(base_rate_change, na.rm=TRUE),\n    capped_raise = PAY_RAISE_CAP * lag(base_rate_hr, order_by = fiscal_year),  \n    excess_raise = pmax(base_rate_change - capped_raise, 0)\n  ) |&gt;\n  filter(!is.na(n_raises),years_worked&gt;0) |&gt;\n  ungroup()\n\n# Plot histogram of raises per year\nggplot(high_earners_with_raises |&gt; filter(raises_per_year &lt;= 1), aes(x = raises_per_year)) +\n  geom_histogram(binwidth = 0.05, fill = \"#69b3a2\", color = \"white\") +\n  labs(\n    title = \"Number of Pay Raises Per Year Among High Earners\",\n    x = \"Raises per Year\",\n    y = \"Number of Employees\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 4: Distribution of Pay Raise Frequency\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\nWe determine the agencies and titles with the highest average number of raises per year, and present the findings in table 6. 523 high-salary jobs have an average pay raise rate of one a year.\n\nTable 6: Distribution of Pay Raise Frequency\n\n\nCode\navg_pay_raise_rate_data &lt;-\n  high_earners_with_raises |&gt;\n  filter(raises_per_year &gt; 0 , raises_per_year &lt;= 1 ) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(avg_pay_raise_rate = mean(raises_per_year, na.rm=TRUE),\n            med_pay_raise_rate = median(raises_per_year, na.rm=TRUE),\n            n_raises_per_agency = sum(n_raises,na.rm=TRUE),\n            .groups=\"drop\")\n\navg_pay_raise_rate_data |&gt; \n  filter(avg_pay_raise_rate == 1) |&gt; \n  select(agency_name,title_description) |&gt;\n  rename(\"Agency Name\" = agency_name,\n         \"Title Description\" = title_description) |&gt;\n  datatable()\n\n\n\n\n\n\n\nWe find the average amount that the hourly base rate increases per agency. The highest average increase is $7.70 at Brooklyn Community Board 12. The Commission also notes the appearance of 11 community boards within the top 20 agencies (Figure 5).\n\nCode\navg_rate_increase_per_agency_data &lt;-\n  high_earners_with_raises |&gt; \n  group_by(agency_name) |&gt;\n  filter(avg_rate_increase_per_person != Inf, avg_rate_increase_per_person != -Inf ) |&gt;\n  summarize(avg_rate_increase_per_agency = mean(avg_rate_increase_per_person, na.rm=TRUE), .groups = \"drop\")\n\navg_rate_increase_per_agency_data |&gt;\n  slice_max(avg_rate_increase_per_agency, n = N_ITEMS) |&gt;  \n  ggplot(aes(x = reorder(agency_name, avg_rate_increase_per_agency), y = avg_rate_increase_per_agency)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    title = paste(\"Top\",N_ITEMS,\"Agencies by Mean Pay Raise Amount\"),\n    x = \"Agency\",\n    y = \"Mean Pay Raise Amount ($/hr)\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 5: Average Increase in Hourly Rate per Agency\n\n\n\n\n\nAverage pay raise amount per agency\n\n\n\n\n\n\nFigure 5\n\n\n\nCombining these two metrics, we get an estimate of the amount spent per agency on high-earner raises over the last 10 fiscal years. The Department of Education far outranks all other agencies with an expenditure of almost $500 million (Figure 6). They are followed by the Police Department at just over $200 million. Over the last 10 years, the total amount spent by the City on pay raises for high earners was about $1.9 billion.\n\nCode\ntotal_raise_spending &lt;- \n  avg_pay_raise_rate_data |&gt; \n  inner_join(avg_rate_increase_per_agency_data, join_by(agency_name == agency_name)) |&gt;  \n  group_by(agency_name) |&gt;\n  summarize(total_raise_spend_per_year = sum(n_raises_per_agency * avg_pay_raise_rate * avg_rate_increase_per_agency * 2000, na.rm=TRUE)) \n  \ntotal_raise_spending |&gt;\n  slice_max(total_raise_spend_per_year,n=20) |&gt;\n  ggplot(aes(x = reorder(agency_name, total_raise_spend_per_year), y = total_raise_spend_per_year)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    title = paste(\"Top\",N_ITEMS,\"Agencies by Amount Spent on Pay Raises for High Earners\"),\n    x = \"Agency\",\n    y = \"Dollars\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 6: Agencies Spending the Most on Raises for High-Earners\n\n\n\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\nThe Commission estimates that, if high-earner raises had been capped at 0.05% in 2014, total savings by the City are approximately $976 million. Such a policy could have slashed 10-year-expenditure by up to 50%. The following analysis demonstrates the potential savings:\n\n\nCode\nsavings_from_capping &lt;- \n  high_earners_with_raises |&gt;\n  filter(n() &gt; 1, base_rate_change &gt; 0, excess_raise &gt; 0, excess_raise != Inf) |&gt;  \n  summarize(\n    total_savings = sum(excess_raise * FT_ANNUAL_HOURS, na.rm=TRUE),  # Assume full-time work hours\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_savings)) |&gt; \n  slice_max(total_savings, n=1)\n\n\nBased on these findings, the Commission recommends adopting a measured approach to this policy. High earners should be permitted to receive higher compensation if it is justified by demonstrable productivity improvements. This ensures that wage growth is tied to measurable value.\n\n\n1.6 Conclusion\nIn conclusion, the Commission’s report highlights the potential benefits of the implementation of three policies: capping salaries at the mayoral level, increasing staffing to reduce overtime, and reducing salary increases for top earners. Given the fiscal challenges facing New York City, and the Commission’s analysis of New York City’s personnel expenditure data, we most strongly support the adoption of Policy III. This option offers reasonable savings and is easy to implement."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "1.6 Conclusion",
    "text": "1.6 Conclusion\nIn conclusion, the Commission’s report highlights the potential benefits of the implementation of three policies: capping salaries at the mayoral level, increasing staffing to reduce overtime, and reducing salary increases for top earners. Given the fiscal challenges facing New York City, and the Commission’s analysis of New York City’s personnel expenditure data, we most strongly support the adoption of Policy III. This option offers reasonable savings and is easy to implement."
  },
  {
    "objectID": "mp01.html#references",
    "href": "mp01.html#references",
    "title": "THE COMMISSION TO ANALYZE TAXPAYER SPENDING (CATS)",
    "section": "References",
    "text": "References\nFox 5 New York. (2024, March 3). NYC population in 2024. FOX 5 New York. https://www.fox5ny.com/news/nyc-population-2024\nGoogle Gemini. (2025). A cartoon purple cat wearing a Sherlock Holmes hat and holding a magnifying glass. AI-generated image. Google AI.\nNew York City Comptroller. (2024). Annual state of the city’s economy and finances 2024. [Expenditure Differences] https://comptroller.nyc.gov/reports/annual-state-of-the-citys-economy-and-finances-2024/\nNew York City Comptroller. (2024). Annual state of the city’s economy and finances 2024. [Expenditure Analysis] https://comptroller.nyc.gov/reports/annual-state-of-the-citys-economy-and-finances-2024/\nOffice of the New York City Comptroller. (2024). Comments on New York City’s fiscal year 2024 adopted budget. New York City Comptroller. https://comptroller.nyc.gov/reports/comments-on-new-york-citys-fiscal-year-2024-adopted-budget/\nNew York City. (n.d.). Citywide payroll data: Fiscal year 2021. NYC Open Data. Retrieved Feruary 21, 2025, from https://data.cityofnewyork.us/City-Government/Citywide-Payroll-Data-Fiscal-Year-/k397-673e/about_data"
  },
  {
    "objectID": "mp01.html#footnotes",
    "href": "mp01.html#footnotes",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis logo was generated with Google Gemini (Gemini, 2025).↩︎\nThe Comptroller estimates that non-OT personnel spending will be $500 million lower than budgeted for in 2025 (Comptroller, 2024b). However, we have shown that an increase in regular personnel spending leads to substantial savings.↩︎"
  },
  {
    "objectID": "bruh.html",
    "href": "bruh.html",
    "title": " Analysis of New York City’s Payroll Expenditure Data",
    "section": "",
    "text": "poop"
  },
  {
    "objectID": "mp01.html#quick-facts",
    "href": "mp01.html#quick-facts",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "1.1 Quick Facts",
    "text": "1.1 Quick Facts\n\nThe job title with the highest base rate of pay is Chairman with a rate of $567 per hour.\n\n\n\nCode\nlibrary(readr)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(DT)\nlibrary(ggplot2)\n\nnyc_payroll &lt;- read_csv(\"data/mp01/nyc_payroll_export.csv\") |&gt; \n  mutate(agency_name = str_to_title(agency_name),\n         last_name = str_to_title(last_name),\n         first_name = str_to_title(first_name),\n         work_location_borough = str_to_title(work_location_borough),\n         title_description = str_to_title(title_description),\n         leave_status_as_of_june_30 = str_to_title(leave_status_as_of_june_30))\n\n# Constants\nWORK_HOURS_IN_DAY = 7.5\nFT_ANNUAL_HOURS = 2000\nN_ITEMS = 20\nPAY_RAISE_CAP = 0.005\n\nHOURLY_RATE_MAX = nyc_payroll |&gt; \n  filter(pay_basis == \"per Hour\") |&gt;\n  arrange(base_salary) |&gt;\n  mutate(jump = base_salary - lag(base_salary)) |&gt;\n  select(jump, base_salary, agency_name) |&gt;\n  arrange(jump) |&gt;\n  slice_max(jump,n=1) |&gt;\n  pull(base_salary)\n\n# Adding total_pay and base_rt_hr columns.\nnyc_payroll &lt;-\n  nyc_payroll |&gt;\n  mutate(total_pay =\n           case_when(\n             (pay_basis == \"per Day\" & regular_hours &gt;= 0 & ot_hours &lt; 0) ~ base_salary * floor(regular_hours / WORK_HOURS_IN_DAY),\n             (pay_basis == \"per Day\" & regular_hours &gt;= 0 & ot_hours &gt;= 0) ~ base_salary * floor(regular_hours / WORK_HOURS_IN_DAY) + ((base_salary/WORK_HOURS_IN_DAY) * ot_hours * 1.5),\n             (pay_basis == \"per Hour\" & regular_hours &gt;= 0 & ot_hours &gt;= 0) ~ base_salary * (regular_hours + ot_hours * 1.5),\n             (pay_basis == \"per Hour\" & regular_hours &gt;= 0 & ot_hours &lt; 0) ~ base_salary * regular_hours,\n             (pay_basis == \"per Hour\" & regular_hours &lt; 0) ~ NA,\n             .default = base_salary + (base_salary/FT_ANNUAL_HOURS * 1.5 * ot_hours),\n             ),\n         base_rate_hr = case_when(\n           pay_basis == \"per Day\" & (base_salary/WORK_HOURS_IN_DAY) &lt; HOURLY_RATE_MAX ~ base_salary / WORK_HOURS_IN_DAY,\n           pay_basis == \"per Day\" & (base_salary/WORK_HOURS_IN_DAY) &gt;= HOURLY_RATE_MAX ~ base_salary / FT_ANNUAL_HOURS, # I assume they meant pay basis is per Annum\n           pay_basis == \"per Hour\" & base_salary &lt; HOURLY_RATE_MAX ~ base_salary,\n           pay_basis == \"per Hour\" & base_salary &gt;= HOURLY_RATE_MAX ~ base_salary / FT_ANNUAL_HOURS, # I assume they meant per Annum\n           TRUE ~ base_salary / FT_ANNUAL_HOURS\n      )\n    )\n\nhighest_base_rate &lt;-\n  nyc_payroll |&gt;\n  select(fiscal_year,\n         last_name,mid_init,first_name,\n         title_description,\n         agency_name,\n         base_rate_hr) |&gt;\n  slice_max(base_rate_hr, n=1)\n\n\n\nThe individual with the single highest city total payroll was Chief Fire Marshal at the Fire Department of New York (FDNY), Daniel E. Flynn. He earned $511,821 in 2021.\n\n\n\nCode\n# Which individual & in what year had the single highest city total payroll (regular and overtime combined)?\nmost_paid &lt;-\n  nyc_payroll |&gt;\n  filter(total_pay == max(total_pay,na.rm = TRUE)) |&gt;\n  select(fiscal_year,last_name,first_name,mid_init, agency_name,title_description, total_pay)\n\n\n\nThe individual who worked the most overtime hours in a single year was James Internicola, Correction Officer at the Department Of Correction. He logged 3,693 hours of overtime in 2022.\n\n\n\nCode\n# Which individual worked the most overtime hours in this data set?\nmost_OT &lt;-\n  nyc_payroll |&gt;\n  filter(ot_hours == max(ot_hours)) |&gt;\n  select(fiscal_year, last_name, first_name, mid_init,agency_name,title_description,ot_hours)\n\n\n\nThe Department of Education has the highest average total annual payroll at $10,083,191,004 a year.\n\n\n\nCode\nhigh_total_annual_payroll &lt;-\n  nyc_payroll |&gt;\n  group_by(agency_name, fiscal_year) |&gt;\n  summarize(\n    sum_total_payroll = sum(total_pay,na.rm=TRUE),\n    n_emp = n()\n  ) |&gt;\n  group_by(agency_name) |&gt;\n  summarize(mean_total_annual_payroll = mean(sum_total_payroll, na.rm = TRUE)) |&gt;\n  slice_max(mean_total_annual_payroll, n=1)\n\n\n\nThe Department of Education has the most employees on payroll in each year.\n\n\n\nCode\nmost_emps &lt;-\n  nyc_payroll |&gt;\n  group_by(fiscal_year, agency_name) |&gt;\n  summarize(\n    num_employees = n(), .groups = \"drop\"\n  ) |&gt;\n  slice_max(num_employees, n = 1, by = fiscal_year)\n\n\n\nThe Board of Elections has the highest overtime (OT) usage, where employees log approximately 1 hour of OT for every 5 regular hours.\n\n\n\nCode\nhighest_OT_usage &lt;-\n  nyc_payroll |&gt;\n  group_by(agency_name) |&gt;\n  summarize(\n    sum_reg = sum(regular_hours,na.rm=TRUE),\n    sum_ot = sum(ot_hours,na.rm=TRUE),\n    ot_ratio = sum_ot/sum_reg\n  ) |&gt;\n  slice_max(ot_ratio, n = 1)\n\n\n\nThe average salary of employees who work outside of the five boroughs is $97,125.\n\n\n\nCode\nsalary_out_nyc &lt;-\n  nyc_payroll |&gt;\n  filter(\n    !is.na(work_location_borough),\n    !work_location_borough %in% c(\"Bronx\",\"Brooklyn\",\"Queens\",\"Manhattan\",\"Staten Island\")\n    ) |&gt;\n  summarize(average = mean(total_pay,na.rm=TRUE))\n\n\n\nFrom 2014 to 2024, the city’s aggregate payroll grew 45%, from $22,452,904,026 to $32,635,103,990.\n\n\n\nCode\nactual_payroll &lt;-\n  nyc_payroll |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(\n    aggregate_pay = sum(total_pay,na.rm=TRUE)\n    ) |&gt;\n  mutate(percent_change = ((aggregate_pay - lag(aggregate_pay)) / lag(aggregate_pay)) * 100)"
  },
  {
    "objectID": "mp01.html#policy-i-capping-salaries-at-mayoral-level",
    "href": "mp01.html#policy-i-capping-salaries-at-mayoral-level",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "5.1 Policy I: Capping Salaries at Mayoral Level",
    "text": "5.1 Policy I: Capping Salaries at Mayoral Level\nOne approach to reducing personnel expenditures is to cap the salaries of subordinate employees, so that the mayor of the City earns the highest salary. Table 2 presents the salaries of New York City’s Mayor from 2014 to 2024.\n\nTable 2: Salaries of New York City’s Mayor (2014-2024)\n\n\nCode\nmayor_pay &lt;- nyc_payroll |&gt;\n  filter(title_description == \"Mayor\" &\n           leave_status_as_of_june_30 == \"Active\") |&gt;\n  select(fiscal_year, mayoral_cap = total_pay)\n\nmayor_pay |&gt;\n  rename(\"Fiscal Year\" = fiscal_year,\n         \"Mayoral Cap ($)\" = mayoral_cap\n         ) |&gt;\n  datatable(options = list(pageLength = N_ITEMS, autoWidth = TRUE), \n            rownames = FALSE) |&gt; \n  formatCurrency(columns = \"Mayoral Cap ($)\", currency = \"\", digits = 0)\n\n\n\n\n\n\n\nIn total, 1,566 job titles earned more money than the mayor did during the the same fiscal year. A distribution of the agencies with employees exceeding the mayoral salary is shown in figure 2. The vast majority of people who earned more money than the mayor work for the Department of Corrections. This is followed by the FDNY and the NYPD. These agencies would see the largest savings if personnel salaries are capped at the mayoral level.\n\nCode\nexceeding_agencies &lt;- nyc_payroll |&gt; \n  left_join(mayor_pay, join_by(fiscal_year == fiscal_year)) |&gt;\n  filter(total_pay &gt; mayoral_cap) |&gt;\n  select(fiscal_year, last_name, mid_init, first_name, agency_name, title_description, total_pay) |&gt;\n  group_by(agency_name) |&gt;\n  summarize(n = n())\n\nremaining_agency_count &lt;- max(nrow(exceeding_agencies) - N_ITEMS, 0)\n\nremaining_sum &lt;- exceeding_agencies |&gt; \n  slice_min(n, n = remaining_agency_count) |&gt; \n  summarize(sum_n = sum(n)) |&gt; \n  pull(sum_n)\n\nggplot(exceeding_agencies |&gt; slice_max(n, n=N_ITEMS), aes(x = reorder(agency_name, n), y = n)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +\n  labs(\n    x = \"Agency\",\n    y = \"# of Entries Exceeding Cap \\n(FY2014-FY2024)\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 2: Top 20 Number of Payroll Entries Exceeding Mayoral Cap, by Agency\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\nThe Commission identifies agency title groupings that have significant numbers of payroll entries over the mayoral cap. These are presented in table 3. The top two rows suggest that Captains at the Department of Corrections and the Fire Department would be most affected by this policy.\n\nTable 3: Agency-Title Pairs Exceeding Mayoral Cap\n\n\nCode\nagency_titles_over_mayoral_cap &lt;-\n  nyc_payroll |&gt; \n  left_join(mayor_pay, join_by(fiscal_year == fiscal_year)) |&gt;\n  mutate(is_over_cap = total_pay &gt; mayoral_cap) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(n = n(), n_over = sum(is_over_cap, na.rm = TRUE), freq = mean(is_over_cap, na.rm = TRUE)) \n\nagency_titles_over_mayoral_cap |&gt;\n  filter(n_over &gt; 0) |&gt;\n  select(agency_name,title_description,n_over) |&gt;\n  arrange(desc(n_over)) |&gt;\n  rename(\"Agency Name\" = agency_name,\n         \"Title Description\" = title_description,\n         \"# of Entries Over Mayoral Cap\" = n_over) |&gt;\n  datatable(options = list(pageLength = N_ITEMS,\n                           autoWidth = TRUE))\n\n\n\n\n\n\n\nThe total savings per fiscal year that could have resulted from enactment of the proposed policy are presented in table 4. The Commission notes that, had the proposed policy been enacted in 2014, New York City would have seen total savings of $39,074,243.\n\nTable 4: Savings per Annum from Policy I\n\n\nCode\n# Determine total savings if these employees’ compensation were capped at the mayor’s salary.\nsavings_per_year &lt;-\n  nyc_payroll |&gt; \n  left_join(mayor_pay, join_by(fiscal_year == fiscal_year)) |&gt;\n  filter(total_pay &gt; mayoral_cap) |&gt;\n  mutate(savings = total_pay - mayoral_cap) |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(total_savings = round(sum(savings,na.rm=TRUE),0), n_over = n())\n\ntotal_savings &lt;- savings_per_year |&gt; summarize(sum(total_savings, na.rm=TRUE))\n\nsavings_per_year |&gt;\n  rename(\"Fiscal Year\" = fiscal_year,\n                   \"# of Entries Exceeding Mayoral Cap\" = n_over,\n                   \"Savings ($)\" = total_savings) |&gt;\n  datatable(options = list(pageLength = N_ITEMS,\n                           autoWidth = TRUE))|&gt;\n  formatCurrency(columns = \"Savings ($)\", currency = \"\", digits = 0)\n\n\n\n\n\n\n\nThe Commission does not recommend a blanket capping subordinate salaries at the mayoral level. A majority of employees earning more than the mayor hold positions that involve significant risks. Given the nature of these high-risk jobs, the Commission believes that these employees should be fairly compensated for the additional responsibilities and dangers they face in their roles. However, the Commission is not against capping salaries on a title-by-title basis."
  },
  {
    "objectID": "mp01.html#policy-ii-increasing-staffing-to-reduce-overtime-expenses",
    "href": "mp01.html#policy-ii-increasing-staffing-to-reduce-overtime-expenses",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "5.2 Policy II: Increasing Staffing to Reduce Overtime Expenses",
    "text": "5.2 Policy II: Increasing Staffing to Reduce Overtime Expenses\nA second approach to curtailing personnel expenditure is increasing staffing to reduce OT expenses. For this analysis, the Commission assumes that a full-time position is 2000 hours a year. Figure 3 presents OT hours converted to the number of new full-time employees required to fulfill the same amount of work hours. The total number of new employees needed to address the current OT load is 200,412. This represents the workforce that would need to be hired if OT hours were eliminated through additional staffing. The Commission notes that this number is approximately 50% of the last fiscal year’s workforce 2.\n\nCode\nnyc_payroll |&gt;\n  filter(ot_hours &gt; 0) |&gt;\n  group_by(agency_name, title_description, fiscal_year) |&gt;\n  summarize(total_ot = sum(ot_hours), .groups = \"drop\") |&gt;\n  mutate(n_fte_yearly = total_ot / 2000) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(total_n_fte = sum(n_fte_yearly, na.rm=TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(total_n_fte)) |&gt;\n  slice_max(total_n_fte, n = N_ITEMS) |&gt;  # Show top 20 only\n  ggplot(aes(x = reorder(paste(agency_name, title_description, sep=\" | \"), total_n_fte), y = total_n_fte)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    x = \"Agency | Job Title\",\n    y = \"Number of Positions\",\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 3: Top 20 Full Time Personnel Counts, by Agency and Title \n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\nThe total potential savings from this proposed policy amount to $15,025,441,508 (Table 5). The commission notes that the potential savings from this policy are almost 400x that of Policy I.\nEnacting this policy entails recruiting:\n\nover 30,000 police officers,\nover 20,000 firefighters,\nand over 15,000 corrections officers.\n\nThese positions are difficult to fill, as they are high-risk in nature and require specialized training. Therefore, the Commission recommends a more feasible, scaled-down version of this policy. Under this modified approach, the City would subsidize the training costs for these critical positions, and the recruitment rate would be regularly assessed to ensure that sufficient progress is being made in filling these roles.\nTable 5 presents total savings by agency.\n\nTable 5: Total Savings by Agency\n\n\nCode\nnyc_payroll |&gt;\n  filter(ot_hours &gt; 0) |&gt;\n  mutate(savings = base_rate_hr * ot_hours) |&gt;\n  group_by(agency_name) |&gt;\n  summarize(total_savings = round(sum(savings, na.rm=TRUE),0)) |&gt;\n  arrange(desc(total_savings)) |&gt;\n  rename(\"Agency Name\" = agency_name,\n         \"Savings ($)\" = total_savings) |&gt;\n  datatable(options = list(pageLength = N_ITEMS,\n                           autoWidth = TRUE)) |&gt;\n  formatCurrency(columns = \"Savings ($)\", currency = \"\", digits = 0)\n\n\n\n\n\n\n\n5.3 Policy III: Reduce Salary Increases for Top Earners\nThe third policy the Commission proposes to reduce personnel spending is the reduction of salary increases for the top 10% of earners in their respective agencies to 1.05% of their base hourly rate. The Commission found that on average, high earners get a raise every 1.85 years, and presents the distribution of the frequency of raises per year in figure 4.\n\nCode\nhigh_earners &lt;- \n  nyc_payroll |&gt;\n  group_by(agency_name) |&gt;\n  mutate(high_earner_threshold = quantile(total_pay, 0.9, na.rm = TRUE)) |&gt;  \n  ungroup() |&gt;  \n  filter(!is.na(total_pay),\n         !is.na(first_name),\n         !is.na(last_name),\n         !is.na(base_rate_hr),\n         !is.na(fiscal_year),\n         total_pay &gt; high_earner_threshold)\n\nhigh_earners_with_raises &lt;- \n  high_earners |&gt;\n  group_by(agency_name, title_description, first_name, mid_init, last_name) |&gt;\n  arrange(fiscal_year) |&gt;\n  mutate(\n    base_rate_change = case_when( is.na(lag(base_rate_hr)) | is.na(lag(fiscal_year)) ~ 0,\n                                  TRUE ~ (base_rate_hr - lag(base_rate_hr))/(fiscal_year - lag(fiscal_year))),\n    years_worked = max(fiscal_year,na.rm=TRUE) - min(fiscal_year,na.rm=TRUE), \n    n_raises = sum(base_rate_change &gt; 0, na.rm = TRUE),\n    raises_per_year = n_raises / years_worked,\n    avg_rate_increase_per_person = mean(base_rate_change, na.rm=TRUE),\n    capped_raise = PAY_RAISE_CAP * lag(base_rate_hr, order_by = fiscal_year),  \n    excess_raise = pmax(base_rate_change - capped_raise, 0)\n  ) |&gt;\n  filter(!is.na(n_raises),years_worked&gt;0) |&gt;\n  ungroup()\nhigh_earners_with_raises |&gt;\n  filter(raises_per_year &lt;= 1) |&gt;\n  ggplot(aes(x = raises_per_year)) +\n  geom_histogram(binwidth = 0.05, fill = \"#69b3a2\", color = \"white\") +\n  labs(\n    x = \"Raises per Year\",\n    y = \"Number of Employees\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 4: Distribution of Pay Raise Frequency\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\nWe determine the agencies and titles with the highest average number of raises per year, and find 523 high-salary jobs with an average pay raise rate of once a year.\n\n\nCode\navg_pay_raise_rate_data &lt;-\n  high_earners_with_raises |&gt;\n  filter(raises_per_year &gt; 0 , raises_per_year &lt;= 1 ) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(avg_pay_raise_rate = mean(raises_per_year, na.rm=TRUE),\n            med_pay_raise_rate = median(raises_per_year, na.rm=TRUE),\n            n_raises_per_agency = sum(n_raises,na.rm=TRUE),\n            .groups=\"drop\")\n\n\nWe find the average amount that the hourly base rate increases per agency. The highest average increase is $7.70 at Brooklyn Community Board 12. The Commission also notes the appearance of 11 community boards within the top 20 agencies (Figure 5).\n\nCode\navg_rate_increase_per_agency_data &lt;-\n  high_earners_with_raises |&gt; \n  group_by(agency_name) |&gt;\n  filter(avg_rate_increase_per_person != Inf, avg_rate_increase_per_person != -Inf ) |&gt;\n  summarize(avg_rate_increase_per_agency = mean(avg_rate_increase_per_person, na.rm=TRUE), .groups = \"drop\")\n\navg_rate_increase_per_agency_data |&gt;\n  slice_max(avg_rate_increase_per_agency, n = N_ITEMS) |&gt;  \n  ggplot(aes(x = reorder(agency_name, avg_rate_increase_per_agency), y = avg_rate_increase_per_agency)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    x = \"Agency\",\n    y = \"Mean Pay Raise Amount ($/hr)\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 5: Top 20 Average Increases in Hourly Rate, by Agency\n\n\n\n\n\nAverage pay raise amount per agency\n\n\n\n\n\n\nFigure 5\n\n\n\nCombining these two metrics, we get an estimate of the amount spent per agency on high-earner raises over the last 10 fiscal years. The Department of Education far outranks all other agencies with an expenditure of almost $500 million (Figure 6). They are followed by the Police Department at just over $200 million. Over the last 10 years, the total amount spent by the City on pay raises for high earners was about $1.9 billion.\n\nCode\ntotal_raise_spending &lt;- \n  avg_pay_raise_rate_data |&gt; \n  inner_join(avg_rate_increase_per_agency_data, join_by(agency_name == agency_name)) |&gt;  \n  group_by(agency_name) |&gt;\n  summarize(total_raise_spend_per_year = sum(n_raises_per_agency * avg_pay_raise_rate * avg_rate_increase_per_agency * 2000, na.rm=TRUE)) \n  \ntotal_raise_spending |&gt;\n  mutate(in_millions = total_raise_spend_per_year/(10^6))|&gt;\n  slice_max(in_millions,n=20) |&gt;\n  ggplot(aes(x = reorder(agency_name, in_millions), y = in_millions)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    x = \"Agency\",\n    y = \"Dollars (in millions)\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 6: Top 20 Agencies Spending the Most on Raises for High-Earners\n\n\n\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\nThe Commission estimates that, if high-earner raises had been capped at 0.05% in 2014, total savings by the City are approximately $976 million. Such a policy could have slashed 10-year-expenditure by up to 50%. The following analysis demonstrates the potential savings:\n\n\nCode\nsavings_from_capping &lt;- \n  high_earners_with_raises |&gt;\n  filter(n() &gt; 1, base_rate_change &gt; 0, excess_raise &gt; 0, excess_raise != Inf) |&gt;  \n  summarize(\n    total_savings = sum(excess_raise * FT_ANNUAL_HOURS, na.rm=TRUE),  # Assume full-time work hours\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_savings)) |&gt; \n  slice_max(total_savings, n=1)\n\n\nBased on these findings, the Commission recommends adopting a measured approach to this policy. High earners should be permitted to receive higher compensation if it is justified by demonstrable productivity improvements. This ensures that wage growth is tied to measurable value.\n\n\n6 Conclusion\nIn conclusion, the Commission’s report highlights the potential benefits of the implementation of three policies: capping salaries at the mayoral level, increasing staffing to reduce overtime, and reducing salary increases for top earners. Given the fiscal challenges facing New York City, and the Commission’s analysis of New York City’s personnel expenditure data, we most strongly support the adoption of Policy III. This option offers reasonable savings and is easy to implement."
  },
  {
    "objectID": "mp01.html#policy-iii-reduce-salary-increases-for-top-earners",
    "href": "mp01.html#policy-iii-reduce-salary-increases-for-top-earners",
    "title": "Analysis of New York City’s Payroll Expenditure Data",
    "section": "5.3 Policy III: Reduce Salary Increases for Top Earners",
    "text": "5.3 Policy III: Reduce Salary Increases for Top Earners\nThe third policy the Commission proposes to reduce personnel spending is the reduction of salary increases for the top 10% of earners in their respective agencies to 1.05% of their base hourly rate. The Commission found that on average, high earners get a raise every 1.85 years, and presents the distribution of the frequency of raises per year in figure 4.\n\nCode\nhigh_earners &lt;- \n  nyc_payroll |&gt;\n  group_by(agency_name) |&gt;\n  mutate(high_earner_threshold = quantile(total_pay, 0.9, na.rm = TRUE)) |&gt;  \n  ungroup() |&gt;  \n  filter(!is.na(total_pay),\n         !is.na(first_name),\n         !is.na(last_name),\n         !is.na(base_rate_hr),\n         !is.na(fiscal_year),\n         total_pay &gt; high_earner_threshold)\n\nhigh_earners_with_raises &lt;- \n  high_earners |&gt;\n  group_by(agency_name, title_description, first_name, mid_init, last_name) |&gt;\n  arrange(fiscal_year) |&gt;\n  mutate(\n    base_rate_change = case_when( is.na(lag(base_rate_hr)) | is.na(lag(fiscal_year)) ~ 0,\n                                  TRUE ~ (base_rate_hr - lag(base_rate_hr))/(fiscal_year - lag(fiscal_year))),\n    years_worked = max(fiscal_year,na.rm=TRUE) - min(fiscal_year,na.rm=TRUE), \n    n_raises = sum(base_rate_change &gt; 0, na.rm = TRUE),\n    raises_per_year = n_raises / years_worked,\n    avg_rate_increase_per_person = mean(base_rate_change, na.rm=TRUE),\n    capped_raise = PAY_RAISE_CAP * lag(base_rate_hr, order_by = fiscal_year),  \n    excess_raise = pmax(base_rate_change - capped_raise, 0)\n  ) |&gt;\n  filter(!is.na(n_raises),years_worked&gt;0) |&gt;\n  ungroup()\nhigh_earners_with_raises |&gt;\n  filter(raises_per_year &lt;= 1) |&gt;\n  ggplot(aes(x = raises_per_year)) +\n  geom_histogram(binwidth = 0.05, fill = \"#69b3a2\", color = \"white\") +\n  labs(\n    x = \"Raises per Year\",\n    y = \"Number of Employees\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 4: Distribution of Pay Raise Frequency\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\nWe determine the agencies and titles with the highest average number of raises per year, and find 523 high-salary jobs with an average pay raise rate of once a year.\n\n\nCode\navg_pay_raise_rate_data &lt;-\n  high_earners_with_raises |&gt;\n  filter(raises_per_year &gt; 0 , raises_per_year &lt;= 1 ) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(avg_pay_raise_rate = mean(raises_per_year, na.rm=TRUE),\n            med_pay_raise_rate = median(raises_per_year, na.rm=TRUE),\n            n_raises_per_agency = sum(n_raises,na.rm=TRUE),\n            .groups=\"drop\")\n\n\nWe find the average amount that the hourly base rate increases per agency. The highest average increase is $7.70 at Brooklyn Community Board 12. The Commission also notes the appearance of 11 community boards within the top 20 agencies (Figure 5).\n\nCode\navg_rate_increase_per_agency_data &lt;-\n  high_earners_with_raises |&gt; \n  group_by(agency_name) |&gt;\n  filter(avg_rate_increase_per_person != Inf, avg_rate_increase_per_person != -Inf ) |&gt;\n  summarize(avg_rate_increase_per_agency = mean(avg_rate_increase_per_person, na.rm=TRUE), .groups = \"drop\")\n\navg_rate_increase_per_agency_data |&gt;\n  slice_max(avg_rate_increase_per_agency, n = N_ITEMS) |&gt;  \n  ggplot(aes(x = reorder(agency_name, avg_rate_increase_per_agency), y = avg_rate_increase_per_agency)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    x = \"Agency\",\n    y = \"Mean Pay Raise Amount ($/hr)\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 5: Top 20 Average Increases in Hourly Rate, by Agency\n\n\n\n\n\nAverage pay raise amount per agency\n\n\n\n\n\n\nFigure 5\n\n\n\nCombining these two metrics, we get an estimate of the amount spent per agency on high-earner raises over the last 10 fiscal years. The Department of Education far outranks all other agencies with an expenditure of almost $500 million (Figure 6). They are followed by the Police Department at just over $200 million. Over the last 10 years, the total amount spent by the City on pay raises for high earners was about $1.9 billion.\n\nCode\ntotal_raise_spending &lt;- \n  avg_pay_raise_rate_data |&gt; \n  inner_join(avg_rate_increase_per_agency_data, join_by(agency_name == agency_name)) |&gt;  \n  group_by(agency_name) |&gt;\n  summarize(total_raise_spend_per_year = sum(n_raises_per_agency * avg_pay_raise_rate * avg_rate_increase_per_agency * 2000, na.rm=TRUE)) \n  \ntotal_raise_spending |&gt;\n  mutate(in_millions = total_raise_spend_per_year/(10^6))|&gt;\n  slice_max(in_millions,n=20) |&gt;\n  ggplot(aes(x = reorder(agency_name, in_millions), y = in_millions)) +\n  geom_col(fill = \"#69b3a2\") +\n  coord_flip() +  \n  labs(\n    x = \"Agency\",\n    y = \"Dollars (in millions)\"\n  ) +\n  theme_minimal()\n\n\n\n\nFigure 6: Top 20 Agencies Spending the Most on Raises for High-Earners\n\n\n\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\nThe Commission estimates that, if high-earner raises had been capped at 0.05% in 2014, total savings by the City are approximately $976 million. Such a policy could have slashed 10-year-expenditure by up to 50%. The following analysis demonstrates the potential savings:\n\n\nCode\nsavings_from_capping &lt;- \n  high_earners_with_raises |&gt;\n  filter(n() &gt; 1, base_rate_change &gt; 0, excess_raise &gt; 0, excess_raise != Inf) |&gt;  \n  summarize(\n    total_savings = sum(excess_raise * FT_ANNUAL_HOURS, na.rm=TRUE),  # Assume full-time work hours\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_savings)) |&gt; \n  slice_max(total_savings, n=1)\n\n\nBased on these findings, the Commission recommends adopting a measured approach to this policy. High earners should be permitted to receive higher compensation if it is justified by demonstrable productivity improvements. This ensures that wage growth is tied to measurable value."
  },
  {
    "objectID": "final_project_proposal.html#the-animating-or-overarching-question-of-the-project",
    "href": "final_project_proposal.html#the-animating-or-overarching-question-of-the-project",
    "title": "Untitled",
    "section": "The Animating or Overarching Question of the Project",
    "text": "The Animating or Overarching Question of the Project\nWe are reviewing the factors that impact housing prices in Connecticut.\nWe hypothesize that these factors are relevant:\n\nSchool rankings\n\nCrime data\n\nLocal economy (property tax, GDP, and household income)\n\nPublic transit accessibility (train station/bus route count, frequency of departures/arrivals, distance to stops)\n\nWe will analyze these factors within CT to determine the best predictor of housing prices.\nFollowing this analysis, we will compare our findings to data from New York or California to identify common trends and state-specific challenges."
  },
  {
    "objectID": "final_project_proposal.html#public-data-sources-you-intend-to-analyze-at-least-two",
    "href": "final_project_proposal.html#public-data-sources-you-intend-to-analyze-at-least-two",
    "title": "Untitled",
    "section": "Public Data Sources You Intend to Analyze (At Least Two)",
    "text": "Public Data Sources You Intend to Analyze (At Least Two)\n\nConnecticut Real Estate Sales 2001-2022\n\nZillow Housing Data\n\nNew York City Sales Data\nConnecticut GDP\n\nNew York State (Excluding NYC) Sales Data\n\nCounty-Level Education Readiness Score"
  },
  {
    "objectID": "final_project_proposal.html#specific-questions-you-hope-to-answer-in-your-analysis",
    "href": "final_project_proposal.html#specific-questions-you-hope-to-answer-in-your-analysis",
    "title": "Untitled",
    "section": "Specific Questions You Hope to Answer in Your Analysis",
    "text": "Specific Questions You Hope to Answer in Your Analysis\n\nHow strongly does the real estate market correlate with economic growth?\n\nHow does the crime rate impact the number of real estate sales?\n\nHow do changes in a school district’s performance affect housing sales/prices?\n\nWhat is the effect of public transit availability on housing prices?\n\nWhat is the relationship between public transit growth and GDP growth?"
  },
  {
    "objectID": "final_project_proposal.html#rough-analytical-plan",
    "href": "final_project_proposal.html#rough-analytical-plan",
    "title": "Untitled",
    "section": "Rough Analytical Plan",
    "text": "Rough Analytical Plan\nWe will examine county-level data for school performance, economic performance, and crime rates.\nKey Metrics to Define:\n\nSchool rankings (standardized test scores, graduation rates, funding per student)\n\nCrime data (violent vs. non-violent crime rates, historical trends)\n\nEconomic indicators (GDP per capita, property tax rates, median household income)\n\nPublic transit accessibility (distance to nearest station, frequency of service)"
  },
  {
    "objectID": "final_project_proposal.html#anticipated-challenges",
    "href": "final_project_proposal.html#anticipated-challenges",
    "title": "Untitled",
    "section": "Anticipated Challenges",
    "text": "Anticipated Challenges\n\nData availability and completeness\n\nDifferences in data formats across states\n\nControlling for confounding variables (e.g., urban vs. suburban housing markets)"
  },
  {
    "objectID": "final_project_proposal.html#list-of-team-members",
    "href": "final_project_proposal.html#list-of-team-members",
    "title": "Untitled",
    "section": "List of Team Members",
    "text": "List of Team Members\n\nHong Zhuang\n\nCraig Allen\n\nJocely Lopez Luna"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "",
    "text": "The Official 2025 Greenies Flyer1.\nNEW YORK, March 2, 2025 – Since 1997, the Green Transit Alliance for Investigation of Variance (GTA IV) scours publicly available transit data to honor the transportation agencies that champion environmental responsibility in their operations. On March 1, 2025, GTA IV presented awards at the annual Greenies in New York City. “These awards highlight the incredible efforts of transit agencies committed to sustainability,” said Jocely Lopez Luna, Executive Director of GTA IV, in her opening statement at the Noli Timere theater. Small-, medium-, and large-sized transit agencies were awarded for their excellence in three categories:\nWhile the Greenies celebrate sustainability leaders, GTA IV also holds accountable those agencies that fall short. This year, the organization introduced the Noxious rating. Before handing out the ratings, Lopez Luna expressed a plucky, “hope that [these transit agencies] will reflect on and lessen the environmental impact of [their] operations.”"
  },
  {
    "objectID": "mp02.html#award-winners",
    "href": "mp02.html#award-winners",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "1 Award Winners",
    "text": "1 Award Winners\n\n1.1 Greenest Transit Agency\nThree winners received Greenies for Greenest Transit Agency:\n\nGreenest Small-sized Transit Agency:\n\nMecklenburg County, dba: Mecklenburg Transportation System from Portland, Oregon, with a green score of \\(0.49\\).\n\nGreenest Medium-sized Transit Agency:\n\nCity of Seattle, dba: Seattle Center Monorail from Seattle, Washington, with an impressive green score of \\(0.07\\).\n\nGreenest Large-sized Transit Agency:\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet, from Portland, Oregon, with a green score\\(^{\\text{tm}}\\) of \\(0.19\\).\n\n\nGTA IV’s green score is calculated from the geometric mean1 of agency emissions per capita (EPC) and agency emissions per mile (EPM). Lower green scores mean greener agencies.\n\nBody Paragraphs (Detailed Information & Key Findings) Discuss the methodology: How data was analyzed (multi-table joins, data sources used)\n\nCriteria for selection (e.g., fuel efficiency, emissions reduction, ridership sustainability) List the award categories and winners (e.g., Best Overall Green Transit, Most Energy-Efficient Fleet, Lowest Carbon Emissions Per Rider). Highlight notable statistics and insights that led to each agency’s selection. Quotes from winning transit agencies or industry experts add credibility and engagement. 7. Closing Paragraph (Call to Action & Future Outlook) Mention any future initiatives or research by GTA IV. Encourage public transit agencies to adopt greener practices. Provide a link or contact for more information. 8. Boilerplate (About GTA IV) A short section about GTA IV, its mission, and its work in transit sustainability. 9. ###END### Indicates the end of the press release."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis image was generated with Google Gemini (Google Gemini, 2025).↩︎\nThe geometric mean was chosen because EPC and EPM are on different scales.↩︎"
  },
  {
    "objectID": "mp02.html#and-the-greenie-goes-to",
    "href": "mp02.html#and-the-greenie-goes-to",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "1 And the Greenie Goes To…",
    "text": "1 And the Greenie Goes To…\n\n1.1 Greenest Transit Agency\nThree winners received Greenies for Greenest Transit Agency:\n\nGreenest Small-sized Transit Agency:\n\nMecklenburg County, dba: Mecklenburg Transportation System from Portland, Oregon, with a green score of \\(0.49\\).\n\nGreenest Medium-sized Transit Agency:\n\nCity of Seattle, dba: Seattle Center Monorail from Seattle, Washington, with an impressive green score of \\(0.07\\).\n\nGreenest Large-sized Transit Agency:\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet, from Portland, Oregon, with a green score of \\(0.19\\).\n\n\nGTA IV’s green score is calculated from the geometric mean1 of agency emissions per capita (EPC) and agency emissions per mile (EPM). Lower green scores mean greener agencies.\n\n\n1.2 Most Emissions Avoided\nThe three winners of the Greenies for Most Emissions Avoided were:\n\nSmall-sized Transit Agency:\n\nHampton Jitney, Inc. from Calverton, New York, which offset an estimated \\(8,586,353\\) kg of carbon dioxide (\\(\\text{CO}_2\\)).\n\nMedium-sized Transit Agency:\n\nHudson Transit Lines, Inc., dba: Short Line from Mahwah, New Jersey, which offset \\(16,147,490\\) kg of \\(\\text{CO}_2\\).\n\nLarge-sized Transit Agency:\n\nMTA New York City Transit from Brooklyn, New York which offset a whopping \\(1,662,602,856\\) kg of \\(\\text{CO}_2\\).\n\n\nThe total value of emissions avoided came from finding out the equivalent number of gallons of gasoline required by the newest model of car to cover the miles traveled by each agency. then multiplied by the average emissions rate of motor fuel gasoline as given in EIA co2 vol mass excel.\n\n\n1.3 Statal Eco-Leader\nThree agencies were awarded a Greenie for outstanding performance in their state.\n\nSmall-sized Transit Agency:\n\nOhio Valley Regional Transportation Authority from Wheeling, West Virginia, whose state’s electricity EPC is \\(15479.34\\) times larger than their EPC.\n\nMedium-sized Transit Agency:\n\nCity of Seattle, dba: Seattle Center Monorail Seattle from Seattle, Washington, with a factor of \\(54627.65\\) difference between their EPC and their state’s electricity EPC.\n\nLarge-sized Transit Agency:\n\nCity of Portland, dba: Portland Streetcar from Portland, Oregon, whose state’s EPC is \\(18649.95\\) times larger than their own.\n\n\n\nBody Paragraphs (Detailed Information & Key Findings) Discuss the methodology: How data was analyzed (multi-table joins, data sources used)\n\nCriteria for selection (e.g., fuel efficiency, emissions reduction, ridership sustainability) List the award categories and winners (e.g., Best Overall Green Transit, Most Energy-Efficient Fleet, Lowest Carbon Emissions Per Rider). Highlight notable statistics and insights that led to each agency’s selection. Quotes from winning transit agencies or industry experts add credibility and engagement. 7. Closing Paragraph (Call to Action & Future Outlook) Mention any future initiatives or research by GTA IV. Encourage public transit agencies to adopt greener practices. Provide a link or contact for more information. 8. Boilerplate (About GTA IV) A short section about GTA IV, its mission, and its work in transit sustainability. 9. ###END### Indicates the end of the press release."
  },
  {
    "objectID": "mp02.html#greenest-transit-agency",
    "href": "mp02.html#greenest-transit-agency",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Greenest Transit Agency",
    "text": "Greenest Transit Agency\nThree winners took home Greenies for Greenest Transit Agency:\n\nGreenest Small-sized Transit Agency:\n\nMecklenburg County, dba: Mecklenburg Transportation System from Charlotte, North Carolina, boasting a green score of \\(0.49\\).\n\nGreenest Medium-sized Transit Agency:\n\nCity of Seattle, dba: Seattle Center Monorail from Seattle, Washington, with an impressively low green score of \\(0.07\\).\n\nGreenest Large-sized Transit Agency:\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet, from Portland, Oregon, earning a green score of \\(0.19\\).\n\n\nGTA IV’s green score was calculated from the geometric mean2 of agency emissions per capita (EPC) and agency emissions per mile (EPM)–with lower green scores meaning greener agencies. When asked about the win, TriMet’s representative proudly declared above the roaring crowd: “We do it for our riders–because they can’t take trips if they’ve died from air pollution.”"
  },
  {
    "objectID": "mp02.html#most-emissions-avoided",
    "href": "mp02.html#most-emissions-avoided",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Most Emissions Avoided",
    "text": "Most Emissions Avoided\nThe three winners of the Greenies for Most Emissions Avoided were:\n\nSmall-sized Transit Agency:\n\nHampton Jitney, Inc. from Calverton, New York, which offset an estimated \\(8,586,353\\) kg of carbon dioxide (\\(\\text{CO}_2\\)). The median value of offset emissions for small-sized agencies was \\(327,703\\) kg.\n\nMedium-sized Transit Agency:\n\nHudson Transit Lines, Inc., dba: Short Line from Mahwah, New Jersey, which offset \\(16,147,490\\) kg of \\(\\text{CO}_2\\). For comparison, the median value of offset emissions for a medium-sized agency was \\(974,179\\) kg.\n\nLarge-sized Transit Agency:\n\nMTA New York City Transit from Brooklyn, New York which offset a whopping \\(1,662,602,856\\) kg of \\(\\text{CO}_2\\)–almost 300 times the median offset emissions for a large-sized agency (\\(6,102,619\\) kg).\n\n\nAnalysts at GTA IV determined the total emissions avoided by first calculating the number of gallons of gasoline required for the newest model car to travel the same distance as each transit agency. This figure was then multiplied by the average emissions rate of motor gasoline, as defined by U.S. CAFE standards ((NHTSA), 2022).\n“People may be getting burned alive on our subways, but at least we’re stopping our riders from burning all that carbon,” said Don Vitiatus, CEO of the MTA, following Jay-Z’s heartfelt rendition of Empire State of Mind."
  },
  {
    "objectID": "mp02.html#statal-eco-leader",
    "href": "mp02.html#statal-eco-leader",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "1.3 Statal Eco-Leader",
    "text": "1.3 Statal Eco-Leader\nThree agencies were awarded Greenies for outstanding performance in their state.\n\nSmall-sized Transit Agency:\n\nOhio Valley Regional Transportation Authority from Wheeling, West Virginia, whose state’s electricity EPC is \\(15479.34\\) times larger than their EPC. To compare, the median factor of difference for small-sized agencies was \\(5,670\\).\n\nMedium-sized Transit Agency:\n\nCity of Seattle, dba: Seattle Center Monorail from Seattle, Washington, with a factor of \\(54,628\\) difference between their EPC and their state’s electricity EPC–almost 3 times the median factor of difference for medium sized-agencies (\\(19,700\\)).\n\nLarge-sized Transit Agency:\n\nCity of Portland, dba: Portland Streetcar from Portland, Oregon, whose state’s EPC is \\(18,650\\) times larger than their own. Among large-sized agencies, the median factor of difference was about \\(12,700\\).\n\n\nThis metric compares state emissions per capita with agency emissions per capita. Agencies with the largest metrics are awarded for outstanding performance compared to their state. “We will continue to take West Virginians home down country roads in a green way, and we hope our Mountain Mama will follow our lead,” said a representative from the Ohio Valley Regional Transportation Authority. While some transit agencies set the gold standard for sustainability, others lean heavily on high-emission fuels."
  },
  {
    "objectID": "mp02.html#section",
    "href": "mp02.html#section",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "2.1 ",
    "text": "2.1 \nagency with the highest percentage of their total emissions coming from the dirtiest transportation fuel, natural gas.\n\nBody Paragraphs (Detailed Information & Key Findings) Discuss the methodology: How data was analyzed (multi-table joins, data sources used)\n\nCriteria for selection (e.g., fuel efficiency, emissions reduction, ridership sustainability) List the award categories and winners (e.g., Best Overall Green Transit, Most Energy-Efficient Fleet, Lowest Carbon Emissions Per Rider). Highlight notable statistics and insights that led to each agency’s selection. Quotes from winning transit agencies or industry experts add credibility and engagement. 7. Closing Paragraph (Call to Action & Future Outlook) Mention any future initiatives or research by GTA IV. Encourage public transit agencies to adopt greener practices. Provide a link or contact for more information. 8. Boilerplate (About GTA IV) A short section about GTA IV, its mission, and its work in transit sustainability. 9. ###END### Indicates the end of the press release."
  },
  {
    "objectID": "mp02.html#insert-headline",
    "href": "mp02.html#insert-headline",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "2.1 insert headline",
    "text": "2.1 insert headline\nNatural gas, with average co2 emissions of X kg/gallon, is by far the dirtiest transportation fuel. agency with the highest percentage of their total emissions coming from the dirtiest transportation fuel, natural gas.\n\nBody Paragraphs (Detailed Information & Key Findings) Discuss the methodology: How data was analyzed (multi-table joins, data sources used)\n\nCriteria for selection (e.g., fuel efficiency, emissions reduction, ridership sustainability) List the award categories and winners (e.g., Best Overall Green Transit, Most Energy-Efficient Fleet, Lowest Carbon Emissions Per Rider). Highlight notable statistics and insights that led to each agency’s selection. Quotes from winning transit agencies or industry experts add credibility and engagement. 7. Closing Paragraph (Call to Action & Future Outlook) Mention any future initiatives or research by GTA IV. Encourage public transit agencies to adopt greener practices. Provide a link or contact for more information. 8. Boilerplate (About GTA IV) A short section about GTA IV, its mission, and its work in transit sustainability. ###END###\nDisclaimer: This press release is a fictional piece created for educational purposes. Any quotes, statements, or references to individuals, agencies, or organizations are entirely fictional and should not be interpreted as actual endorsements or statements made by real persons or entities."
  },
  {
    "objectID": "mp02.html#most-noxious",
    "href": "mp02.html#most-noxious",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Most Noxious",
    "text": "Most Noxious\nNatural gas, with average \\(\\text{CO}_2\\) emissions of \\(410,043\\) kg per gallon, is by far the dirtiest transportation fuel. GTA IV handed out Noxious ratings to the three agencies with the highest percentage of their total emissions coming from natural gas.\n\nSmall-sized Transit Agency:\n\nCity of Albany, dba: Albany Transit System from Albany, Georgia, with \\(99.99999\\%\\) of their total emissions being derived from natural gas. For comparison, the average small-sized transit agency derives \\(11.1\\%\\) of its emissions from natural gas.\n\nMedium-sized Transit Agency:\n\nBirmingham-Jefferson County Transit Authority from Birmingham, Alabama with \\(99.99993\\%\\) of their total emissions coming from natural gas. The typical medium-sized transit agency gets \\(18.6\\%\\) of its emissions from natural gas.\n\nLarge-sized Transit Agency:\n\nCounty of Nassau, dba: Nassau Inter County Express from Mineola, New York, with \\(99.99984\\%\\) of total emissions from natural gas–far exceeding the large-agency average of \\(28.7\\%\\).\n\n\nTo help recipients of the Noxious rating truly understand the impact of natural gas on the environment, GTA IV staged an unforgettable closing ceremony. A harmless yet pungent gas was released on stage, allowing awardees to experience their contribution to air pollution firsthand. “What is that smell?!” shouted a representative from Nassau Inter County Express between gags. Meanwhile, Chief Legal Officer Sue Salot from Albany Transit System threatened GTA IV with litigation.\nGTA IV announced their intention to continue funding grants for transit agencies committed to reducing their environmental impact. “We implore all of you to look deep within your operations and find ways to minimize your footprint. The world we inherit also belongs to those who come after us,” stated Lopez Luna before dropping her microphone."
  },
  {
    "objectID": "mp02.html#greenest-transit-agency-1",
    "href": "mp02.html#greenest-transit-agency-1",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "4.1 Greenest Transit Agency",
    "text": "4.1 Greenest Transit Agency\nThree winners received Greenies for Greenest Transit Agency:\n\nGreenest Small-sized Transit Agency:\n\nMecklenburg County, dba: Mecklenburg Transportation System from Portland, Oregon, with a green score of \\(0.49\\).\n\nGreenest Medium-sized Transit Agency:\n\nCity of Seattle, dba: Seattle Center Monorail from Seattle, Washington, with an impressive green score of \\(0.07\\).\n\nGreenest Large-sized Transit Agency:\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet, from Portland, Oregon, with a green score of \\(0.19\\).\n\n\nGTA IV’s green score is calculated from the geometric mean2 of agency emissions per capita (EPC) and agency emissions per mile (EPM). Lower green scores mean greener agencies."
  },
  {
    "objectID": "mp02.html#most-emissions-avoided-1",
    "href": "mp02.html#most-emissions-avoided-1",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "4.2 Most Emissions Avoided",
    "text": "4.2 Most Emissions Avoided\nThe three winners of the Greenies for Most Emissions Avoided were:\n\nSmall-sized Transit Agency:\n\nHampton Jitney, Inc. from Calverton, New York, which offset an estimated \\(8,586,353\\) kg of carbon dioxide (\\(\\text{CO}_2\\)).\n\nMedium-sized Transit Agency:\n\nHudson Transit Lines, Inc., dba: Short Line from Mahwah, New Jersey, which offset \\(16,147,490\\) kg of \\(\\text{CO}_2\\).\n\nLarge-sized Transit Agency:\n\nMTA New York City Transit from Brooklyn, New York which offset a whopping \\(1,662,602,856\\) kg of \\(\\text{CO}_2\\).\n\n\nThe total value of emissions avoided came from finding out the equivalent number of gallons of gasoline required by the newest model of car to cover the miles traveled by each agency. then multiplied by the average emissions rate of motor fuel gasoline as given in EIA co2 vol mass excel.\n\n# Compute emissions avoided and structure the data\nbubble_data &lt;- agency_modes |&gt; \n  group_by(size, `NTD ID`) |&gt;  \n  summarise(\n    Agency = first(Agency),\n    City = first(City),\n    state = first(state),\n    MILES = first(MILES),\n    emissions_avoided = first((MILES / MPG) * (co2_vol_mass |&gt; \n      filter(co2_factors == \"Finished Motor Gasoline\") |&gt; \n      pull(kg_co2)))\n  ) |&gt; \n  arrange(desc(emissions_avoided))  # Ensure sorting for annotation\n\n# Generate x and y positions using a normal distribution\nn &lt;- nrow(bubble_data)\nbubble_data$x_position &lt;- rnorm(n, mean = 0, sd = 1)  # Centered around 0\nbubble_data$y_position &lt;- rnorm(n, mean = 5, sd = 15)  # Spread around 5\n\n# Normalize sizes and colors within each size category\nbubble_data &lt;- bubble_data |&gt; \n  group_by(size) |&gt; \n  mutate(\n    size_scaled = scales::rescale(emissions_avoided, to = c(5, 20)),  # Normalize size\n    color_scaled = scales::rescale(emissions_avoided, to = c(0, 1))  # Normalize color\n  ) |&gt; \n  ungroup()\n\n# Find the largest emissions_avoided in each size category\nlargest_per_size &lt;- bubble_data |&gt; \n  group_by(size) |&gt; \n  slice_max(order_by = emissions_avoided, n = 1, with_ties = FALSE)  # Select the max per group\n\n# Create the plot\nggplot(bubble_data, aes(x = x_position, y = y_position, size = size_scaled, color = color_scaled)) +\n  geom_point(alpha = 0.5) +  # Transparent bubbles\n  scale_size_continuous(range = c(5, 20)) +  # Adjust bubble sizes\n  scale_color_gradient(low = \"gray60\", high = MINTY) +  # Uses already-defined MINTY\n  labs(\n    title = \"Emissions Avoided by Transit Agencies\",\n    x = \"\",\n    y = \"\",\n    size = \"Emissions Avoided (kg CO2)\"\n  ) +\n  theme_minimal() +\n\n  # Label only the largest agency per size category\n  geom_text(\n    data = largest_per_size,  # Use pre-filtered top agencies\n    aes(label = str_wrap(Agency, width = 15)),\n    vjust = -1, size = 3, fontface = \"bold\", color = \"black\"\n  ) +\n\n  # Remove legends and grid\n  theme(\n    legend.position = \"none\",  # Removes legend\n    panel.grid = element_blank(),  # Removes grid\n    axis.text.x = element_blank(),  # Hide x-axis labels\n    axis.ticks.x = element_blank(),  # Remove x-axis ticks\n    axis.text.y = element_blank(),  # Hide y-axis labels\n    axis.ticks.y = element_blank()  # Remove y-axis ticks\n  ) +\n\n  # Facet by size category\n  facet_wrap(~size, scales = \"free\")"
  },
  {
    "objectID": "mp02.html#data-sources",
    "href": "mp02.html#data-sources",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "3.1 Data Sources",
    "text": "3.1 Data Sources\nTransit Data: Retrieved from the National Transit Database (Federal Transit Administration, 2023) and the U.S. Department of Transportation Open Data Portal (U.S. Department of Transportation, 2023).\nEmission Factors: Based on values from the U.S. Energy Information Administration (U.S. Energy Information Administration, 2023),(U.S. Energy Information Administration, 2024).\nState Population Data: Based on U.S. Census Data (U.S. Census Bureau, 2024).\nComparative Metrics: Calculated using R and R packages [].\nB. Calculation Methodologies 1. Green Score Calculation Green Score = Emissions per Capita × Emissions per Mile Green Score= Emissions per Capita×Emissions per Mile ​\nLower scores indicate more environmentally friendly agencies.\nMedian green score by agency size:\nSmall: X.XX\nMedium: X.XX\nLarge: X.XX\n\nEmissions Avoided Avoided Emissions = ( Passenger Miles × Avg Car Emissions ) − Agency Emissions Avoided Emissions=(Passenger Miles×Avg Car Emissions)−Agency Emissions Used average gasoline vehicle efficiency as the baseline.\nStatal Eco-Leader Metric State Factor = State’s Emissions per Capita Agency’s Emissions per Capita State Factor= Agency’s Emissions per Capita State’s Emissions per Capita ​\n\nHighlights agencies outperforming their state in environmental efficiency.\n\nMost Noxious Rating Agencies ranked by percentage of emissions from natural gas.\n\nMedian natural gas reliance by agency size:\nSmall: X.XX%\nMedium: X.XX%\nLarge: X.XX%\nC. Data Processing & Code All calculations performed in R using dplyr, ggplot2, and tidyverse.\nThe full dataset and scripts are available at [GitHub link or appendix section]. 🔲 Add at least two visualizations with clear labels, a legend, and citations. 🔲 Cite original data sources explicitly in both the text and visual captions. 🔲 Include an Appendix summarizing analysis methods and code (if required).\n\nDisclaimer: This press release is a fictional piece created for educational purposes. Any quotes, statements, or references to individuals, agencies, or organizations are entirely fictional and should not be interpreted as actual endorsements or statements made by real persons or entities."
  },
  {
    "objectID": "mp02.html#data-sources-and-retrieval",
    "href": "mp02.html#data-sources-and-retrieval",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Data Sources and Retrieval",
    "text": "Data Sources and Retrieval\nTransit data were obtained from the National Transit Database (Federal Transit Administration, 2023) and the U.S. Department of Transportation Open Data Portal (U.S. Department of Transportation, 2023). Emission factors were derived from the U.S. Energy Information Administration (U.S. Energy Information Administration, 2023, 2024), while state population data were sourced from the U.S. Census (U.S. Census Bureau, 2024).\nComparative metrics were calculated using R, leveraging packages including dplyr, ggplot2, readr, stringr, lubridate, forcats, ggbeeswarm, readxl, DT, and knitr (Clarke & Sherrill-Mix, 2023; Grolemund & Wickham, 2023; Team, 2024; Wickham, 2016, 2023, 2024; Wickham et al., 2024; Wickham & Bryan, 2023; Wickham & Hester, 2024; Xie et al., 2024; Xie, 2024).\n\n\nCode\n# More library imports\nlibrary(stringr)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(forcats)\nlibrary(lubridate)\nlibrary(ggbeeswarm)\n\n# Instructor provided code for downloading and tidying up data.\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\n\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n               # * 1000 kWh to MWH \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nensure_package(scales)\nensure_package(DT)\n\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n    replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\n\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n    mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n    rename(Agency = agency, \n           City   = max_city, \n           State  = max_state,\n           UPT    = sum_unlinked_passenger_trips_upt, \n           MILES  = sum_passenger_miles) |&gt;\n    select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n    filter(MILES &gt; 0)\n\n# Read the Excel file containing fuel co2 emissions info...\nco2_vol_mass &lt;- read_excel(\"co2_vol_mass.xlsx\")\n\n# Read the Excel file containing population data...\nNST_EST_POP &lt;- read_excel(\"NST-EST2024-POP.xlsx\")\n\n\n\nData Processing & Cleaning\nThe data sets were cleaned up as in the code below:\n\n\nCode\n# Constants\nMPG &lt;- 49 # standard vehicle fuel economy for 2026.\nMCF_TO_GALLONS &lt;- 7.48052 * 1000 # conversion factor for metric cubic feet to gallons.\nMINTY &lt;- \"#78c2ad\"\n# Facet labels\nFACET_BY_SIZE_LABELS &lt;- c(\n  \"Small\" = \"Small-Sized\\nTransit Agencies\",\n  \"Medium\" = \"Medium-Sized\\nTransit Agencies\",\n  \"Large\" = \"Large-Sized\\nTransit Agencies\"\n)\n\n# clean up mode entries in ntd energy\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n    mutate(Mode=case_when(\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"AR\" ~ \"Alaska Railroad\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Bus\",\n        Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail\",\n        Mode == \"TB\" ~ \"Trolleybus\",\n        Mode == \"TR\" ~ \"Aerial Tramways\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"))\n\n# clean up fuel co2 emissions data.\nco2_vol_mass &lt;-\n  co2_vol_mass |&gt;\n  rename(\n    co2_factors = \"Carbon Dioxide Emissions Coefficients by Fuel\",\n    pounds_co2 = \"...2\",\n    pounds_co2_per_unit = \"...3\",\n    kg_co2 = \"...4\",\n    kg_co2_per_unit  = \"...5\",\n    pounds_co2_per_mil_btu = \"...6\",\n    kg_co2_per_mil_btu = \"...7\"\n  ) |&gt;\n  filter(!is.na(pounds_co2),\n         !is.na(pounds_co2_per_unit),\n         !is.na(kg_co2),\n         !is.na(kg_co2_per_unit),\n         !is.na(pounds_co2_per_mil_btu),\n         !is.na(kg_co2_per_mil_btu)) |&gt;\n  mutate(\n    pounds_co2 = as.numeric(pounds_co2),\n    kg_co2 = as.numeric(kg_co2),\n    pounds_co2_per_mil_btu = as.numeric(pounds_co2_per_mil_btu),\n    kg_co2_per_mil_btu = as.numeric(kg_co2_per_mil_btu),\n    )\n\n# clean up state population data\nNST_EST_POP &lt;-\n  NST_EST_POP |&gt;\n  rename(\n    state = \"table with row headers in column A and column headers in rows 3 through 4. (leading dots indicate sub-parts)\",\n    population = \"...7\",\n  ) |&gt;\n  select(state,population) |&gt;\n  mutate(state = str_remove(state, \"^\\\\.+\")) |&gt;\n  filter(!is.na(population),\n         !is.na(state))"
  },
  {
    "objectID": "mp02.html#analysis-of-eia-state-electricity-data-set",
    "href": "mp02.html#analysis-of-eia-state-electricity-data-set",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Analysis of EIA State Electricity Data Set",
    "text": "Analysis of EIA State Electricity Data Set\nThe code below details the analysis of the EIA State Electricity dataset.\n\n\nCode\n# Which state has the most expensive retail electricity?\nmost_expensive_state &lt;- EIA_SEP_REPORT |&gt;\n  slice_max(electricity_price_MWh, n = 1) |&gt;\n  select(!abbreviation) |&gt;\n  rename(\n    `Pounds of CO$_2$ Emitted per MWh of Electricity Produced` = CO2_MWh, \n    `Primary Source of Electricity Generation` = primary_source,\n    `Average Retail Price for 1000 kWh` = electricity_price_MWh,\n    `Total Generation Capacity (MWh)` = generation_MWh,\n    State = state\n  ) |&gt;\n  mutate(\n    `Pounds of CO$_2$ Emitted per MWh of Electricity Produced` = comma(`Pounds of CO$_2$ Emitted per MWh of Electricity Produced`),\n    `Total Generation Capacity (MWh)` = comma(`Total Generation Capacity (MWh)`),\n    `Average Retail Price for 1000 kWh` = dollar(`Average Retail Price for 1000 kWh`, accuracy = 0.01)\n  )\nkable(most_expensive_state, caption = \"State with the Most Expensive Retail Electricity\")\n\n\n\nState with the Most Expensive Retail Electricity\n\n\n\n\n\n\n\n\n\nPounds of CO\\(_2\\) Emitted per MWh of Electricity Produced\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nTotal Generation Capacity (MWh)\nState\n\n\n\n\n1,444\nPetroleum\n$386.00\n9,194,164\nHawaii\n\n\n\n\n\nCode\n# Which state has the ‘dirtiest’ electricity mix?\ndirtiest_state &lt;- EIA_SEP_REPORT |&gt;\n  slice_max(CO2_MWh, n = 1) |&gt;\n  select(!abbreviation) |&gt;\n  rename(\n    `State` = state,\n    `Pounds of CO$_2$ Emitted per MWh of Electricity Produced` = CO2_MWh,\n    `Primary Source of Electricity Generation` = primary_source,\n    `Total Generation Capacity (MWh)` = generation_MWh,\n    `Average Retail Price for 1000 kWh` = electricity_price_MWh\n  ) |&gt;\n  mutate(\n    `Pounds of CO$_2$ Emitted per MWh of Electricity Produced` = comma(`Pounds of CO$_2$ Emitted per MWh of Electricity Produced`),\n    `Total Generation Capacity (MWh)` = comma(`Total Generation Capacity (MWh)`),\n    `Average Retail Price for 1000 kWh` = dollar(`Average Retail Price for 1000 kWh`, accuracy = 0.01)\n  )\n\nkable(dirtiest_state, caption = \"State with the Dirtiest Electricity Mix\")\n\n\n\nState with the Dirtiest Electricity Mix\n\n\n\n\n\n\n\n\n\nPounds of CO\\(_2\\) Emitted per MWh of Electricity Produced\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nTotal Generation Capacity (MWh)\nState\n\n\n\n\n1,925\nCoal\n$102.60\n52,286,784\nWest Virginia\n\n\n\n\n\nCode\n# On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US?\nweighted_avg_co2 &lt;- EIA_SEP_REPORT |&gt;\n  summarize(\n    `Weighted Average CO$_2$ Emissions (lbs/MWh)` = sum(CO2_MWh * generation_MWh) / sum(generation_MWh)\n  ) |&gt;\n  mutate(`Weighted Average CO$_2$ Emissions (lbs/MWh)` = comma(`Weighted Average CO$_2$ Emissions (lbs/MWh)`))\n\nkable(weighted_avg_co2, caption = \"Weighted Average CO2 Emissions per MWh in the US\")\n\n\n\nWeighted Average CO2 Emissions per MWh in the US\n\n\nWeighted Average CO\\(_2\\) Emissions (lbs/MWh)\n\n\n\n\n805\n\n\n\n\n\nCode\n# What is the rarest primary energy source in the US?\nrarest_source &lt;- EIA_SEP_REPORT |&gt;\n  group_by(primary_source) |&gt;\n  summarize(`Number of States Using This Source` = n()) |&gt;\n  slice_min(`Number of States Using This Source`, n = 1) |&gt;\n  pull(primary_source)\n\nrarest_source_data &lt;- EIA_SEP_REPORT |&gt;\n  filter(primary_source == rarest_source) |&gt;\n  select(primary_source, electricity_price_MWh, state) |&gt;\n  rename(\n    `Primary Source of Electricity Generation` = primary_source,\n    `Average Retail Price for 1000 kWh` = electricity_price_MWh,\n    `State` = state\n  ) |&gt;\n  mutate(\n    `Average Retail Price for 1000 kWh` = dollar(`Average Retail Price for 1000 kWh`, accuracy = 0.01)\n  )\n\nkable(rarest_source_data, caption = \"Rarest Primary Energy Source and Its Associated Cost\")\n\n\n\nRarest Primary Energy Source and Its Associated Cost\n\n\n\n\n\n\n\nPrimary Source of Electricity Generation\nAverage Retail Price for 1000 kWh\nState\n\n\n\n\nPetroleum\n$386.00\nHawaii\n\n\n\n\n\nCode\n# How many times cleaner is NY’s energy mix than Texas'?\ntx_co2 &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  pull(CO2_MWh)\n\nny_co2 &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  pull(CO2_MWh)\n\ncleaner_ratio &lt;- data.frame(\n  `State` = c(\"Texas\", \"New York\"),\n  `Pounds of CO$_2$ Emitted per MWh of Electricity Produced` = c(tx_co2, ny_co2),\n  `Times Cleaner (TX/NY)` = c(NA, round(tx_co2 / ny_co2, 2))\n) |&gt;\n  rename(\n    \"Pounds of CO$_2$ Emitted per MWh of Electricity Produced\" = \"Pounds.of.CO._2..Emitted.per.MWh.of.Electricity.Produced\",\n    `Times Cleaner (TX/NY)` = \"Times.Cleaner..TX.NY.\"\n    ) |&gt;\n  mutate(\n    `Pounds of CO$_2$ Emitted per MWh of Electricity Produced` = comma(`Pounds of CO$_2$ Emitted per MWh of Electricity Produced`)\n  )\n\nkable(cleaner_ratio, caption = \"Comparison of CO2 Emissions Between Texas and New York\")\n\n\n\nComparison of CO2 Emissions Between Texas and New York\n\n\n\n\n\n\n\nState\nPounds of CO\\(_2\\) Emitted per MWh of Electricity Produced\nTimes Cleaner (TX/NY)\n\n\n\n\nTexas\n855\nNA\n\n\nNew York\n522\n1.64"
  },
  {
    "objectID": "mp02.html#analysis-of-ntd-service-data-set",
    "href": "mp02.html#analysis-of-ntd-service-data-set",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Analysis of NTD Service Data Set",
    "text": "Analysis of NTD Service Data Set\nThe code below details the analysis of the NTD Service dataset.\n\n\nCode\n# Which transit service has the most UPT annually?\nmost_upt &lt;- NTD_SERVICE |&gt;\n  select(`NTD ID`, Agency, City, State, UPT) |&gt;\n  mutate(UPT = comma(UPT)) |&gt;\n  slice_max(UPT, n = 1)\n\nkable(most_upt, caption = \"Transit Service with the Most UPT Annually\")\n\n\n\nTransit Service with the Most UPT Annually\n\n\nNTD ID\nAgency\nCity\nState\nUPT\n\n\n\n\n50058\nRockford Mass Transit District\nRockford\nIL\n994,754\n\n\n\n\n\nCode\n# What is the average trip length of a trip on MTA NYC?\navg_mta_trip &lt;- NTD_SERVICE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  mutate(`Average Trip Length (miles)` = comma(MILES / UPT)) |&gt;\n  select(`NTD ID`, Agency, City, State, `Average Trip Length (miles)`)\n\nkable(avg_mta_trip, caption = \"Average Trip Length on MTA NYC Transit\")\n\n\n\nAverage Trip Length on MTA NYC Transit\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nCity\nState\nAverage Trip Length (miles)\n\n\n\n\n20008\nMTA New York City Transit\nBrooklyn\nNY\n4\n\n\n\n\n\nCode\n# Which transit service in NYC has the longest average trip length?\nnyc_longest_avg &lt;- NTD_SERVICE |&gt;\n  filter(State == \"NY\", City == \"New York\" | City == \"Brooklyn\") |&gt;\n  mutate(`Average Trip Length (miles)` = MILES / UPT) |&gt;\n  slice_max(`Average Trip Length (miles)`, n = 1) |&gt;\n  select(`NTD ID`, Agency, City, State, `Average Trip Length (miles)`)\n\nkable(nyc_longest_avg, caption = \"NYC Transit Service with the Longest Average Trip Length\")\n\n\n\nNYC Transit Service with the Longest Average Trip Length\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency\nCity\nState\nAverage Trip Length (miles)\n\n\n\n\n20100\nMTA Long Island Rail Road\nNew York\nNY\n24.25799\n\n\n\n\n\nCode\n# Which state has the fewest total miles traveled by public transit?\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(`Total Miles` = comma(sum(MILES, na.rm = TRUE))) |&gt;\n  slice_min(`Total Miles`, n = 1)\n\nkable(state_fewest_miles, caption = \"State with the Fewest Total Miles Traveled by Public Transit\")\n\n\n\nState with the Fewest Total Miles Traveled by Public Transit\n\n\nState\nTotal Miles\n\n\n\n\nWA\n1,059,910,614\n\n\n\n\n\nCode\n# Are all states represented in this data? If no, which ones are missing?\nmissing_states &lt;- EIA_SEP_REPORT |&gt;\n  anti_join(NTD_SERVICE, join_by(\"abbreviation\" == \"State\")) |&gt;\n  rename(`Missing States` = state) |&gt;\n  select(`Missing States`)\n\nkable(missing_states, caption = \"States Missing from the Transit Data\")\n\n\n\nStates Missing from the Transit Data\n\n\nMissing States\n\n\n\n\nArizona\n\n\nArkansas\n\n\nCalifornia\n\n\nColorado\n\n\nHawaii\n\n\nIowa\n\n\nKansas\n\n\nLouisiana\n\n\nMissouri\n\n\nMontana\n\n\nNebraska\n\n\nNevada\n\n\nNew Mexico\n\n\nNorth Dakota\n\n\nOklahoma\n\n\nSouth Dakota\n\n\nTexas\n\n\nUtah\n\n\nWyoming"
  },
  {
    "objectID": "mp02.html#merging-of-data-sets",
    "href": "mp02.html#merging-of-data-sets",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Merging of Data Sets",
    "text": "Merging of Data Sets\nThe following process was used to merge the primary datasets:\n\n\nCode\n# task 5: table merge\nagency_modes &lt;- \n  NTD_SERVICE |&gt;\n  inner_join(\n    NTD_ENERGY |&gt;\n      select(-c(`Kerosene`, `Bunker Fuel`, `Ethanol`, `Methonal`)), # these columns have 0 values. \n    join_by(\"NTD ID\" == \"NTD ID\")\n    ) |&gt; \n  inner_join(\n    EIA_SEP_REPORT,\n    join_by(State == \"abbreviation\")\n    ) |&gt;\n  select(-c(`Agency Name`, State)) |&gt;\n  rename(\n    agency = `Agency`,\n    city = `City`,\n    mode = `Mode`,\n    upt = `UPT`,\n    miles = `MILES`,\n    ntd_id = `NTD ID`,\n    biodiesel = `Bio-Diesel`,\n    cnaturalgas = `C Natural Gas`,\n    diesel = `Diesel Fuel`,\n    gasoline = `Gasoline`,\n    liqnatgas = `Liquified Nat Gas`,\n    liqpetgas =`Liquified Petroleum Gas`,\n    electric_battery = `Electric Battery`,\n    electric_propulsion = `Electric Propulsion`,\n    co2_mwh = CO2_MWh\n    ) |&gt;\n  mutate(\n    biodiesel_emissions = biodiesel * (co2_vol_mass |&gt; \n                                         filter(co2_factors == \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\") |&gt;\n                                         pull(kg_co2)),\n    cnaturalgas_emissions = cnaturalgas * (co2_vol_mass |&gt;\n                                             filter(co2_factors == \"Natural Gas\") |&gt;\n                                             pull(kg_co2) * MCF_TO_GALLONS),\n    diesel_emissions = diesel * (co2_vol_mass |&gt;\n                                   filter(co2_factors == \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\") |&gt;\n                                   pull(kg_co2)),\n    gasoline_emissions = gasoline * (co2_vol_mass |&gt;\n                                       filter(co2_factors == \"Finished Motor Gasoline\") |&gt;\n                                       pull(kg_co2)),\n    liqnatgas_emissions = liqnatgas * (co2_vol_mass |&gt;\n                                         filter(co2_factors == \"Natural Gas\") |&gt;\n                                         pull(kg_co2) * MCF_TO_GALLONS),\n    liqpetgas_emissions = liqpetgas * (co2_vol_mass |&gt;\n                                         filter(co2_factors == \"Propane\") |&gt;\n                                         pull(kg_co2)),\n    electricbattery_emissions = electric_battery * (co2_mwh / 1000),\n    electricprop_emissions = electric_propulsion * (co2_mwh / 1000),\n    total_emissions_of_mode = (\n      biodiesel_emissions +\n        cnaturalgas_emissions +\n        diesel_emissions +\n        gasoline_emissions +\n        liqnatgas_emissions +\n        liqpetgas_emissions +\n        electricbattery_emissions +\n        electricprop_emissions\n      )\n  ) |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(total_emissions_of_agency = sum(total_emissions_of_mode)) |&gt;\n  ungroup()\n\n# task 6\n# finding the percentile values that will define small, medium, and large sized agencies. \nPERCENTILE_33 &lt;- quantile(agency_modes |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.33)\nPERCENTILE_66 &lt;- quantile(agency_modes |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.66)\n\n# updating agency modes with agency size categorical value + adding green score metric\nagency_modes &lt;-\n  agency_modes |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(\n    agency_emissions_per_capita = sum(total_emissions_of_mode) / upt,\n    emissions_per_transit = sum(total_emissions_of_mode) / miles,\n    green_score = round(sqrt(agency_emissions_per_capita * emissions_per_transit),2),\n    size = case_when(\n      upt &lt; PERCENTILE_33 ~ \"Small\",\n      upt &gt;= PERCENTILE_33 & upt &lt;= PERCENTILE_66 ~ \"Medium\",\n      upt &gt; PERCENTILE_66 ~ \"Large\",\n    )\n    ) |&gt;\n  ungroup()"
  },
  {
    "objectID": "mp02.html#calculation-methodologies",
    "href": "mp02.html#calculation-methodologies",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Calculation Methodologies",
    "text": "Calculation Methodologies\nThis section provides a brief summary of the key metrics used in the analysis and the code used to identify the most–and least–environmentally friendly transit agencies.\n\nGreenest Transit Agency\nGTA IV calculates the green score as,\n\\[\\text{Green Score} = \\sqrt{\\text{Emissions per Capita} \\cdot \\text{Emissions per Mile}}\\] Lower scores indicate more environmentally friendly agencies. The table below presents the small-, medium-, and large-sized transit agencies with the lowest green score.\n\n\nCode\n# grouping for ease of use\ngreen_scores &lt;-\n  agency_modes |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarize(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    green_score = first(green_score),\n    .groups = 'drop'\n  ) |&gt;\n  ungroup()\n\n# Select the greenest agency per size category\ngreenest_agencies &lt;-\n  green_scores |&gt;\n  group_by(size) |&gt;\n  slice_min(green_score, n = 1) |&gt;\n  select(Agency = agency, City = city, State = state, `Green Score` = green_score)\n\n# Compute median green score per size category\nmedian_green &lt;- green_scores |&gt;\n  group_by(size) |&gt;\n  summarise(median_green = median(green_score, na.rm = TRUE))\n\n# Display table of greenest agencies with nice formatting\ngreenest_agencies |&gt;\n  left_join(median_green, join_by(\"size\" == \"size\")) |&gt;\n  rename(\n    \"Agency Size\" = size,\n    \"Median Green Score\" = median_green\n  ) |&gt;\n  kable(caption = \"**Greenest Transit Agency in Each Size Category**\")\n\n\n\nGreenest Transit Agency in Each Size Category\n\n\n\n\n\n\n\n\n\n\nAgency Size\nAgency\nCity\nState\nGreen Score\nMedian Green Score\n\n\n\n\nLarge\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet\nPortland\nOregon\n0.19\n1.230\n\n\nMedium\nCity of Seattle, dba: Seattle Center Monorail\nSeattle\nWashington\n0.07\n1.410\n\n\nSmall\nMecklenburg County , dba: Mecklenburg Transportation System\nCharlotte\nNorth Carolina\n0.49\n2.315\n\n\n\n\n\nThe figure below presents the top three greenest transit agencies in each size category.\n\n\nCode\n# create tibble with data that will go on plot\ngreen_agencies_plot &lt;-\n  agency_modes |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarize(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    green_score = first(green_score)\n  ) |&gt;\n  slice_min(green_score, n = 3, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  left_join(median_green, join_by(\"size\" == \"size\"))\n\n# Fix agency names, and fix case for size\ngreen_agencies_plot &lt;- green_agencies_plot |&gt;\n  mutate(agency = str_replace_all(agency, \"-\", \" \")) |&gt;\n  mutate(\n    agency = case_when(\n      str_detect(agency, \", dba:\") == TRUE ~ str_wrap(str_extract(agency, \"(?&lt;=, dba: ).*\"), width = 12),\n      TRUE ~ str_wrap(agency, width = 12)\n    )\n  ) |&gt;\n  group_by(agency) |&gt;\n  mutate(\n    agency = case_when(\n      n() &gt; 1 ~ str_wrap(paste0(agency, \" (\", city, \")\"), width = 12),\n      TRUE ~ agency\n      )\n    ) |&gt;\n  ungroup()\n\n# plot code\nggplot(green_agencies_plot, aes(y = fct_reorder(agency, -green_score), x = green_score)) +\n  geom_col(fill = MINTY) +\n  geom_vline(data = median_green, aes(xintercept = median_green, linetype = \"Median\"),\n             color = MINTY, linewidth = 1) +\n  labs(\n    x = \"Green Score\",\n    y = \"Agency Name\",\n    title = \"3 Greenest Agencies By Size\",\n    linetype = \"Legend\",\n    caption = str_wrap(\"Data sources: National Transit Database (NTD) Service and Energy Use datasets, and the U.S. Energy Information Administration (EIA) State Energy Profiles (SEP).\", width = 90)\n  ) +\n  scale_linetype_manual(values = c(\"Median\" = \"dashed\")) +\n  scale_x_continuous(breaks = seq(0, 2, by = 0.5)) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  facet_wrap(~size, scales = \"free_y\", labeller = labeller(size = FACET_BY_SIZE_LABELS))\n\n\n\n\n\n\n\n\n\n\n\nEmissions Avoided\nGTA IV found the emissions avoided using,\n\\[\\text{Avoided Emissions} = (\\text{Passenger Miles}\\cdot\\text{Avg Car Emissions}) - \\text{Agency Emissions}\\]\nAverage gasoline vehicle efficiency was used as the baseline ((NHTSA), 2022). The table below presents the small-, medium-, and large-sized agencies with the largest mass of emissions avoided. The median value of avoided emissions is also provided for each agency size.\n\n\nCode\n# Compute emissions avoided per agency\nemissions_avoided &lt;- agency_modes |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarise(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    emissions_avoided = first((miles / MPG) * \n      co2_vol_mass |&gt; filter(co2_factors == \"Finished Motor Gasoline\") |&gt; pull(kg_co2))\n  ) |&gt;  \n  slice_max(emissions_avoided, n = 1) # Select agency with highest emissions avoided per size\n\n# Compute emissions avoided for all agencies, sorted\nbubble_data &lt;- agency_modes |&gt;\n  group_by(size, ntd_id) |&gt;\n  summarise(\n    agency = first(agency),\n    city = first(city),\n    state = first(state),\n    miles = first(miles),\n    emissions_avoided = first((miles / MPG) * \n      co2_vol_mass |&gt; filter(co2_factors == \"Finished Motor Gasoline\") |&gt; pull(kg_co2))\n  ) |&gt;  \n  arrange(desc(emissions_avoided))\n\n# Calculate median emissions avoided per size\nmedian_emissions_avoided &lt;- bubble_data |&gt;\n  group_by(size) |&gt;\n  summarize(median_emissions = median(emissions_avoided, na.rm = TRUE))\n\nemissions_avoided |&gt;\n  left_join(median_emissions_avoided, join_by(size == size)) |&gt;\n  rename(`Agency` = agency,`Size` = size, `NTD ID` = ntd_id, `City` = city, `State` = state, `Emissions Avoided (kg CO2)` = emissions_avoided, `Median Emissions Avoided` = median_emissions) |&gt;\n  mutate(`NTD ID` = as.character(`NTD ID`)) |&gt;\n  kable(digits = 2, format.args = list(big.mark = \",\"), caption = \"Top Agencies by Emissions Avoided (kg CO2)\")\n\n\n\nTop Agencies by Emissions Avoided (kg CO2)\n\n\n\n\n\n\n\n\n\n\n\nSize\nNTD ID\nAgency\nCity\nState\nEmissions Avoided (kg CO2)\nMedian Emissions Avoided\n\n\n\n\nLarge\n20008\nMTA New York City Transit\nBrooklyn\nNew York\n1,662,602,856\n6,102,619.2\n\n\nMedium\n20126\nHudson Transit Lines, Inc., dba: Short Line\nMahwah\nNew Jersey\n16,147,490\n974,179.2\n\n\nSmall\n20217\nHampton Jitney, Inc.\nCalverton\nNew York\n8,586,353\n327,702.6\n\n\n\n\n\nThe figure below presents the emissions avoided by different sized transit agencies.\n\n\nCode\n# Normalize sizes and colors within each size category\nbubble_data &lt;- bubble_data |&gt;\n  group_by(size) |&gt;\n  mutate(\n    size_scaled = scales::rescale(emissions_avoided, to = c(5, 20)),\n    color_scaled = scales::rescale(emissions_avoided, to = c(0, 1))\n  ) |&gt;\n  ungroup() |&gt;\n  arrange(emissions_avoided)\n\n# Plot emissions avoided by transit agencies\nggplot(bubble_data, aes(x = size, y = emissions_avoided, size = size_scaled, color = color_scaled)) +\n  geom_quasirandom(alpha = 0.9, width = 0.3) +\n  scale_y_log10() +\n  coord_cartesian(ylim = c(min(bubble_data$emissions_avoided) * 0.8, max(bubble_data$emissions_avoided) * 5)) +\n  scale_size_continuous(range = c(5, 20)) +\n  scale_color_gradient(low = \"gray75\", high = MINTY) +\n  labs(\n    title = \"Emissions Avoided by Transit Agencies\",\n    x = \"\",\n    y = \"Emissions Avoided (kg CO2, Log Scale)\",\n    caption = str_wrap(\"Data sources: National Transit Database (NTD) Service and Energy Use datasets, and the U.S. Energy Information Administration (EIA) State Energy Profiles (SEP). Baseline MPG from National Highway Traffic Safety Administration (NHTSA).\", width = 90)\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank()\n  ) +\n  facet_wrap(~ size, scales = \"free_x\", labeller = labeller(size = FACET_BY_SIZE_LABELS))\n\n\n\n\n\n\n\n\n\n\n\nGreen Giant\nGTA IV identified the Statal Eco-Leader using the following metric:\n\\[\\text{State Factor} = \\frac{\\text{State’s Emissions per Capita}}{\\text{Agency’s Emissions per Capita}}\\]\nThis metric highlights agencies that achieve greater environmental efficiency compared to the average emissions per capita in their respective states.\nThe table below shows agencies with highest state-to-agency emissions ratio. The higher the ratio, the better the agency’s performance.\n\n\nCode\n# Compute state emissions per capita and ratio to agency emissions\nstatal_scores &lt;- agency_modes |&gt; \n  inner_join(NST_EST_POP, join_by(state == state)) |&gt;\n  mutate(\n    state_emissions_per_capita = (co2_mwh * generation_MWh) / population,\n    ratio_in_k = state_emissions_per_capita / (agency_emissions_per_capita * 10^3)\n  ) |&gt; \n  distinct(ntd_id, .keep_all = TRUE)\n\n# Get the top agency with the highest ratio per size category\ntop_statal &lt;- statal_scores |&gt; \n  group_by(size) |&gt; \n  slice_max(ratio_in_k, n = 1, with_ties = FALSE) |&gt;  \n  ungroup()\n\n# Get the top 5 agencies per size category\ntop_5 &lt;- statal_scores |&gt; \n  group_by(size) |&gt; \n  slice_max(ratio_in_k, n = 5) |&gt;  \n  ungroup()\n\n# Calculate the median ratio for each size category\nmedian_ratios &lt;- top_5 |&gt; \n  group_by(size) |&gt; \n  summarise(median_ratio = median(ratio_in_k))  \n\n# Format agency names for better readability\ntop_5 &lt;- \n  top_5 |&gt; \n  mutate(\n    agency = case_when(\n      str_detect(agency, \", dba:\") == TRUE ~ str_wrap(str_extract(agency, \"(?&lt;=, dba: ).*\"), width = 15),\n      TRUE ~ str_wrap(agency, width = 15)\n    )\n  ) |&gt;\n  group_by(agency) |&gt;\n  mutate(\n    agency = case_when(\n      n() &gt; 1 ~ str_wrap(paste0(agency, \" (\", city, \")\"), width = 15),\n      TRUE ~ agency\n      )\n    ) |&gt;\n  ungroup()\n\n# table of agencies with highest state to agency emissions ratio. \ntop_statal |&gt; \n  select(size, agency,city,state,ratio_in_k) |&gt;\n  left_join(median_ratios, join_by(size==size)) |&gt;\n  mutate(\n    `State-to-Agency Ratio` = ratio_in_k * (10^3),\n    `Median State-to-Agency Ratio` = median_ratio * 10^3\n    ) |&gt;\n  select(-c(ratio_in_k, median_ratio)) |&gt;\n  rename(\n    `Size` = size,\n    `Agency` = agency, \n    `City` = city, \n    `State` = state, \n  ) |&gt; \n  kable(caption = \"Agencies with the Highest State-to-Agency Emissions Ratio\")\n\n\n\nAgencies with the Highest State-to-Agency Emissions Ratio\n\n\n\n\n\n\n\n\n\n\nSize\nAgency\nCity\nState\nState-to-Agency Ratio\nMedian State-to-Agency Ratio\n\n\n\n\nLarge\nCity of Portland, dba: Portland Streetcar\nPortland\nOregon\n18649.95\n12732.983\n\n\nMedium\nCity of Seattle, dba: Seattle Center Monorail\nSeattle\nWashington\n54627.65\n19714.104\n\n\nSmall\nOhio Valley Regional Transportation Authority\nWheeling\nWest Virginia\n15479.34\n5668.236\n\n\n\n\n\nCode\ntop_statal &lt;- \n  top_statal |&gt; \n  mutate(\n    agency = case_when(\n      str_detect(agency, \", dba:\") == TRUE ~ str_wrap(str_extract(agency, \"(?&lt;=, dba: ).*\"), width = 15),\n      TRUE ~ str_wrap(agency, width = 15)\n    )\n  ) |&gt;\n  ungroup()\n\n\nThe plot below presents the top 5 agencies outperforming their states in emissions per capita.\n\n\nCode\n# Create the lollipop plot\nggplot(top_5, aes(x = reorder(agency, ratio_in_k), y = ratio_in_k)) +\n  geom_segment(aes(xend = agency, y = 0, yend = ratio_in_k), color = \"gray70\") +\n  geom_point(color = \"gray50\", size = 3) +\n  geom_point(data = top_statal, aes(y = ratio_in_k), color = MINTY, size = 5) + \n  geom_hline(data = median_ratios, aes(yintercept = median_ratio, linetype = \"Median\"),\n             color = MINTY, linewidth = 1) +\n  coord_flip() +\n  facet_wrap(~size, scales = \"free\", labeller = labeller(size = FACET_BY_SIZE_LABELS)) +\n  scale_y_continuous(expand = expansion(mult = 0.15)) +\n  scale_linetype_manual(name = NULL, values = c(\"Median\" = \"dashed\")) +\n  labs(\n    title = \"Top 5 Agencies: State vs. Agency Emissions Per Capita\",\n    x = \"Agency\",\n    y = \"State-to-Agency Emissions Ratio, in Thousands\",\n    caption = str_wrap(\"Data sources: National Transit Database (NTD) Service and Energy Use datasets, the U.S. Energy Information Administration (EIA) State Energy Profiles (SEP), and 2020 U.S. Census Population Estimates.\")\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nNoxious Rating\nSince natural gas was the “dirtiest” fuel type in this data set, GTA IV identified agencies deserving of the Noxious Rating as those with the highest percentage of their total emissions coming from natural gas. The table below presents the small-, medium-, and large-sized agencies with the highest proportion of emissions from natural gas.\n\n\nCode\n# Compute percentage of emissions from natural gas per agency\nnoxious_scores &lt;- agency_modes |&gt; \n  group_by(size, ntd_id) |&gt;  \n  summarise(\n    agency = first(agency),\n    state = first(state),\n    city = first(city),\n    total_natural_gas_of_agency = sum(cnaturalgas_emissions + liqnatgas_emissions, na.rm = TRUE),\n    percent_of_total_emissions = (total_natural_gas_of_agency * 100) / first(total_emissions_of_agency),\n    .groups = \"drop\"\n  )\n\n# Find the agency with the highest % of emissions from natural gas in each size category\nnoxious_winners &lt;- \n  noxious_scores |&gt;\n  group_by(size) |&gt;\n  slice_max(percent_of_total_emissions, n = 1, with_ties = FALSE)\n\n# table of winning agencies\nnoxious_winners |&gt;\n  select(-total_natural_gas_of_agency,ntd_id) |&gt;\n  rename(\n    `Size` = size,\n    `NTD ID` = ntd_id,\n    `Agency` = agency, \n    `City` = city, \n    `State` = state, \n    `Natural Gas Emissions (%)` = percent_of_total_emissions\n  ) |&gt; \n  kable(digits = 5, caption = \"Agencies with the Highest Natural Gas Emissions Percentage\")\n\n\n\nAgencies with the Highest Natural Gas Emissions Percentage\n\n\n\n\n\n\n\n\n\n\nSize\nNTD ID\nAgency\nState\nCity\nNatural Gas Emissions (%)\n\n\n\n\nLarge\n20206\nCounty of Nassau, dba: Nassau Inter County Express\nNew York\nMineola\n99.99984\n\n\nMedium\n40042\nBirmingham-Jefferson County Transit Authority\nAlabama\nBirmingham\n99.99993\n\n\nSmall\n40021\nCity of Albany , dba: Albany Transit System\nGeorgia\nAlbany\n99.99999\n\n\n\n\n\nCode\n# Compute average emissions proportion per fuel type\nmean_emissions_by_size &lt;- agency_modes |&gt;\n  group_by(ntd_id, size) |&gt;  \n  summarise(\n    total_te = sum(total_emissions_of_mode, na.rm = TRUE),  \n    mean_bd = sum(biodiesel_emissions, na.rm = TRUE) / total_te,\n    mean_ng = (sum(cnaturalgas_emissions, na.rm = TRUE) + sum(liqnatgas_emissions, na.rm = TRUE)) / total_te,\n    mean_diesel = sum(diesel_emissions, na.rm = TRUE) / total_te,\n    mean_gas = sum(gasoline_emissions, na.rm = TRUE) / total_te,\n    mean_lpg = sum(liqpetgas_emissions, na.rm = TRUE) / total_te,\n    mean_battery = (sum(electricbattery_emissions, na.rm = TRUE) + sum(electricprop_emissions, na.rm = TRUE)) / total_te,\n    .groups = \"drop\"\n  ) |&gt;  \n  group_by(size) |&gt;  \n  summarise(across(everything(), mean, na.rm = TRUE))\n\n# Identify the most \"noxious\" agency (highest natural gas emissions %)\nmost_noxious_agency &lt;- noxious_scores |&gt;\n  slice_max(percent_of_total_emissions, n = 1, with_ties = FALSE) |&gt;  \n  pull(ntd_id)\n\n\nThe table below presents the average distribution of emissions at small-, medium-, and large-sized agencies.\n\n\nCode\n# Table of percentage of total emissions by fuel Type Across Transit Agency Sizes.\nmean_emissions_by_size |&gt; \n  pivot_longer(-c(size, ntd_id), names_to = \"fuel_type\", values_to = \"pct_of_emissions\") |&gt;\n  mutate(\n    pct_of_emissions = round(pct_of_emissions * 100, 5),\n    fuel_type = case_when(\n      fuel_type == \"mean_bd\" ~ \"Bio-Diesel\",\n      fuel_type == \"mean_diesel\" ~ \"Diesel\",\n      fuel_type == \"mean_ng\" ~ \"C Natural Gas and Liquified Natural Gas\",\n      fuel_type == \"mean_gas\" ~ \"Gasoline\",\n      fuel_type == \"mean_battery\" ~ \"Electric (Propulsion and Battery)\",\n      fuel_type == \"mean_hydrogen\" ~ \"Hydrogen\",\n      fuel_type == \"mean_lpg\" ~ \"Liquified Petroleum Gas\",\n      TRUE ~ fuel_type  # Keep other names unchanged\n    )) |&gt;\n  filter(fuel_type != \"total_te\") |&gt; \n  select(-ntd_id) |&gt; \n  pivot_wider(\n    names_from = size,\n    values_from = pct_of_emissions,\n    names_prefix = \"Emissions (%) - \"\n  ) |&gt; \n  rename(`Fuel Type` = fuel_type) |&gt;\n  kable(caption = \"Percentage of Total Emissions by Fuel Type Across Transit Agency Sizes.\")\n\n\n\nPercentage of Total Emissions by Fuel Type Across Transit Agency Sizes.\n\n\n\n\n\n\n\n\nFuel Type\nEmissions (%) - Large\nEmissions (%) - Medium\nEmissions (%) - Small\n\n\n\n\nBio-Diesel\n5.05045\n5.53451\n2.61511\n\n\nC Natural Gas and Liquified Natural Gas\n29.69935\n18.55579\n11.11746\n\n\nDiesel\n47.36432\n58.01950\n38.93629\n\n\nGasoline\n6.95929\n12.95149\n43.93558\n\n\nLiquified Petroleum Gas\n0.79746\n0.78587\n1.71727\n\n\nElectric (Propulsion and Battery)\n10.12913\n4.15284\n1.67828\n\n\n\n\n\nThe figure below compares the average emissions distribution of medium-sized agencies to that of the medium-sized agency with the highest proportion of natural gas emissions in the data set.\n\n\nCode\n# Get emissions breakdown for the most noxious agency\nnoxious_emissions_pie &lt;- \n  agency_modes |&gt;\n  filter(ntd_id == most_noxious_agency) |&gt;\n  summarise(\n    total_me = sum(total_emissions_of_mode, na.rm = TRUE),\n    mean_ng = (sum(cnaturalgas_emissions, na.rm = TRUE) + sum(liqnatgas_emissions, na.rm = TRUE)) / total_me,\n    mean_diesel = sum(diesel_emissions, na.rm = TRUE) / total_me,\n    mean_gas = sum(gasoline_emissions, na.rm = TRUE) / total_me,\n    mean_lpg = sum(liqpetgas_emissions, na.rm = TRUE) / total_me,\n    mean_battery = (sum(electricbattery_emissions, na.rm = TRUE) + sum(electricprop_emissions, na.rm = TRUE)) / total_me\n  ) |&gt;  \n  pivot_longer(cols = everything(), names_to = \"fuel_type\", values_to = \"pct_of_emissions\") |&gt;  \n  mutate(type = \"Most Noxious Agency\")\n\n# Compute the **average** agency emissions breakdown\navg_emissions_pie &lt;- \n  mean_emissions_by_size |&gt; \n  pivot_longer(-c(size,ntd_id), names_to = \"fuel_type\", values_to = \"pct_of_emissions\") |&gt;\n  filter(size == unique(agency_modes |&gt;\n                    filter(ntd_id == most_noxious_agency) |&gt;\n                    pull(size))\n         ) |&gt;\n  mutate(type = \"Average Transit Agency\")\n\n# Combine both data sets for visualization\ncombined_pie_data &lt;- bind_rows(avg_emissions_pie, noxious_emissions_pie) |&gt;  \n  mutate(fuel_type = recode(fuel_type,\n                            \"mean_bd\" = \"Bio-Diesel\",\n                            \"mean_ng\" = \"Natural Gas (CNG & LNG)\",\n                            \"mean_diesel\" = \"Diesel\",\n                            \"mean_gas\" = \"Gasoline\",\n                            \"mean_lpg\" = \"Liquefied Petroleum Gas\",\n                            \"mean_battery\" = \"Electric (Battery & Propulsion)\"\n  )) |&gt;\n  filter(pct_of_emissions &lt; 2)\n\n# Create pie chart\nggplot(combined_pie_data, aes(x = \"\", y = pct_of_emissions, fill = fuel_type)) +\n  geom_col(width = 1, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    x = \"\", y = \"\", fill = \"Fuel Type\",\n    title = \"Transit Agency Emissions Breakdown\",\n    caption = str_wrap(\"Data sources: National Transit Database (NTD) Service and Energy Use datasets, and the U.S. Energy Information Administration (EIA) State Energy Profiles (SEP).\")\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  facet_wrap(~ type)\n\n\n\n\n\n\n\n\n\n\nDisclaimer: This press release is a fictional piece created for educational purposes. Any quotes, statements, or references to individuals, agencies, or organizations are entirely fictional and should not be interpreted as actual endorsements or statements made by real persons or entities."
  },
  {
    "objectID": "mp02.html#green-giant",
    "href": "mp02.html#green-giant",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Green Giant",
    "text": "Green Giant\nThree agencies were awarded Greenies for outstanding performance in their state.\n\nSmall-sized Transit Agency:\n\nOhio Valley Regional Transportation Authority from Wheeling, West Virginia, whose state’s electricity EPC is \\(15479.34\\) times larger than their EPC. To compare, the median factor of difference for small-sized agencies was \\(5,670\\).\n\nMedium-sized Transit Agency:\n\nCity of Seattle, dba: Seattle Center Monorail from Seattle, Washington, with a factor of \\(54,628\\) difference between their EPC and their state’s electricity EPC–almost 3 times the median factor of difference for medium sized-agencies (\\(19,700\\)).\n\nLarge-sized Transit Agency:\n\nCity of Portland, dba: Portland Streetcar from Portland, Oregon, whose state’s EPC is \\(18,650\\) times larger than their own. Among large-sized agencies, the median factor of difference was about \\(12,700\\).\n\n\nThis metric compares state emissions per capita with agency emissions per capita. Agencies with the largest metrics are awarded for outstanding performance compared to their state. “We will continue to take West Virginians home down country roads in a green way, and we hope our Mountain Mama will follow our lead,” said a representative from the Ohio Valley Regional Transportation Authority."
  },
  {
    "objectID": "mp02.html#combining-data-sources",
    "href": "mp02.html#combining-data-sources",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Combining Data Sources",
    "text": "Combining Data Sources\nThe following process was used to merge the primary datasets:\n\n\nCode\n# task 5: table merge\nagency_modes &lt;- \n  NTD_SERVICE |&gt;\n  inner_join(\n    NTD_ENERGY |&gt;\n      select(-c(`Kerosene`, `Bunker Fuel`, `Ethanol`, `Methonal`)), # these columns have 0 values. \n    join_by(\"NTD ID\" == \"NTD ID\")\n    ) |&gt; \n  inner_join(\n    EIA_SEP_REPORT,\n    join_by(State == \"abbreviation\")\n    ) |&gt;\n  select(-c(`Agency Name`, State)) |&gt;\n  rename(\n    agency = `Agency`,\n    city = `City`,\n    mode = `Mode`,\n    upt = `UPT`,\n    miles = `MILES`,\n    ntd_id = `NTD ID`,\n    biodiesel = `Bio-Diesel`,\n    cnaturalgas = `C Natural Gas`,\n    diesel = `Diesel Fuel`,\n    gasoline = `Gasoline`,\n    liqnatgas = `Liquified Nat Gas`,\n    liqpetgas =`Liquified Petroleum Gas`,\n    electric_battery = `Electric Battery`,\n    electric_propulsion = `Electric Propulsion`,\n    co2_mwh = CO2_MWh\n    ) |&gt;\n  mutate(\n    biodiesel_emissions = biodiesel * (co2_vol_mass |&gt; \n                                         filter(co2_factors == \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\") |&gt;\n                                         pull(kg_co2)),\n    cnaturalgas_emissions = cnaturalgas * (co2_vol_mass |&gt;\n                                             filter(co2_factors == \"Natural Gas\") |&gt;\n                                             pull(kg_co2) * MCF_TO_GALLONS),\n    diesel_emissions = diesel * (co2_vol_mass |&gt;\n                                   filter(co2_factors == \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\") |&gt;\n                                   pull(kg_co2)),\n    gasoline_emissions = gasoline * (co2_vol_mass |&gt;\n                                       filter(co2_factors == \"Finished Motor Gasoline\") |&gt;\n                                       pull(kg_co2)),\n    liqnatgas_emissions = liqnatgas * (co2_vol_mass |&gt;\n                                         filter(co2_factors == \"Natural Gas\") |&gt;\n                                         pull(kg_co2) * MCF_TO_GALLONS),\n    liqpetgas_emissions = liqpetgas * (co2_vol_mass |&gt;\n                                         filter(co2_factors == \"Propane\") |&gt;\n                                         pull(kg_co2)),\n    electricbattery_emissions = electric_battery * (co2_mwh / 1000),\n    electricprop_emissions = electric_propulsion * (co2_mwh / 1000),\n    total_emissions_of_mode = (\n      biodiesel_emissions +\n        cnaturalgas_emissions +\n        diesel_emissions +\n        gasoline_emissions +\n        liqnatgas_emissions +\n        liqpetgas_emissions +\n        electricbattery_emissions +\n        electricprop_emissions\n      )\n  ) |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(total_emissions_of_agency = sum(total_emissions_of_mode)) |&gt;\n  ungroup()\n\n# task 6\n# finding the percentile values that will define small, medium, and large sized agencies. \nPERCENTILE_33 &lt;- quantile(agency_modes |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.33)\nPERCENTILE_66 &lt;- quantile(agency_modes |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.66)\n\n# updating agency modes with agency size categorical value + adding green score metric\nagency_modes &lt;-\n  agency_modes |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(\n    agency_emissions_per_capita = sum(total_emissions_of_mode) / upt,\n    emissions_per_transit = sum(total_emissions_of_mode) / miles,\n    green_score = round(sqrt(agency_emissions_per_capita * emissions_per_transit),2),\n    size = case_when(\n      upt &lt; PERCENTILE_33 ~ \"Small\",\n      upt &gt;= PERCENTILE_33 & upt &lt;= PERCENTILE_66 ~ \"Medium\",\n      upt &gt; PERCENTILE_66 ~ \"Large\",\n    )\n    ) |&gt;\n  ungroup()\n\n\nThe table below presents the agency with the lowest emissions per capita.\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency Name\nCity\nState\nAgency Emissions Per Capita\n\n\n\n\n23\nCity of Seattle, dba: Seattle Center Monorail\nSeattle\nWashington\n0.06916\n\n\n\n\n\nThe table below presents the agency with the lowest emissions per transit mile.\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\nNTD ID\nAgency Name\nCity\nState\nAgency Emissions Per Mile\n\n\n\n\n23\nCity of Seattle, dba: Seattle Center Monorail\nSeattle\nWashington\n0.07684"
  },
  {
    "objectID": "mp02.html#noxious-ratings",
    "href": "mp02.html#noxious-ratings",
    "title": "GTA IV Announces Winners of the 2025 Greenies",
    "section": "Noxious Ratings",
    "text": "Noxious Ratings\nWhile some transit agencies set the gold standard for sustainability, others rely heavily on high-emission fuels. Natural gas, with average \\(\\text{CO}_2\\) emissions of approximately \\(410,000\\) kg per gallon, is by far the most polluting transportation fuel. Therefore, GTA IV handed out Noxious ratings to the three agencies with the highest percentage of their total emissions coming from natural gas.\n\nSmall-sized Transit Agency:\n\nCity of Albany, dba: Albany Transit System from Albany, Georgia, with \\(99.99999\\%\\) of their total emissions being derived from natural gas. For comparison, the average small-sized transit agency derives \\(11.1\\%\\) of its emissions from natural gas.\n\nMedium-sized Transit Agency:\n\nBirmingham-Jefferson County Transit Authority from Birmingham, Alabama with \\(99.99993\\%\\) of their total emissions coming from natural gas. The typical medium-sized transit agency gets \\(18.6\\%\\) of its emissions from natural gas.\n\nLarge-sized Transit Agency:\n\nCounty of Nassau, dba: Nassau Inter County Express from Mineola, New York, with \\(99.99984\\%\\) of total emissions from natural gas–far exceeding the large-agency average of \\(28.7\\%\\).\n\n\nTo help recipients of the Noxious rating truly understand the impact of natural gas on the environment, GTA IV staged an unforgettable closing ceremony. A harmless yet pungent gas was released on stage, allowing awardees to experience their contribution to air pollution firsthand. “What is that smell?!” gasped a Nassau Inter County Express representative between coughs, while Albany Transit System’s Chief Legal Officer, Sue Salot, wasted no time in threatening legal action against GTA IV.\nAfter the olfactory assault, GTA IV announced their intention to continue funding grants for transit agencies committed to reducing their environmental impact. “We implore all of you to look deep within your operations and find ways to minimize your footprint. The world we inherit also belongs to those who come after us,” stated Lopez Luna. Agencies wishing to apply for next year’s awards can do so at www.gtaiv.org/apply. Applications open on July 11, 2025, and the alliance encourages all transit agencies to participate. Lopez Luna punctuated the night with a decisive mic drop.\nGTA IV is a nonprofit organization dedicated to improving sustainability in America’s transit agencies. Established in 1997, it provides grants, education, and resources to transit agency leaders to help reduce their environmental impact and pave the way for a greener future.\nFor media inquiries, please contact:\nMargaret Cortona\nGreen Transit Alliance for Investigation of Variance (GTA IV)\nPhone: 212-000-0000\nEmail: mcortona@gtaiv.org"
  },
  {
    "objectID": "mp03_wri.html",
    "href": "mp03_wri.html",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nRows: 4059909 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): artist_name, track_id, artist_id, track_name, album_id, album_name...\ndbl (19): playlist_position, duration, playlist_id, playlist_followers, year...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 21784 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): artist_name, track_id, artist_id, track_name, album_id, album_name...\ndbl (19): playlist_position, duration, playlist_id, playlist_followers, year...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 734 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): artist_name, track_id, artist_id, track_name, album_id, album_name...\ndbl (19): playlist_position, duration, playlist_id, playlist_followers, year...\nlgl  (1): similar_tempo\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 11 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): artist_name, track_id, artist_id, track_name, album_id, album_name...\ndbl (19): playlist_position, duration, playlist_id, playlist_followers, year...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 120 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): artist_name, track_id, artist_id, track_name, album_id, album_name...\ndbl (20): playlist_position, duration, playlist_id, playlist_followers, year...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 9480 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): artist_name, track_id, artist_id, track_name, album_id, album_name...\ndbl (19): playlist_position, duration, playlist_id, playlist_followers, year...\nlgl  (1): similar_duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nWidening tolerance at slot 1\n\nNo song could be placed at slot 1\n\nWidening tolerance at slot 2\n\nNo song could be placed at slot 2\n\nWidening tolerance at slot 3\n\nNo song could be placed at slot 3\n\nWidening tolerance at slot 4\n\nNo song could be placed at slot 4\n\nWidening tolerance at slot 5\n\nNo song could be placed at slot 5\n\nWidening tolerance at slot 6\n\nNo song could be placed at slot 6\n\nWidening tolerance at slot 7\n\nNo song could be placed at slot 7\n\nWidening tolerance at slot 8\n\nNo song could be placed at slot 8\n\nWidening tolerance at slot 11\n\nNo song could be placed at slot 11\n\nWidening tolerance at slot 12\n\nNo song could be placed at slot 12\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "mp03_wri.html#footnotes",
    "href": "mp03_wri.html#footnotes",
    "title": "Heartbeats in Stereo",
    "section": "Footnotes",
    "text": "Footnotes"
  },
  {
    "objectID": "mp03_wri.html#description",
    "href": "mp03_wri.html#description",
    "title": "The Ultimate Playlist",
    "section": "Description",
    "text": "Description\nThis playlist is a nod to the range of sentimentality that only the 80s could deliver–a time when synthesizers carried both dreams and dread. The opening song, “Forever Young” by Alphaville, sets a somber mood, with lead singer Marian Gold reflecting on the looming threat of nuclear annihilation haunting his generation, “Hoping for the best, but expecting the worst / Are you gonna drop the bomb or not?” This existential dread gives way to something more personal in a-ha’s “The Blue Sky,” where we find ourselves at a coffee shop mourning a lost love. A sensitive and forlorn Morten Harket laments, “It doesn’t seem like this / Blue sky’s here for me.”\nThe mood slightly lightens with “Saved By Zero” by The Fixx, a track that hides existential angst beneath hypnotic, looping guitars. R.E.M.’s “Stand,” with a funky guitar and playful melody, it plucks us out of the melodrama and onto a sun-drenched beach where Michael Stipe assures us that “Your feet are going to be on the ground / Your head is there to move you around.”\nTinges of melancholy return in “Love My Way” by The Psychedelic Furs. Richard Butler challenges the “old way” with a defiant, “I follow where my mind goes,” but there’s a naïveté to it, like a kid announcing he’s running away from home. Meanwhile, Yazoo’s “Situation” shifts gears into full dancefloor mode. It’s a takedown of a manipulative presence, but the confrontational message gets a bit lost in the pulsing synths of the club.\nThe mood darkens with George Thorogood & The Destroyers’ “I Drink Alone.” It’s edgy and bluesy, a defiant anthem of solitude that doubles as a cry for help. Thorogood delivers each line with such raspy conviction you can practically smell the whiskey on his breath. We continue our descent into the gloom with Twisted Sister’s “The Price,” a quintessential 80s power ballad lamenting the heavy cost of chasing dreams: a rare moment of vulnerability from a band better known for their rebellious swagger.\nIn keeping with the lugubriousness, Boz Scaggs’ “Look What You’ve Done to Me” is a vulnerable break up song with a heartwrenching delivery of plaintive lines like “Look what you’ve done to me / Never thought I’d fall again so easily.” In “Fast Car,” Tracy Chapman brightens the mood with a sweet, hopeful melody, which starkly contrasts with the darker undertones of the protagonist’s story, as she swings from the chaos of an alcoholic father to the same struggles with her alcoholic husband. The Car’s “Heartbeat City” transports us to a dreamy electronic soundscape of a city with glittering lights pulsing with the rhythm of the beat.\nThe playlist wraps up with Lisa Lisa & Cult Jam’s “Lost In Emotion.” It’s a delightfully smooth landing—confident, tender, hopeful, and exactly the kind of elation we’ve been circling toward all along.\nClick play to start an emotionally engineered ride through the 1980s!"
  },
  {
    "objectID": "mp03_wri.html#contents",
    "href": "mp03_wri.html#contents",
    "title": "The Ultimate Playlist",
    "section": "Contents",
    "text": "Contents\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nPosition\nTrack Name\nArtist Name\nAlbum Name\nYear\n\n\n\n\n1\nForever Young\nAlphaville\nForever Young\n1984\n\n\n2\nThe Blue Sky\na-ha\nHunting High And Low\n1985"
  },
  {
    "objectID": "mp03_wri.html#song-characteristics-dataset",
    "href": "mp03_wri.html#song-characteristics-dataset",
    "title": "The Ultimate Playlist",
    "section": "Song Characteristics Dataset",
    "text": "Song Characteristics Dataset\nThe song characteristics data set was downloaded from this URL, using the code below:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(scales)\n\n#' Load Spotify song data\n#'\n#' This function checks for the existence of a local CSV file containing Spotify song data.\n#' If the file does not exist, it downloads it from a remote URL. The function then reads and returns the data as a tibble.\n#'\n#' @return A tibble containing Spotify song analytics data.\nload_songs &lt;- function() {\n  directory &lt;- \"data/mp03/\"\n  file_name &lt;- paste0(directory, \"spotify_song_analytics.csv\")\n  \n  # Create the data directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  # Download the data file if it's not already present\n  if (!file.exists(file_name)) {\n    download.file(\n      url = \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_name,\n      method = \"auto\"\n    )\n  }\n  \n  # Read the CSV into a tibble\n  songs &lt;- readr::read_csv(file_name)\n  return(songs)\n}\n\n# Load songs into a dataframe\nsongs &lt;- load_songs()\n\n\nA small amount of clean up was required for this data set. The code used to perform it is presented below.\n\n\nCode\n# Instructor provided code for cleaning artist string\nclean_artist_string &lt;- function(x){\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\n# Split songs with multiple artists into separate rows and clean artist names\nsongs &lt;-\n  songs |&gt; \n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)"
  },
  {
    "objectID": "mp03_wri.html#playlists-dataset",
    "href": "mp03_wri.html#playlists-dataset",
    "title": "The Ultimate Playlist",
    "section": "Playlists Dataset",
    "text": "Playlists Dataset\nThe playlists data set was downloaded from this URL using the code outlined below:\n\n\nCode\n#' Load Spotify playlist data\n#'\n#' This function downloads and/or reads the Million Playlist Dataset slices from \n#' a GitHub repository. If `read_only = TRUE`, it skips downloading and only reads \n#' existing local files.\n#'\n#' @param read_only Logical. If TRUE, skips download and only loads local data. Default is FALSE.\n#' @return A list of parsed JSON objects representing playlist data.\nload_playlists &lt;- function(read_only = FALSE) {\n  source_root_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  directory &lt;- \"data/mp03/mpd/\"\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  if (!read_only) {\n    # Loop over all playlist slice file names\n    for (i in seq(0, 999000, by = 1000)) {\n      fname &lt;- paste0(\n        \"mpd.slice.\",\n        format(i, scientific = FALSE), \"-\",\n        format(i + 999, scientific = FALSE),\n        \".json\"\n      )\n      fpath &lt;- file.path(directory, fname)\n      \n      # Download file if it's not already present\n      if (!file.exists(fpath)) {\n        Sys.sleep(0.01)  # Avoid hitting rate limits\n        tryCatch({\n          invisible(download.file(\n            url = paste0(source_root_url, fname),\n            destfile = fpath,\n            method = \"auto\",\n            quiet = TRUE\n          ))\n        }, error = function(e) {\n          message(\"Download failed: \", fname)\n        })\n      }\n    }\n  }\n  \n  # Read all the JSON files in the directory\n  json_files &lt;- list.files(directory, pattern = \"\\\\.json$\", full.names = TRUE)\n  all_data &lt;- vector(\"list\", length(json_files))\n  \n  for (i in seq_along(json_files)) {\n    tryCatch({\n      all_data[[i]] &lt;- suppressWarnings(jsonlite::fromJSON(json_files[i]))\n    }, error = function(e) {\n      message(\"Failed to read: \", json_files[i])\n    })\n  }\n  \n  return(all_data)\n}\n\nSPOTIFY_MILLIONS &lt;- load_playlists()\n\n\nSince the playlist data set was hierarchical, it required “rectangling,” to be adapted for use in this analysis. This process was accomplished with the following code.\n\n\nCode\n#' Process a Spotify playlist slice\n#'\n#' This function processes a single playlist slice from the Million Playlist Dataset,\n#' extracting track information and saving it as a CSV file for faster future loading.\n#' It strips Spotify URI prefixes and appends playlist metadata.\n#'\n#' @param playlist A list object representing one slice of the Spotify playlist data (parsed JSON).\n#' @return A data frame (tibble) of processed track data with playlist-level metadata.\nprocess &lt;- function(playlist) {\n  \n  # Instructor provided helper function to strip Spotify URI prefix\n  strip_spotify_prefix &lt;- function(x) {\n    str_replace(x, \".*:.*:\", \"\")\n  }\n  \n  # Create the output directory if it doesn't exist\n  output_dir &lt;- \"data/mp03/processed\"\n  if (!dir.exists(output_dir)){\n    dir.create(output_dir, showWarnings = FALSE)\n  }\n  \n  # Generate filename for processed slice\n  slice_range &lt;- playlist$info$slice\n  csv_outfile &lt;- paste0(output_dir, \"/mpd.slice.\", slice_range, \".csv\")\n  \n  # If CSV already exists, load it instead of reprocessing\n  if (file.exists(csv_outfile)){\n    all_tracks &lt;- readr::read_csv(csv_outfile)\n  } else {\n    all_tracks &lt;- data.frame()  # Placeholder for all processed tracks\n    \n    for (i in seq_along(playlist$playlists$tracks)) {\n      tracks_df &lt;- as.data.frame(playlist$playlists$tracks[[i]])\n      \n      # Skip empty playlists\n      if (nrow(tracks_df) == 0) next\n      \n      # Add metadata and clean up Spotify URIs\n      tracks_df &lt;- tracks_df |&gt;\n        mutate(\n          playlist_name = playlist$playlists$name[i],\n          playlist_id = playlist$playlists$pid[i],\n          playlist_followers = playlist$playlists$num_followers[i],\n          track_uri = strip_spotify_prefix(track_uri),\n          artist_uri = strip_spotify_prefix(artist_uri),\n          album_uri = strip_spotify_prefix(album_uri)\n        )\n      \n      # Combine with all previously processed tracks\n      all_tracks &lt;- bind_rows(all_tracks, tracks_df)\n    }\n    \n    # Save the processed data for this slice\n    write.csv(all_tracks, csv_outfile, row.names = FALSE)\n    message(\"Saved: \", csv_outfile)\n  }\n  return(all_tracks)\n}\n\n#' Merge all processed playlist slices into a single dataset\n#'\n#' This function checks if a master CSV file (`spotify_millions.csv`) already exists.\n#' If not, it processes all playlist JSON slices provided and merges them into one\n#' unified dataframe, then saves it as a CSV for future use. If the file already exists,\n#' it loads the data directly.\n#'\n#' @param data A list of playlist slices (parsed JSON). Defaults to `master_json`.\n#' @return A dataframe containing the merged Spotify playlist data.\nmerge_processed_playlists &lt;- function(data = master_json) {\n  # Path to the master output CSV\n  spotify_millions_fpath &lt;- \"data/mp03/spotify_millions.csv\"\n  \n  if (!file.exists(spotify_millions_fpath)) {\n    # If no master file exists, process each playlist slice\n    playlists &lt;- data.frame()  # Initialize an empty dataframe\n    \n    for (i in seq_along(master_json)) {\n      playlist &lt;- master_json[[i]]\n      processed_df &lt;- process(playlist)  # Process individual slice\n      playlists &lt;- bind_rows(playlists, processed_df)  # Append to master dataframe\n    }\n\n    # Clean up and standardize column names\n    playlists &lt;- playlists |&gt; \n      rename(\n        playlist_position = `pos`,\n        track_id = `track_uri`,\n        artist_id = `artist_uri`,\n        album_id = `album_uri`,\n        duration = `duration_ms`\n      ) |&gt;\n      select(!`...1`)  # Drop the unnamed index column\n\n    # Save merged data to CSV\n    write.csv(playlists, spotify_millions_fpath, row.names = FALSE)\n  } else {\n    # If file exists, just read it\n    playlists &lt;- readr::read_csv(spotify_millions_fpath)\n  }\n\n  return(playlists)\n}\n\n# Load or process the Spotify Millions json dataset\nSPOTIFY_MILLIONS &lt;- merge_processed_playlists(SPOTIFY_MILLIONS)\n\n\n\n\nCode\n# Load or process the Spotify Millions dataset\nSPOTIFY_MILLIONS &lt;- merge_processed_playlists()"
  },
  {
    "objectID": "mp03_wri.html#joining-the-data-sets",
    "href": "mp03_wri.html#joining-the-data-sets",
    "title": "The Ultimate Playlist:",
    "section": "Joining the Data Sets",
    "text": "Joining the Data Sets\nI joined the playlist and song characteristic data sets as in the code below. The Spotify playlist data and the song characteristics data was subsequently deleted to free up RAM.\n\n\nCode\nmerged_fpath &lt;- \"data/mp03/spotify_millions_songs_join.csv\"\nif (!file.exists(merged_fpath)){\n  spotify_millions_songs &lt;- \n    SPOTIFY_MILLIONS |&gt;\n    inner_join(songs, join_by(track_id == id, track_name == name, artist_name == artist, duration == duration_ms)) |&gt;\n    mutate(decade = paste0((year %/% 10) * 10, \"s\"))\n  write.csv(spotify_millions_songs, merged_fpath, row.names = FALSE)\n} else{\n  spotify_millions_songs &lt;- readr::read_csv(merged_fpath)\n}\nif (exists(SPOTIFY_MILLIONS) & exists(songs)){\n  rm(SPOTIFY_MILLIONS, songs)\n}"
  },
  {
    "objectID": "mp03_wri.html#analysis",
    "href": "mp03_wri.html#analysis",
    "title": "The Ultimate Playlist",
    "section": "Analysis",
    "text": "Analysis\nThe figure below presents the relationship between that characteristic popularity and playlist appearances.\n\n\nCode\n# Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n# Prep the data\ndata &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  mutate(log_appearances = log1p(playlist_appearances))\n\n# Fit the model\nmodel &lt;- lm(log_appearances ~ popularity, data = data)\n\n# Get model metrics\nmodel_eq &lt;- broom::tidy(model)\nr_squared &lt;- broom::glance(model)$r.squared\ncorrelation &lt;- cor(data$popularity, data$log_appearances, use = \"complete.obs\")\n\n# Build the equation text\neq_text &lt;- paste0(\n  \"y = \", round(model_eq$estimate[1], 2), \" + \",\n  round(model_eq$estimate[2], 2), \"x\\n\",\n  \"r = \", round(correlation, 2), \n  \" | R² = \", round(r_squared, 2)\n)\n\n# Plot\nggplot(data, aes(x = popularity, y = log_appearances)) +\n  geom_jitter(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  annotate(\"text\", x = 12.75, y = -1,\n           label = eq_text, hjust = 0, size = 4, fontface = \"italic\") +\n  labs(\n    title = \"Figure 1: Relationship Between Popularity and Playlist Appearances\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Popularity\",\n    y = \"Log(Playlist Appearances)\"\n  ) +\n  theme_minimal()\n\n\nMedian track popularity by year is presented in the figure below. Notably, 2017 stands out as the year with the highest median song popularity.\n\n\nCode\n# In what year were the most popular songs released?\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  group_by(year) |&gt;\n  summarize(median_popularity = median(popularity, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = median_popularity)) +\n  geom_line(color = MINTY, linewidth = 1.2) +\n  geom_point(color = MINTY, size = 2) +\n  scale_x_continuous(\n    breaks = seq(min(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 max(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 by = 10)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Median Track Popularity\",\n    title = \"Median Track Popularity by Year\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal()\n\n\nWhile median song popularity continues to keep up with the times, median track dance-ability peaked in 1929. The figure below presents the evolution of track dance ability over the past 90 years.\n\n\nCode\n# In what year did danceability peak?\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  group_by(year) |&gt;\n  summarize(median_danceability = median(danceability, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = median_danceability)) +\n  geom_line(color = MINTY, linewidth = 1.2) +\n  geom_point(color = MINTY, size = 2) +\n  scale_x_continuous(\n    breaks = seq(min(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 max(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 by = 10)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Median Track Danceability\",\n    title = \"Median Track Danceability by Year\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal()\n\n\nThe most represented decade on user playlists is the 2010s. The bar chart below presents the distribution of tracks belonging to a particular decade.\n\n\nCode\n# Which decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\n\nspotify_millions_songs |&gt;\n  mutate(decade = paste0((year %/% 10) * 10, \"s\")) |&gt;\n  group_by(decade) |&gt;\n  summarize(decade_count = n()/10000) |&gt;\n  ggplot(aes(x = decade_count, y = factor(decade))) +\n  geom_bar(stat = \"identity\", fill = MINTY) +\n  labs(\n    x = \"Number of Tracks (in tens of thousands)\",\n    y = \"Decade\",\n    title = \"Decade Representation in Playlists\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(angle = 45, hjust = 1)\n  )\n\n\nThe most popular musical key is G, as can be seen in the figure of key distribution frequency below.\n\n\nCode\n# Create the key names\nkey_names &lt;- c(\n  \"C\", \"C♯/Db\", \"D\", \"D♯/Eb\", \"E\", \"F\",\n  \"F♯/Gb\", \"G\", \"G♯/Ab\", \"A\", \"A♯/Bb\", \"B\"\n)\n\n# Count frequency per key and label with musical key names\ndata &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  count(key, name = \"key_frequency\") |&gt;\n  mutate(key_label = factor(key_names[key + 1], levels = key_names))\n\n# Create label positions at fixed theta, variable radius\ny_values &lt;- seq(1000, max(data$key_frequency), by = 1000)\n\nradial_labels &lt;- tibble(\n  x = rep(\"D♯/Eb\", length(y_values)),  # Repeat \"C\" the same number of times as y values\n  y = y_values,\n  label = as.character(y_values)\n)\n\n# Plot with polar coordinates and annotations\nggplot(data, aes(x = key_label, y = key_frequency, fill = key_label)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(start = 0) +\n  geom_text(data = radial_labels, aes(x = x, y = y, label = label),\n            inherit.aes = FALSE, size = 3, color = \"gray20\", hjust = -0.2) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of Musical Keys\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\nCode\n# What are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\n\nduration_percentiles &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  filter(!is.na(duration)) |&gt;\n  mutate(duration_sec = duration / 1000) |&gt;\n  summarize(\n    p33 = quantile(duration_sec, 0.33),\n    p66 = quantile(duration_sec, 0.66)\n  )\n\n# Pull out threshold values\np33 &lt;- duration_percentiles$p33\np66 &lt;- duration_percentiles$p66\n\n# Categorize and plot\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  filter(!is.na(duration)) |&gt;\n  mutate(\n    duration_sec = duration / 1000,\n    duration_category = case_when(\n      duration_sec &lt;= p33 ~ \"Short\",\n      duration_sec &lt;= p66 ~ \"Medium\",\n      TRUE ~ \"Long\"\n    )\n  ) |&gt;\n  count(duration_category) |&gt;\n  ggplot(aes(x = duration_category, y = n, fill = duration_category)) +\n  geom_col() +\n  labs(\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    title = \"Song Duration Categories\",\n    x = \"Duration Category\",\n    y = \"Number of Unique Tracks\"\n  ) +\n  scale_y_continuous(labels = comma_format()) +\n  scale_fill_manual(\n    name = \"Duration Category\",\n    values = c(\"Short\" = MINTY, \"Medium\" = \"#fdd835\", \"Long\" = \"#ef5350\")) +\n  theme_minimal()\n\n\n\n\nCode\n# Does danceability correlate with popularity?\ndata &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE)\n# Fit the model\nmodel &lt;- lm(danceability ~ popularity, data = data)\n\n# Get model metrics\nmodel_eq &lt;- broom::tidy(model)\nr_squared &lt;- broom::glance(model)$r.squared\ncorrelation &lt;- cor(data$popularity, data$danceability, use = \"complete.obs\")\n\n# set up annotation\neq_text &lt;- paste0(\n  \"y = \", round(model_eq$estimate[1], 2), \" + \",\n  round(model_eq$estimate[2], 3), \"x\\n\",\n  \"r = \", round(correlation, 2), \n  \" | R² = \", round(r_squared, 2)\n)\n\n# Plot\nggplot(data, aes(x = popularity, y = danceability)) +\n  geom_jitter(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  annotate(\"text\", x = 5, y = 0,\n           label = eq_text, hjust = 0, size = 4, fontface = \"italic\") +\n  labs(\n    title = \"Relationship Between Popularity and Danceability\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Popularity\",\n    y = \"Log(Playlist Appearances)\"\n  ) +\n  theme_minimal()\n\n\n\n\nCode\n# What are the top 5 most common opening playlist songs?\n\nspotify_millions_songs |&gt;\n  filter(pos == 0) |&gt; \n  add_count(track_id, name = \"song_frequency\") |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt; \n  arrange(desc(song_frequency)) |&gt;\n  slice_head(n = 10) |&gt;  \n  ggplot(aes(x = song_frequency, y = fct_reorder(paste0(track_name, \"\\n\", artist_name), song_frequency))) +\n  geom_bar(stat = \"identity\", fill = \"#69b3a2\") +\n  labs(\n    title = \"Top 10 Most Popular Opening Playlist Songs\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Number of Playlists (Opening Position)\",\n    y = \"Track Name\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp03_wri.html#playlist-construction",
    "href": "mp03_wri.html#playlist-construction",
    "title": "The Ultimate Playlist",
    "section": "Playlist Construction",
    "text": "Playlist Construction\n\nChoosing Anchor Songs\nSince I’ve been on an 80s kick recently, I picked the anchor songs tabulated below.\n\n\nCode\n# base songs\nsong1 &lt;- spotify_millions_songs |&gt; filter(track_id == \"4S1VYqwfkLit9mKVY3MXoo\") # Forever Young by Alphaville\nsong2 &lt;- spotify_millions_songs |&gt; filter(track_id == \"22Ca1a5rQ5g2UFEZ3pp4tL\") # The Blue Sky by a-ha\n\nanchor_songs &lt;- list(song1, song2)\n\nbind_rows(song1,song2) |&gt; \n  select(track_name, artist_name, album_name, year) |&gt;\n  distinct(track_name, .keep_all = TRUE) |&gt;\n  rename(\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Year\" = year\n  ) |&gt; \n  kable()\n\n\n\n\nCreating a Candidate Pool\nTo determine compatibility with the anchor songs, I applied the following heuristics:\n\nCo-occurrence: Songs that frequently appear on playlists alongside the anchor songs.\nMusical similarity: Songs in the same key and with a tempo within $$5 BPM of the anchor.\nArtist match: Songs by the same artist(s) as the anchor tracks.\nAudio profile similarity: Songs released in the same year and exhibiting similar values for acousticness, dance-ability, energy, instrumentalness, liveness, loudness, speechiness, and valence. Similarity is measured using Euclidean distance: \\[d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\] where \\(x_i\\) and \\(y_i\\) are the characteristic values for the anchor and candidate songs, respectively.\nDecade match and structural similarity: Songs from the same decade and duration within \\(\\pm\\) 15 seconds of the anchor song.\n\nThe application of these heuristics is implemented in the code below.\n\n\nCode\nfname &lt;- \"data/mp03/peer_songs.csv\"\nif (!file.exists(fname)){\n# What other songs commonly appear on playlists along side these songs?\npeer_songs &lt;- data.frame()\nfor (song in anchor_songs) {\n  playlist_ids &lt;- song |&gt; pull(playlist_id) |&gt; unique() |&gt; as.vector()\n  for (pid in playlist_ids) {\n    peer_songs &lt;- bind_rows(\n      peer_songs,\n      spotify_millions_songs |&gt;\n        filter(playlist_id == pid) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }\n}\n  write.csv(peer_songs, fname, row.names = FALSE)\n} else{\n  peer_songs &lt;- readr::read_csv(fname)\n}\n\n# What songs have a similar key and tempo to these songs\nfname &lt;- \"data/mp03/key_and_tempo.csv\"\nif (!file.exists(fname)){\n  key_and_tempo &lt;- data.frame()\n  for (song in anchor_songs){\n    song_key &lt;- unique(song |&gt; pull(key))\n    song_tempo &lt;- unique(song |&gt; pull(tempo))\n    key_and_tempo &lt;- \n      bind_rows(\n        key_and_tempo,\n        spotify_millions_songs |&gt;\n          mutate(\n            similar_tempo = case_when(\n              abs(tempo - unique(song |&gt; pull(tempo))) &lt;= 5 ~ TRUE,\n              TRUE ~ FALSE\n            )) |&gt;\n          filter(\n            key == song_key,\n            similar_tempo == TRUE) |&gt;\n          distinct(track_id, .keep_all = TRUE)\n      )\n  }\n  write.csv(key_and_tempo, fname, row.names = FALSE)\n} else{\n    key_and_tempo &lt;- readr::read_csv(fname)\n}\n\n# What songs have the same artist?\nfname &lt;- \"data/mp03/same_artist.csv\"\nif (!file.exists(fname)){\n  same_artist = data.frame()\n  for (song in anchor_songs){\n    song_artist_id &lt;- unique(song |&gt; pull(artist_id))\n    same_artist &lt;- bind_rows(\n      same_artist,\n      spotify_millions_songs |&gt;\n        filter(artist_id == song_artist_id) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }\n  write.csv(same_artist, fname, row.names = FALSE)\n} else{\n    same_artist &lt;- readr::read_csv(fname)\n}\n\n# What other songs were released in the same year and have similar levels of acousticness, danceability, etc.\nfname &lt;- \"data/mp03/all_similar_songs.csv\"\nif (!file.exists(fname)){\n  all_similar_songs &lt;- data.frame()\n  \n  #' Calculate Similar Songs Based on Selected Metrics\n  #'\n  #' This function identifies songs similar to a given song by calculating the Euclidean distance between \n  #' the selected metrics (e.g., acousticness, danceability, energy, etc.) of the input song and all other songs\n  #' from the same year. Only songs with a distance smaller than a specified threshold (2.5) are considered similar.\n  #'\n  #' @param song A data frame representing a single song, containing columns for song characteristics (e.g., `track_id`, `year`, and metrics like `acousticness`, `danceability`, etc.).\n  #' @param std_songs A data frame representing the standardized set of songs to compare against, which must include the same metrics as in the `song` argument.\n  #' @param metrics A character vector containing the names of the metrics to be used in calculating the similarity (e.g., `c(\"acousticness\", \"danceability\", \"energy\", \"valence\")`).\n  #'\n  #' @return A data frame with songs that are similar to the input song, based on the Euclidean distance between their metric values. The songs are filtered by a distance threshold of 2.5, \n  #'         and they are ordered by their proximity to the input song.\n  #'\n  #' @details The function scales the metrics for all songs in the same year as the input song and calculates the Euclidean distance between the input song's metrics and each song's metrics.\n  #'          The distance is computed as the square root of the sum of squared differences between the selected metrics of the input song and each candidate song.\n  #'          Only songs with a distance smaller than 2.5 are considered similar.\n  #'\n  #' @examples\n  #' # Assuming `song` and `std_songs` are data frames with appropriate structure\n  #' similar_songs &lt;- calculate_similar_songs(song = song_data, std_songs = song_dataset, metrics = c(\"acousticness\", \"danceability\", \"energy\"))\n  #'\n  #' @export\n  calculate_similar_songs &lt;- function(song, std_songs, metrics) {\n    # Filter for songs from the same year and apply scaling to the metrics\n    std_songs_scaled &lt;- std_songs |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      filter(year == unique(song |&gt; pull(year))) |&gt;\n      mutate(across(all_of(metrics), scale))\n    \n    # Get the song vector (metrics for the current song)\n    song_vec &lt;- std_songs_scaled |&gt;\n      filter(track_id == unique(song |&gt; pull(track_id))) |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      select(all_of(metrics)) |&gt;\n      as.numeric()\n    \n    # Assign proper names to the song vector based on metrics\n    names(song_vec) &lt;- metrics\n    \n    # Calculate the distance for each song\n    similar_songs &lt;- std_songs_scaled |&gt;\n      mutate(distance = sqrt(\n        (acousticness - song_vec[\"acousticness\"])^2 +\n          (danceability - song_vec[\"danceability\"])^2 +\n          (energy - song_vec[\"energy\"])^2 +\n          (instrumentalness - song_vec[\"instrumentalness\"])^2 +\n          (liveness - song_vec[\"liveness\"])^2 +\n          (loudness - song_vec[\"loudness\"])^2 +\n          (speechiness - song_vec[\"speechiness\"])^2 +\n          (valence - song_vec[\"valence\"])^2\n      )) |&gt;\n      arrange(distance) |&gt;\n      filter(distance &lt; 2)\n    \n    # Return the result\n    return(similar_songs)\n  }\n  \n  anchor_songs &lt;- list(song1, song2)\n  \n  # For loop to calculate similar songs for each song and bind the rows\n  for (song in anchor_songs) {\n    similar_songs &lt;- calculate_similar_songs(song, spotify_millions_songs, metrics)\n    all_similar_songs &lt;- bind_rows(all_similar_songs, similar_songs)\n  }\n  write.csv(all_similar_songs, fname, row.names = FALSE)\n} else{\n    all_similar_songs &lt;- readr::read_csv(fname)\n}\n\n# What songs are in the same decade and have a similar duration?\n# add decade to song data.frames\nsong1 &lt;- song1 |&gt; mutate(decade = paste0((year %/% 10) * 10, \"s\"))\nsong2 &lt;- song2 |&gt; mutate(decade = paste0((year %/% 10) * 10, \"s\"))\nanchor_songs &lt;- list(song1, song2)\nfname &lt;- \"data/mp03/decade_and_song_length.csv\"\nif (!file.exists(fname)){\n  decade_and_song_length &lt;- data.frame()\n  for (song in anchor_songs){\n    song_decade &lt;- unique(song |&gt; pull(decade))\n    song_length &lt;- unique(song |&gt; pull(duration))\n    decade_and_song_length &lt;- bind_rows(\n      decade_and_song_length,\n      spotify_millions_songs |&gt;\n        mutate(\n          similar_duration = case_when(\n            abs(song_length - duration) &lt;= 10 * 1000 ~ TRUE, \n            TRUE ~ FALSE\n          )\n        ) |&gt;\n        filter(\n          decade == song_decade,\n        ) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }  \n  write.csv(decade_and_song_length, fname, row.names = FALSE)\n} else{\n    decade_and_song_length &lt;- readr::read_csv(fname)\n}\n\n# combining heuristics to generate candidates for ultimate playlist selection.\n\ncombined_heuristics &lt;-\n  bind_rows(\n    mutate(all_similar_songs, source = \"all_similar_songs\"),\n    mutate(key_and_tempo, source = \"key_and_tempo\"),\n    mutate(same_artist, source = \"same_artist\"),\n    mutate(peer_songs, source = \"peer_songs\"),\n    mutate(decade_and_song_length, source = \"decade_and_song_length\")\n  ) |&gt;\n  add_count(track_id, name = \"heuristic_count\") |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\n\nThe full list of candidate songs is shown below:\n\n\nSelecting Playlist Songs\nSince I wanted the playlist to have a deliberate “rise and fall,” in mood, I decided to methodically control the valence of the playlist. Specifically, I shaped the playlist so that the valence of the selected songs would follow the shape of a sine wave, and create an emotional roller-coaster for listeners to ride.\nTo implement this:\n\nI generated a cosine wave scaled between 0 and 1 to define the target valence levels for each of the 12 songs in the playlist.\nI added the anchor songs to the playlist first, placing each one in the position where its valence most closely matched the wave.\nRemaining slots were filled with songs from a filtered pool of candidates that met 4 or more compatibility heuristics (see definitions above). I also restricted the decade of the song to 1980.\nFor each remaining position, I selected a song with a valence close to the target value. I allowed for a small tolerance to ensure flexibility in the song choice and still maintain the shape of the wave.\n\nThe code for implementing this is shown below.\n\n\nCode\n# Set seed for reproducibility\nset.seed(7)\nplaylist_length &lt;- 12\n\n# Generate cosine wave to model valence progression\nvalence_wave &lt;- (cos(seq(0, 2 * pi, length.out = playlist_length)) + 1) / 2\n\n# Define valence similarity tolerance\ntolerance &lt;- 0.05\n\n# Initialize empty playlist\nplaylist &lt;- data.frame()\n\n# Combine anchor songs and ensure uniqueness\nbase_songs &lt;- bind_rows(anchor_songs) |&gt; distinct(track_id, .keep_all = TRUE)\n\n# Select high-confidence candidate songs\ncandidates &lt;- combined_heuristics |&gt; \n  filter(heuristic_count &gt;= 4, decade == '1980s') |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\n# Create a vector of available positions\navailable_slots &lt;- 1:playlist_length\n\n# Place anchor songs into playlist by matching valence to wave\nfor (i in seq_len(nrow(base_songs))) {\n  song &lt;- base_songs[i, ]\n  if (length(available_slots) == 0) break\n  \n  diffs &lt;- abs(valence_wave[available_slots] - song$valence)\n  best_slot &lt;- available_slots[which.min(diffs)]\n  \n  playlist &lt;- bind_rows(playlist, song)\n  available_slots &lt;- setdiff(available_slots, best_slot)\n  candidates &lt;- candidates |&gt; filter(track_id != song$track_id)\n}\n\n# Fill remaining slots with best valence matches from candidates\nfor (slot in available_slots) {\n  target_valence &lt;- valence_wave[slot]\n  \n  pool &lt;- candidates |&gt; filter(abs(valence - target_valence) &lt;= tolerance)\n  \n  # Expand tolerance if needed\n  if (nrow(pool) == 0) {\n    message(glue::glue(\"Widening tolerance at slot {slot}\"))\n    pool &lt;- candidates |&gt; filter(abs(valence - target_valence) &lt;= (tolerance * 2))\n  }\n  \n  # Select one matching song if available\n  if (nrow(pool) &gt; 0) {\n    selected &lt;- sample_n(pool, 1)\n    playlist &lt;- bind_rows(playlist, selected)\n    candidates &lt;- candidates |&gt; filter(track_id != selected$track_id)\n  } else {\n    message(glue::glue(\"No song could be placed at slot {slot}\"))\n  }\n}\n\n# Add song position index\nplaylist &lt;- playlist |&gt; mutate(position = row_number())\n\n# Reshape for plotting\nplaylist_long &lt;- playlist |&gt; pivot_longer(cols = all_of(metrics), names_to = \"metric\", values_to = \"value\")\n\n# Plot how each metric changes across the playlist\nggplot(playlist_long, aes(x = position, y = value, group = metric, color = metric)) +\n  geom_line(size = 1) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free_y\") +\n  labs(\n    title = \"Evolution of Song Metrics Across Playlist\",\n    x = \"Playlist Order\", y = \"Metric Value\"\n  ) +\n  theme_minimal()\n\n\nThe final result is an 80s playlist designed to, at some points, make you feel like you’re about to hit 88 mph in Doc’s Delorean, and at others, like you’re slow-dancing alone at prom under a disco ball."
  },
  {
    "objectID": "mp03_wri.html#choosing-anchor-songs",
    "href": "mp03_wri.html#choosing-anchor-songs",
    "title": "The Ultimate Playlist:",
    "section": "Choosing Anchor Songs",
    "text": "Choosing Anchor Songs\nSince I’ve been on an 80s kick recently, I picked the anchor songs tabulated below.\n\n\nCode\n# base songs\nsong1 &lt;- spotify_millions_songs |&gt; filter(track_id == \"4S1VYqwfkLit9mKVY3MXoo\") # Forever Young by Alphaville\nsong2 &lt;- spotify_millions_songs |&gt; filter(track_id == \"22Ca1a5rQ5g2UFEZ3pp4tL\") # The Blue Sky by a-ha\n\nanchor_songs &lt;- list(song1, song2)\n\nbind_rows(song1,song2) |&gt; \n  select(track_name, artist_name, album_name, year) |&gt;\n  distinct(track_name, .keep_all = TRUE) |&gt;\n  rename(\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Year\" = year\n  ) |&gt; \n  kable()"
  },
  {
    "objectID": "mp03_wri.html#creating-a-candidate-pool",
    "href": "mp03_wri.html#creating-a-candidate-pool",
    "title": "The Ultimate Playlist:",
    "section": "Creating a Candidate Pool",
    "text": "Creating a Candidate Pool\nTo determine compatibility with the anchor songs, I applied the following heuristics:\n\nCo-occurrence: Songs that frequently appear on playlists alongside the anchor songs.\nMusical similarity: Songs in the same key and with a tempo within $$5 BPM of the anchor.\nArtist match: Songs by the same artist(s) as the anchor tracks.\nAudio profile similarity: Songs released in the same year and exhibiting similar values for acousticness, dance-ability, energy, instrumentalness, liveness, loudness, speechiness, and valence. Similarity is measured using Euclidean distance: \\[d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\] where \\(x_i\\) and \\(y_i\\) are the characteristic values for the anchor and candidate songs, respectively.\nDecade match and structural similarity: Songs from the same decade and duration within \\(\\pm\\) 15 seconds of the anchor song.\n\nThe application of these heuristics is implemented in the code below.\n\n\nCode\nfname &lt;- \"data/mp03/peer_songs.csv\"\nif (!file.exists(fname)){\n# What other songs commonly appear on playlists along side these songs?\npeer_songs &lt;- data.frame()\nfor (song in anchor_songs) {\n  playlist_ids &lt;- song |&gt; pull(playlist_id) |&gt; unique() |&gt; as.vector()\n  for (pid in playlist_ids) {\n    peer_songs &lt;- bind_rows(\n      peer_songs,\n      spotify_millions_songs |&gt;\n        filter(playlist_id == pid) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }\n}\n  write.csv(peer_songs, fname, row.names = FALSE)\n} else{\n  peer_songs &lt;- readr::read_csv(fname)\n}\n\n# What songs have a similar key and tempo to these songs\nfname &lt;- \"data/mp03/key_and_tempo.csv\"\nif (!file.exists(fname)){\n  key_and_tempo &lt;- data.frame()\n  for (song in anchor_songs){\n    song_key &lt;- unique(song |&gt; pull(key))\n    song_tempo &lt;- unique(song |&gt; pull(tempo))\n    key_and_tempo &lt;- \n      bind_rows(\n        key_and_tempo,\n        spotify_millions_songs |&gt;\n          mutate(\n            similar_tempo = case_when(\n              abs(tempo - unique(song |&gt; pull(tempo))) &lt;= 5 ~ TRUE,\n              TRUE ~ FALSE\n            )) |&gt;\n          filter(\n            key == song_key,\n            similar_tempo == TRUE) |&gt;\n          distinct(track_id, .keep_all = TRUE)\n      )\n  }\n  write.csv(key_and_tempo, fname, row.names = FALSE)\n} else{\n    key_and_tempo &lt;- readr::read_csv(fname)\n}\n\n# What songs have the same artist?\nfname &lt;- \"data/mp03/same_artist.csv\"\nif (!file.exists(fname)){\n  same_artist = data.frame()\n  for (song in anchor_songs){\n    song_artist_id &lt;- unique(song |&gt; pull(artist_id))\n    same_artist &lt;- bind_rows(\n      same_artist,\n      spotify_millions_songs |&gt;\n        filter(artist_id == song_artist_id) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }\n  write.csv(same_artist, fname, row.names = FALSE)\n} else{\n    same_artist &lt;- readr::read_csv(fname)\n}\n\n# What other songs were released in the same year and have similar levels of acousticness, danceability, etc.\nfname &lt;- \"data/mp03/all_similar_songs.csv\"\nif (!file.exists(fname)){\n  all_similar_songs &lt;- data.frame()\n  \n  #' Calculate Similar Songs Based on Selected Metrics\n  #'\n  #' This function identifies songs similar to a given song by calculating the Euclidean distance between \n  #' the selected metrics (e.g., acousticness, danceability, energy, etc.) of the input song and all other songs\n  #' from the same year. Only songs with a distance smaller than a specified threshold (2.5) are considered similar.\n  #'\n  #' @param song A data frame representing a single song, containing columns for song characteristics (e.g., `track_id`, `year`, and metrics like `acousticness`, `danceability`, etc.).\n  #' @param std_songs A data frame representing the standardized set of songs to compare against, which must include the same metrics as in the `song` argument.\n  #' @param metrics A character vector containing the names of the metrics to be used in calculating the similarity (e.g., `c(\"acousticness\", \"danceability\", \"energy\", \"valence\")`).\n  #'\n  #' @return A data frame with songs that are similar to the input song, based on the Euclidean distance between their metric values. The songs are filtered by a distance threshold of 2.5, \n  #'         and they are ordered by their proximity to the input song.\n  #'\n  #' @details The function scales the metrics for all songs in the same year as the input song and calculates the Euclidean distance between the input song's metrics and each song's metrics.\n  #'          The distance is computed as the square root of the sum of squared differences between the selected metrics of the input song and each candidate song.\n  #'          Only songs with a distance smaller than 2.5 are considered similar.\n  #'\n  #' @examples\n  #' # Assuming `song` and `std_songs` are data frames with appropriate structure\n  #' similar_songs &lt;- calculate_similar_songs(song = song_data, std_songs = song_dataset, metrics = c(\"acousticness\", \"danceability\", \"energy\"))\n  #'\n  #' @export\n  calculate_similar_songs &lt;- function(song, std_songs, metrics) {\n    # Filter for songs from the same year and apply scaling to the metrics\n    std_songs_scaled &lt;- std_songs |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      filter(year == unique(song |&gt; pull(year))) |&gt;\n      mutate(across(all_of(metrics), scale))\n    \n    # Get the song vector (metrics for the current song)\n    song_vec &lt;- std_songs_scaled |&gt;\n      filter(track_id == unique(song |&gt; pull(track_id))) |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      select(all_of(metrics)) |&gt;\n      as.numeric()\n    \n    # Assign proper names to the song vector based on metrics\n    names(song_vec) &lt;- metrics\n    \n    # Calculate the distance for each song\n    similar_songs &lt;- std_songs_scaled |&gt;\n      mutate(distance = sqrt(\n        (acousticness - song_vec[\"acousticness\"])^2 +\n          (danceability - song_vec[\"danceability\"])^2 +\n          (energy - song_vec[\"energy\"])^2 +\n          (instrumentalness - song_vec[\"instrumentalness\"])^2 +\n          (liveness - song_vec[\"liveness\"])^2 +\n          (loudness - song_vec[\"loudness\"])^2 +\n          (speechiness - song_vec[\"speechiness\"])^2 +\n          (valence - song_vec[\"valence\"])^2\n      )) |&gt;\n      arrange(distance) |&gt;\n      filter(distance &lt; 2)\n    \n    # Return the result\n    return(similar_songs)\n  }\n  \n  anchor_songs &lt;- list(song1, song2)\n  # defining metrics that will determine \"closeness\"\n  metrics &lt;- c(\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"valence\")\n  \n  # For loop to calculate similar songs for each song and bind the rows\n  for (song in anchor_songs) {\n    similar_songs &lt;- calculate_similar_songs(song, spotify_millions_songs, metrics)\n    all_similar_songs &lt;- bind_rows(all_similar_songs, similar_songs)\n  }\n  write.csv(all_similar_songs, fname, row.names = FALSE)\n} else{\n    all_similar_songs &lt;- readr::read_csv(fname)\n}\n\n# What songs are in the same decade and have a similar duration?\n# add decade to song data.frames\nsong1 &lt;- song1 |&gt; mutate(decade = paste0((year %/% 10) * 10, \"s\"))\nsong2 &lt;- song2 |&gt; mutate(decade = paste0((year %/% 10) * 10, \"s\"))\nanchor_songs &lt;- list(song1, song2)\nfname &lt;- \"data/mp03/decade_and_song_length.csv\"\nif (!file.exists(fname)){\n  decade_and_song_length &lt;- data.frame()\n  for (song in anchor_songs){\n    song_decade &lt;- unique(song |&gt; pull(decade))\n    song_length &lt;- unique(song |&gt; pull(duration))\n    decade_and_song_length &lt;- bind_rows(\n      decade_and_song_length,\n      spotify_millions_songs |&gt;\n        mutate(\n          similar_duration = case_when(\n            abs(song_length - duration) &lt;= 10 * 1000 ~ TRUE, \n            TRUE ~ FALSE\n          )\n        ) |&gt;\n        filter(\n          decade == song_decade,\n        ) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }  \n  write.csv(decade_and_song_length, fname, row.names = FALSE)\n} else{\n    decade_and_song_length &lt;- readr::read_csv(fname)\n}\n\n# combining heuristics to generate candidates for ultimate playlist selection.\n\ncombined_heuristics &lt;-\n  bind_rows(\n    mutate(all_similar_songs, source = \"all_similar_songs\"),\n    mutate(key_and_tempo, source = \"key_and_tempo\"),\n    mutate(same_artist, source = \"same_artist\"),\n    mutate(peer_songs, source = \"peer_songs\"),\n    mutate(decade_and_song_length, source = \"decade_and_song_length\")\n  ) |&gt;\n  add_count(track_id, name = \"heuristic_count\") |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\n\nThe full list of candidate songs is shown below:\n\nSelecting Playlist Songs\nSince I wanted the playlist to have a deliberate “rise and fall,” in mood, I decided to methodically control the valence of the playlist. Specifically, I shaped the playlist so that the valence of the selected songs would follow the shape of a sine wave, and create an emotional roller-coaster for listeners to ride.\nTo implement this:\n\nI generated a cosine wave scaled between 0 and 1 to define the target valence levels for each of the 12 songs in the playlist.\nI added the anchor songs to the playlist first, placing each one in the position where its valence most closely matched the wave.\nRemaining slots were filled with songs from a filtered pool of candidates that met 4 or more compatibility heuristics (see definitions above). I also restricted the decade of the song to 1980.\nFor each remaining position, I selected a song with a valence close to the target value. I allowed for a small tolerance to ensure flexibility in the song choice and still maintain the shape of the wave.\n\nThe code for implementing this is shown below.\n\n\nCode\n# Set seed for reproducibility\nset.seed(7)\nplaylist_length &lt;- 12\n\n# Generate cosine wave to model valence progression\nvalence_wave &lt;- (cos(seq(0, 2 * pi, length.out = playlist_length)) + 1) / 2\n\n# Define valence similarity tolerance\ntolerance &lt;- 0.05\n\n# Initialize empty playlist\nplaylist &lt;- data.frame()\n\n# Combine anchor songs and ensure uniqueness\nbase_songs &lt;- bind_rows(anchor_songs) |&gt; distinct(track_id, .keep_all = TRUE)\n\n# Select high-confidence candidate songs\ncandidates &lt;- combined_heuristics |&gt; \n  filter(heuristic_count &gt;= 4, decade == '1980s') |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\n# Create a vector of available positions\navailable_slots &lt;- 1:playlist_length\n\n# Place anchor songs into playlist by matching valence to wave\nfor (i in seq_len(nrow(base_songs))) {\n  song &lt;- base_songs[i, ]\n  if (length(available_slots) == 0) break\n  \n  diffs &lt;- abs(valence_wave[available_slots] - song$valence)\n  best_slot &lt;- available_slots[which.min(diffs)]\n  \n  playlist &lt;- bind_rows(playlist, song)\n  available_slots &lt;- setdiff(available_slots, best_slot)\n  candidates &lt;- candidates |&gt; filter(track_id != song$track_id)\n}\n\n# Fill remaining slots with best valence matches from candidates\nfor (slot in available_slots) {\n  target_valence &lt;- valence_wave[slot]\n  \n  pool &lt;- candidates |&gt; filter(abs(valence - target_valence) &lt;= tolerance)\n  \n  # Expand tolerance if needed\n  if (nrow(pool) == 0) {\n    message(glue::glue(\"Widening tolerance at slot {slot}\"))\n    pool &lt;- candidates |&gt; filter(abs(valence - target_valence) &lt;= (tolerance * 2))\n  }\n  \n  # Select one matching song if available\n  if (nrow(pool) &gt; 0) {\n    selected &lt;- sample_n(pool, 1)\n    playlist &lt;- bind_rows(playlist, selected)\n    candidates &lt;- candidates |&gt; filter(track_id != selected$track_id)\n  } else {\n    message(glue::glue(\"No song could be placed at slot {slot}\"))\n  }\n}\n\n# Add song position index\nplaylist &lt;- playlist |&gt; mutate(position = row_number())\n\n# Reshape for plotting\nplaylist_long &lt;- playlist |&gt; pivot_longer(cols = all_of(metrics), names_to = \"metric\", values_to = \"value\")\n\n# Plot how each metric changes across the playlist\nggplot(playlist_long, aes(x = position, y = value, group = metric, color = metric)) +\n  geom_line(size = 1) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free_y\") +\n  labs(\n    title = \"Evolution of Song Metrics Across Playlist\",\n    x = \"Playlist Order\", y = \"Metric Value\"\n  ) +\n  theme_minimal()\n\n\nThe final result is an 80s playlist designed to, at some points, make you feel like you’re about to hit 88 mph in Doc’s Delorean, and at others, like you’re slow-dancing alone at prom under a disco ball."
  },
  {
    "objectID": "mp03_wri.html#selecting-playlist-songs",
    "href": "mp03_wri.html#selecting-playlist-songs",
    "title": "The Ultimate Playlist:",
    "section": "Selecting Playlist Songs",
    "text": "Selecting Playlist Songs\nSince I wanted the playlist to have a deliberate “rise and fall,” in mood, I decided to methodically control the valence of the playlist. Specifically, I shaped the playlist so that the valence of the selected songs would follow the shape of a sine wave, and create an emotional rollercoaster for listeners to ride.\nTo implement this:\n\nI generated a cosine wave scaled between 0 and 1 to define the target valence levels for each of the 12 songs in the playlist.\nI added the anchor songs to the playlist first, placing each one in the position where its valence most closely matched the wave.\nRemaining slots were filled with songs from a filtered pool of candidates that met 3 or more compatibility heuristics (see definitions above).\nFor each remaining position, I selected a song with a valence close to the target value. I allowed for a small tolerance to ensure flexibility in the song choice and still maintain the shape of the wave.\n\nThe code for implementing this is shown below.\n\n\nCode\n# setting a random seed.\nset.seed(1631181)\nplaylist_length &lt;- 12\n\n# 1. Define wave (cosine values)\nvalence_wave &lt;- (cos(seq(0, 2 * pi, length.out = playlist_length)) + 1) / 2  # One full wave\n\n# 2. Set the tolerance for similarity\ntolerance &lt;- 0.05\n\n# 3. Initialize an empty data frame for the playlist\nplaylist &lt;- data.frame()\n\n# 4. Prepare the anchor songs (base songs)\nbase_songs &lt;- bind_rows(anchor_songs) |&gt; distinct(track_id, .keep_all = TRUE)\n\n# 5. Prepare candidates (distinct songs)\ncandidates &lt;- combined_heuristics |&gt; \n  filter(heuristic_count &gt;= 4) |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\ncandidates |&gt; kable()\n\n# 6. Initialize a vector of available slots (1 to 13)\navailable_slots &lt;- 1:playlist_length\n\n# 7. Place the anchor songs into the playlist following the valence wave\nfor (i in seq_len(nrow(base_songs))) {\n  song &lt;- base_songs[i, ]\n  \n  # If no available slots left, break\n  if (length(available_slots) == 0) break\n  \n  # Find the best slot for the song based on valence similarity\n  diffs &lt;- abs(valence_wave[available_slots] - song$valence)\n  best_slot_index &lt;- which.min(diffs)\n  \n  # Get the slot and place the song there\n  best_slot &lt;- available_slots[best_slot_index]\n  \n  # Add song to playlist\n  playlist &lt;- bind_rows(playlist, song)\n  \n  # Remove used slot and song\n  available_slots &lt;- setdiff(available_slots, best_slot)\n  candidates &lt;- candidates |&gt; filter(track_id != song$track_id)\n}\n\n# 8. Fill the remaining slots with candidates based on valence wave\nfor (slot in available_slots) {\n  target_valence &lt;- valence_wave[slot]\n  \n  # Find candidates within the tolerance of the target valence\n  pool &lt;- candidates |&gt; \n    filter(abs(valence - target_valence) &lt;= tolerance)\n  \n  # If no song found, widen the tolerance\n  if (nrow(pool) == 0) {\n    message(glue::glue(\"Widening tolerance at slot {slot}\"))\n    pool &lt;- candidates |&gt; \n      filter(abs(valence - target_valence) &lt;= (tolerance * 2))\n  }\n  \n  if (nrow(pool) &gt; 0) {\n    # Randomly sample one song\n    selected &lt;- sample_n(pool, 1)\n    \n    # Add the selected song to the playlist\n    playlist &lt;- bind_rows(playlist, selected)\n    \n    # Remove the chosen song from candidates\n    candidates &lt;- candidates |&gt; filter(track_id != selected$track_id)\n  } else {\n    message(glue::glue(\"No song could be placed at slot {slot}\"))\n  }\n}\n\n# 9. Add position/order column for plotting\nplaylist &lt;- playlist |&gt; mutate(position = row_number())\n\n# 10. Convert to long format for plotting\nplaylist_long &lt;- playlist |&gt; pivot_longer(cols = all_of(metrics), names_to = \"metric\", values_to = \"value\")\n\n# 11. Plot the evolution of the song metrics across the playlist\nggplot(playlist_long, aes(x = position, y = value, group = metric, color = metric)) +\n  geom_line(size = 1) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free_y\") +\n  labs(\n    title = \"Evolution of Song Metrics Across Playlist\",\n    x = \"Playlist Order\", y = \"Metric Value\"\n  ) +\n  theme_minimal()\n\nplaylist |&gt;\n  select(c(position), track_name, artist_name, album_name, year) |&gt;\n  rename(\n    \"Position\" = position,\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Year\" = year\n    ) |&gt;\n  kable()"
  },
  {
    "objectID": "mp03_wri.html#initial-exploration",
    "href": "mp03_wri.html#initial-exploration",
    "title": "The Ultimate Playlist",
    "section": "Initial Exploration",
    "text": "Initial Exploration\nOnce the data are squared away, I was able to complete a preliminary data analysis.\n\n\nCode\n# How many distinct tracks and artists are represented in the playlist data?\nfname &lt;- \"data/mp03/num_distinct.csv\"\nif (!file.exists(fname)){\n  num_distinct &lt;-\n    SPOTIFY_MILLIONS |&gt;\n      distinct(track_id) |&gt;\n      summarize(`Distinct Tracks` = n()) |&gt;\n      cross_join(SPOTIFY_MILLIONS |&gt;\n      distinct(artist_id) |&gt;\n      summarize(`Distinct Artists` = n()))\n  write.csv(num_distinct, fname, row.names = FALSE)\n} else{\n  num_distinct &lt;- readr::read_csv(fname)\n}\n  \nnum_distinct |&gt;\n  mutate(across(where(is.numeric), comma)) |&gt;\n  kable(caption = \"Table 1: Count of Distinct Tracks and Artists in the Playlist Dataset\")\n\n# What are the 5 most popular tracks in the playlist data?\nfname &lt;- \"data/mp03/most_pop_tracks.csv\"\nif (!file.exists(fname)){\n  most_pop_tracks &lt;-\n    SPOTIFY_MILLIONS |&gt;\n      add_count(track_id, name = \"track_count\") |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      slice_max(track_count, n=5)\n  write.csv(most_pop_tracks, fname, row.names = FALSE)\n} else{\n    most_pop_tracks &lt;- readr::read_csv(fname)\n  }\n\nmost_pop_tracks |&gt;\n  select(\n    track_name,\n    artist_name,\n    album_name,\n    track_count,\n  ) |&gt;\n  rename(\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Playlist Appearances\" = track_count,\n  ) |&gt;\n  mutate(across(where(is.numeric), comma)) |&gt;\n  kable(caption = \"Table 2: Top 5 Most Popular Tracks in the Playlist Data\")\n\n# What is the most popular track in the playlist data that does not have a corresponding entry in the \n# song characteristics data?\n\nfname &lt;- \"data/mp03/most_pop_not_songs.csv\"\nif (!file.exists(fname)){\n  most_pop_not_songs &lt;-\n    SPOTIFY_MILLIONS |&gt;\n      add_count(track_id, name = \"track_count\") |&gt;\n      anti_join(songs |&gt; select(id), join_by(track_id == id)) |&gt;\n      slice_max(track_count, n=1) |&gt;\n    distinct(track_id, .keep_all = TRUE)\n  write.csv(most_pop_not_songs, fname, row.names = FALSE)\n} else{\n  most_pop_not_songs &lt;- readr::read_csv(fname)\n}\nmost_pop_not_songs |&gt;\n  select(\n    track_name,\n    artist_name,\n    album_name,\n    track_count,\n  ) |&gt;\n  rename(\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Playlist Appearances\" = track_count,\n  ) |&gt;\n  kable(caption = \"Table 3: Most Popular Track in the Playlist Data Without a Corresponding Entry in the \nSong Characteristics Data\")\n\n# According to the song characteristics data, what is the most “danceable” track?\n# How often does it appear in a playlist?\n\nfname &lt;- \"data/mp03/most_danceable.csv\"\nif (!file.exists(fname)){\n  # Get the most danceable track\n  most_danceable &lt;- \n    songs |&gt;\n    select(id, name, artist, danceability) |&gt;\n    slice_max(danceability, n = 1)\n\n  # Join with playlist data and summarize appearances\n  most_danceable &lt;-\n    SPOTIFY_MILLIONS |&gt;\n    filter(track_id == most_danceable$id) |&gt;\n    select(track_id, album_name) |&gt;\n    mutate(appearances = n()) |&gt;\n    distinct(track_id, album_name, .keep_all = TRUE) |&gt;\n    inner_join(most_danceable, by = c(\"track_id\" = \"id\")) |&gt;\n    select(name, artist, album_name, danceability, appearances)\n  write.csv(most_danceable, fname, row.names = FALSE)\n} else{\n  most_danceable &lt;- readr::read_csv(fname)\n}\n# Display as kable with caption\nmost_danceable |&gt;\n  rename(\n    `Track Name` = name,\n    `Artist Name` = artist,\n    `Album Name` = album_name,\n    `Danceability` = danceability,\n    `Playlist Appearances` = appearances\n    ) |&gt;\n  kable(caption = \"Table 4: Most Danceable Song and Its Playlist Appearances\")\n\n# Which playlist has the longest average track length?\nfname &lt;- \"data/mp03/longest_avg_track_length.csv\"\n\nif (!file.exists(fname)) {\n  longest_avg_track_length &lt;-\n    SPOTIFY_MILLIONS |&gt; \n    select(playlist_id, playlist_name, duration) |&gt;\n    group_by(playlist_id) |&gt;\n    summarize(\n      `Longest Average Track Length (s)` = mean(duration / 1000, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt;\n    slice_max(`Longest Average Track Length (s)`, n = 1) |&gt;\n    left_join(\n      SPOTIFY_MILLIONS |&gt; select(playlist_id, playlist_name) |&gt; distinct(),\n      by = \"playlist_id\"\n    ) |&gt;\n    relocate(playlist_name, .after = playlist_id)\n  write.csv(longest_avg_track_length, fname, row.names = FALSE)\n} else {\n  longest_avg_track_length &lt;- readr::read_csv(fname)\n}\n\nlongest_avg_track_length |&gt;\n  mutate(`Longest Average Track Length (s)` = comma(`Longest Average Track Length (s)`)) |&gt;\n  rename(\n    `Playlist ID` = playlist_id,\n    `Playlist Name` = playlist_name\n    ) |&gt;\n  kable(caption = \"Table 5: Playlist with the Longest Average Track Length\")\n\n# What is the most popular playlist on Spotify?\nfname &lt;- \"data/mp03/most_pop_playlist.csv\"\nif (!file.exists(fname)){\n  most_pop_playlist &lt;-\n    SPOTIFY_MILLIONS |&gt;\n    select(playlist_id, playlist_name, playlist_followers) |&gt;\n    slice_max(playlist_followers, n=1) |&gt;\n    distinct(playlist_id, .keep_all = TRUE)\n  write.csv(most_pop_playlist, fname, row.names = FALSE)\n} else{\n    most_pop_playlist &lt;- readr::read_csv(fname)\n  }\n  \nmost_pop_playlist |&gt;\n  mutate(playlist_followers = comma(playlist_followers)) |&gt;\n  rename(\n    `Playlist ID` = playlist_id,\n    `Playlist Name` = playlist_name,\n    `Playlist Followers` = playlist_followers\n    ) |&gt;\n  kable(caption = \"Table 6: Most Popular Playlist\")\n\n\n\nIdentifying Characteristics of Popular Songs\n\n\nJoining the Data Sets\nI joined the playlist and song characteristic data sets as in the code below. The Spotify playlist data and the song characteristics data was subsequently deleted to free up RAM.\n\n\nCode\nmerged_fpath &lt;- \"data/mp03/spotify_millions_songs_join.csv\"\nif (!file.exists(merged_fpath)){\n  spotify_millions_songs &lt;- \n    SPOTIFY_MILLIONS |&gt;\n    inner_join(songs, join_by(track_id == id, track_name == name, artist_name == artist, duration == duration_ms)) |&gt;\n    mutate(decade = paste0((year %/% 10) * 10, \"s\"))\n  write.csv(spotify_millions_songs, merged_fpath, row.names = FALSE)\n} else{\n  spotify_millions_songs &lt;- readr::read_csv(merged_fpath)\n}\nif (exists(SPOTIFY_MILLIONS) & exists(songs)){\n  rm(SPOTIFY_MILLIONS, songs)\n}"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "",
    "text": "I hereby nominate this playlist for the Internet’s Best Playlist award. Valence Synth Wave was intentionally crafted to optimize an emotional build-up and release by ordering the songs so that their valence (a measure of musical positivity) follows a cosine wave. This approach was algorithmically driven to create an emotional arc that feels both natural and dynamic.\nThe figure below visualizes how various song characteristics evolve across the playlist. There is a clear fluctuation across the different characteristics–guaranteeing an emotionally diverse listening experience.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosition\nTrack Name\nArtist Name\nAlbum Name\nYear\n\n\n\n\n1\nForever Young\nAlphaville\nForever Young\n1984\n\n\n2\nThe Blue Sky\na-ha\nHunting High And Low\n1985\n\n\n3\nWhip It\nDEVO\nFreedom Of Choice\n1980\n\n\n4\nLove Plus One\nHaircut 100\nPelican West Plus\n1982\n\n\n5\nLonely Ol’ Night\nJohn Mellencamp\nScarecrow\n1985\n\n\n6\nWhat You Need\nINXS\nListen Like Thieves\n1985\n\n\n7\nI Drink Alone\nGeorge Thorogood & The Destroyers\nMaverick\n1985\n\n\n8\nI Need Someone Like Me\nGeorge Strait\nDoes Fort Worth Ever Cross Your Mind\n1984\n\n\n9\nMusic For A Found Harmonium - 2008 Digital Remaster\nPenguin Cafe Orchestra\nBroadcasting From Home\n1984\n\n\n10\nFlash - Single Version\nQueen\nFlash Gordon\n1980\n\n\n11\nI Love You Babe\nBabyface\nLovers\n1986\n\n\n12\nStrut\nSheena Easton\nA Private Heaven [Bonus Tracks Version]\n1984\n\n\n\n\n\n\n\n\nValence Synth Wave is a nod to the range of sentimentality that only the 80s could deliver—a time when synthesizers carried both dreams and dread. The opening song, “Forever Young” by Alphaville, sets a somber mood, with lead singer Marian Gold reflecting on the looming threat of nuclear annihilation haunting his generation, “Hoping for the best, but expecting the worst / Are you gonna drop the bomb or not?” This existential dread gives way to something more personal in a-ha’s “The Blue Sky,” where we find ourselves at a coffee shop mourning a lost love. A sensitive and forlorn Morten Harket laments, “It doesn’t seem like this / Blue sky’s here for me.”\nWe pivot into something more energetic with “Whip It” by DEVO, a nonsensical but peppy call to arms for personal empowerment. “Love Plus One” by Haircut 100 delves deeper into playful absurdity, hinting at the sun-soaked tropics with bongos and xylophones. This is followed by John Mellencamp’s “Lonely Ol’ Night,” a Springsteen-esque rock ballad about seeking fleeting comfort in the face of isolation. “It’s a lonely ol’ night / Can I put my arms around you?” Mellencamp pleads. Meanwhile, INXS’s “What You Need” snaps us out of the melancholy with swagger and urgency with a thumping bass-line and slashes of saxophone.\nThe mood darkens with George Thorogood & The Destroyers’ “I Drink Alone”—an edgy, bluesy anthem of solitude that doubles as a cry for help. Thorogood delivers each line with such boozy conviction, that you can practically smell the whiskey on his breath. Then George Strait’s “I Need Someone Like Me” shifts that loneliness to a country register.\nWe are then plunged into a simmering tension that gently unravels into a disarmingly playful, jig-like melody in Penguin Cafe Orchestra’s “Music For A Found Harmonium.” A flash-bang startles us out of that folksy daydream and straight into a comic book panel during Queen’s “Flash”. “I Love You Babe” by Babyface follows with warmth and sincerity, an earnest slow jam that finally says what it means. We close on a high note with Sheena Easton’s “Strut,” a fiercely confident song that dares anyone to look away.\nClick play to start an emotionally engineered ride through the 1980s!\n\n\n\n\n \n\n\n\n1"
  },
  {
    "objectID": "mp03.html#contents",
    "href": "mp03.html#contents",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "Position\nTrack Name\nArtist Name\nAlbum Name\nYear\n\n\n\n\n1\nForever Young\nAlphaville\nForever Young\n1984\n\n\n2\nThe Blue Sky\na-ha\nHunting High And Low\n1985\n\n\n3\nWhip It\nDEVO\nFreedom Of Choice\n1980\n\n\n4\nLove Plus One\nHaircut 100\nPelican West Plus\n1982\n\n\n5\nLonely Ol’ Night\nJohn Mellencamp\nScarecrow\n1985\n\n\n6\nWhat You Need\nINXS\nListen Like Thieves\n1985\n\n\n7\nI Drink Alone\nGeorge Thorogood & The Destroyers\nMaverick\n1985\n\n\n8\nI Need Someone Like Me\nGeorge Strait\nDoes Fort Worth Ever Cross Your Mind\n1984\n\n\n9\nMusic For A Found Harmonium - 2008 Digital Remaster\nPenguin Cafe Orchestra\nBroadcasting From Home\n1984\n\n\n10\nFlash - Single Version\nQueen\nFlash Gordon\n1980\n\n\n11\nI Love You Babe\nBabyface\nLovers\n1986\n\n\n12\nStrut\nSheena Easton\nA Private Heaven [Bonus Tracks Version]\n1984"
  },
  {
    "objectID": "mp03.html#description",
    "href": "mp03.html#description",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "",
    "text": "Valence Synth Wave is a nod to the range of sentimentality that only the 80s could deliver—a time when synthesizers carried both dreams and dread. The opening song, “Forever Young” by Alphaville, sets a somber mood, with lead singer Marian Gold reflecting on the looming threat of nuclear annihilation haunting his generation, “Hoping for the best, but expecting the worst / Are you gonna drop the bomb or not?” This existential dread gives way to something more personal in a-ha’s “The Blue Sky,” where we find ourselves at a coffee shop mourning a lost love. A sensitive and forlorn Morten Harket laments, “It doesn’t seem like this / Blue sky’s here for me.”\nWe pivot into something more energetic with “Whip It” by DEVO, a nonsensical but peppy call to arms for personal empowerment. “Love Plus One” by Haircut 100 delves deeper into playful absurdity, hinting at the sun-soaked tropics with bongos and xylophones. This is followed by John Mellencamp’s “Lonely Ol’ Night,” a Springsteen-esque rock ballad about seeking fleeting comfort in the face of isolation. “It’s a lonely ol’ night / Can I put my arms around you?” Mellencamp pleads. Meanwhile, INXS’s “What You Need” snaps us out of the melancholy with swagger and urgency with a thumping bass-line and slashes of saxophone.\nThe mood darkens with George Thorogood & The Destroyers’ “I Drink Alone”—an edgy, bluesy anthem of solitude that doubles as a cry for help. Thorogood delivers each line with such boozy conviction, that you can practically smell the whiskey on his breath. Then George Strait’s “I Need Someone Like Me” shifts that loneliness to a country register.\nWe are then plunged into a simmering tension that gently unravels into a disarmingly playful, jig-like melody in Penguin Cafe Orchestra’s “Music For A Found Harmonium.” A flash-bang startles us out of that folksy daydream and straight into a comic book panel during Queen’s “Flash”. “I Love You Babe” by Babyface follows with warmth and sincerity, an earnest slow jam that finally says what it means. We close on a high note with Sheena Easton’s “Strut,” a fiercely confident song that dares anyone to look away.\nClick play to start an emotionally engineered ride through the 1980s!\n\n\n\n\n \n\n\n\n1"
  },
  {
    "objectID": "mp03.html#song-characteristics-dataset",
    "href": "mp03.html#song-characteristics-dataset",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "Song Characteristics Dataset",
    "text": "Song Characteristics Dataset\nThe song characteristics data set was downloaded from this URL, using the code below:\n\n\nCode\n# Imports\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(scales)\n# constants\nMINTY &lt;- \"#78c2ad\"\n\n#' Load Spotify song data\n#'\n#' This function checks for the existence of a local CSV file containing Spotify song data.\n#' If the file does not exist, it downloads it from a remote URL. The function then reads and returns the data as a tibble.\n#'\n#' @return A tibble containing Spotify song analytics data.\nload_songs &lt;- function() {\n  directory &lt;- \"data/mp03/\"\n  file_name &lt;- paste0(directory, \"spotify_song_analytics.csv\")\n  \n  # Create the data directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  # Download the data file if it's not already present\n  if (!file.exists(file_name)) {\n    download.file(\n      url = \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_name,\n      method = \"auto\"\n    )\n  }\n  \n  # Read the CSV into a tibble\n  songs &lt;- readr::read_csv(file_name)\n  return(songs)\n}\n\n# Load songs into a dataframe\nsongs &lt;- load_songs()\n\n\nA small amount of clean up was required for this data set. The code used to process the raw data set is presented below.\n\n\nCode\n# Instructor provided code for cleaning artist string\nclean_artist_string &lt;- function(x){\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\n# Split songs with multiple artists into separate rows and clean artist names\nsongs &lt;-\n  songs |&gt; \n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)"
  },
  {
    "objectID": "mp03.html#playlists-dataset",
    "href": "mp03.html#playlists-dataset",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "Playlists Dataset",
    "text": "Playlists Dataset\nThe playlists data set was downloaded from this URL, using the code outlined below:\n\n\nCode\n#' Load Spotify playlist data\n#'\n#' This function downloads and/or reads the Million Playlist Dataset slices from \n#' a GitHub repository. If `read_only = TRUE`, it skips downloading and only reads \n#' existing local files.\n#'\n#' @param read_only Logical. If TRUE, skips download and only loads local data. Default is FALSE.\n#' @return A list of parsed JSON objects representing playlist data.\nload_playlists &lt;- function(read_only = FALSE) {\n  source_root_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  directory &lt;- \"data/mp03/mpd/\"\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  if (!read_only) {\n    # Loop over all playlist slice file names\n    for (i in seq(0, 999000, by = 1000)) {\n      fname &lt;- paste0(\n        \"mpd.slice.\",\n        format(i, scientific = FALSE), \"-\",\n        format(i + 999, scientific = FALSE),\n        \".json\"\n      )\n      fpath &lt;- file.path(directory, fname)\n      \n      # Download file if it's not already present\n      if (!file.exists(fpath)) {\n        Sys.sleep(0.01)  # Avoid hitting rate limits\n        tryCatch({\n          invisible(download.file(\n            url = paste0(source_root_url, fname),\n            destfile = fpath,\n            method = \"auto\",\n            quiet = TRUE\n          ))\n        }, error = function(e) {\n          message(\"Download failed: \", fname)\n        })\n      }\n    }\n  }\n  \n  # Read all the JSON files in the directory\n  json_files &lt;- list.files(directory, pattern = \"\\\\.json$\", full.names = TRUE)\n  all_data &lt;- vector(\"list\", length(json_files))\n  \n  for (i in seq_along(json_files)) {\n    tryCatch({\n      all_data[[i]] &lt;- suppressWarnings(jsonlite::fromJSON(json_files[i]))\n    }, error = function(e) {\n      message(\"Failed to read: \", json_files[i])\n    })\n  }\n  \n  return(all_data)\n}\n\nSPOTIFY_MILLIONS &lt;- load_playlists()\n\n\nSince the playlist data set was hierarchical, it required “rectangling,” to be adapted for use in this analysis. This process was accomplished with the following code.\n\n\nCode\n#' Process a Spotify playlist slice\n#'\n#' This function processes a single playlist slice from the Million Playlist Dataset,\n#' extracting track information and saving it as a CSV file for faster future loading.\n#' It strips Spotify URI prefixes and appends playlist metadata.\n#'\n#' @param playlist A list object representing one slice of the Spotify playlist data (parsed JSON).\n#' @return A data frame (tibble) of processed track data with playlist-level metadata.\nprocess &lt;- function(playlist) {\n  \n  # Instructor provided helper function to strip Spotify URI prefix\n  strip_spotify_prefix &lt;- function(x) {\n    str_replace(x, \".*:.*:\", \"\")\n  }\n  \n  # Create the output directory if it doesn't exist\n  output_dir &lt;- \"data/mp03/processed\"\n  if (!dir.exists(output_dir)){\n    dir.create(output_dir, showWarnings = FALSE)\n  }\n  \n  # Generate filename for processed slice\n  slice_range &lt;- playlist$info$slice\n  csv_outfile &lt;- paste0(output_dir, \"/mpd.slice.\", slice_range, \".csv\")\n  \n  # If CSV already exists, load it instead of reprocessing\n  if (file.exists(csv_outfile)){\n    all_tracks &lt;- readr::read_csv(csv_outfile)\n  } else {\n    all_tracks &lt;- data.frame()  # Placeholder for all processed tracks\n    \n    for (i in seq_along(playlist$playlists$tracks)) {\n      tracks_df &lt;- as.data.frame(playlist$playlists$tracks[[i]])\n      \n      # Skip empty playlists\n      if (nrow(tracks_df) == 0) next\n      \n      # Add metadata and clean up Spotify URIs\n      tracks_df &lt;- tracks_df |&gt;\n        mutate(\n          playlist_name = playlist$playlists$name[i],\n          playlist_id = playlist$playlists$pid[i],\n          playlist_followers = playlist$playlists$num_followers[i],\n          track_uri = strip_spotify_prefix(track_uri),\n          artist_uri = strip_spotify_prefix(artist_uri),\n          album_uri = strip_spotify_prefix(album_uri)\n        )\n      \n      # Combine with all previously processed tracks\n      all_tracks &lt;- bind_rows(all_tracks, tracks_df)\n    }\n    \n    # Save the processed data for this slice\n    write.csv(all_tracks, csv_outfile, row.names = FALSE)\n    message(\"Saved: \", csv_outfile)\n  }\n  return(all_tracks)\n}\n\n#' Merge all processed playlist slices into a single dataset\n#'\n#' This function checks if a master CSV file (`spotify_millions.csv`) already exists.\n#' If not, it processes all playlist JSON slices provided and merges them into one\n#' unified dataframe, then saves it as a CSV for future use. If the file already exists,\n#' it loads the data directly.\n#'\n#' @param data A list of playlist slices (parsed JSON). Defaults to `master_json`.\n#' @return A dataframe containing the merged Spotify playlist data.\nmerge_processed_playlists &lt;- function(data = master_json) {\n  # Path to the master output CSV\n  spotify_millions_fpath &lt;- \"data/mp03/spotify_millions.csv\"\n  \n  if (!file.exists(spotify_millions_fpath)) {\n    # If no master file exists, process each playlist slice\n    playlists &lt;- data.frame()  # Initialize an empty dataframe\n    \n    for (i in seq_along(master_json)) {\n      playlist &lt;- master_json[[i]]\n      processed_df &lt;- process(playlist)  # Process individual slice\n      playlists &lt;- bind_rows(playlists, processed_df)  # Append to master dataframe\n    }\n\n    # Clean up and standardize column names\n    playlists &lt;- playlists |&gt; \n      rename(\n        playlist_position = `pos`,\n        track_id = `track_uri`,\n        artist_id = `artist_uri`,\n        album_id = `album_uri`,\n        duration = `duration_ms`\n      ) |&gt;\n      select(!`...1`)  # Drop the unnamed index column\n\n    # Save merged data to CSV\n    write.csv(playlists, spotify_millions_fpath, row.names = FALSE)\n  } else {\n    # If file exists, just read it\n    playlists &lt;- readr::read_csv(spotify_millions_fpath)\n  }\n\n  return(playlists)\n}\n\n# Load or process the Spotify Millions json dataset\nSPOTIFY_MILLIONS &lt;- merge_processed_playlists(SPOTIFY_MILLIONS)"
  },
  {
    "objectID": "mp03.html#initial-exploration",
    "href": "mp03.html#initial-exploration",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "Initial Exploration",
    "text": "Initial Exploration\nOnce the data were squared away, I completed an instructor-led preliminary data analysis, the results of which are tabulated below. First, I compute the total number of distinct tracks and artists contained in this data set.\n\n\nCode\n# How many distinct tracks and artists are represented in the playlist data?\nfname &lt;- \"data/mp03/num_distinct.csv\"\nif (!file.exists(fname)){\n  num_distinct &lt;-\n    SPOTIFY_MILLIONS |&gt;\n      distinct(track_id) |&gt;\n      summarize(`Distinct Tracks` = n()) |&gt;\n      cross_join(SPOTIFY_MILLIONS |&gt;\n      distinct(artist_id) |&gt;\n      summarize(`Distinct Artists` = n()))\n  write.csv(num_distinct, fname, row.names = FALSE)\n} else{\n  num_distinct &lt;- readr::read_csv(fname)\n}\n  \nnum_distinct |&gt;\n  mutate(across(where(is.numeric), comma)) |&gt;\n  kable(caption = \"Table 1: Count of Distinct Tracks and Artists in the Playlist Dataset\")\n\n\n\nTable 1: Count of Distinct Tracks and Artists in the Playlist Dataset\n\n\nDistinct Tracks\nDistinct Artists\n\n\n\n\n1,200,590\n173,604\n\n\n\n\n\nNext, I identify the 5 most popular tracks in the playlist data.\n\n\nCode\n# What are the 5 most popular tracks in the playlist data?\nfname &lt;- \"data/mp03/most_pop_tracks.csv\"\nif (!file.exists(fname)){\n  most_pop_tracks &lt;-\n    SPOTIFY_MILLIONS |&gt;\n      add_count(track_id, name = \"track_count\") |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      slice_max(track_count, n=5)\n  write.csv(most_pop_tracks, fname, row.names = FALSE)\n} else{\n    most_pop_tracks &lt;- readr::read_csv(fname)\n  }\n\nmost_pop_tracks |&gt;\n  select(\n    track_name,\n    artist_name,\n    album_name,\n    track_count,\n  ) |&gt;\n  rename(\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Playlist Appearances\" = track_count,\n  ) |&gt;\n  mutate(across(where(is.numeric), comma)) |&gt;\n  kable(caption = \"Table 2: Top 5 Most Popular Tracks in the Playlist Data\")\n\n\n\nTable 2: Top 5 Most Popular Tracks in the Playlist Data\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nAlbum Name\nPlaylist Appearances\n\n\n\n\nHUMBLE.\nKendrick Lamar\nDAMN.\n13,314\n\n\nOne Dance\nDrake\nViews\n12,179\n\n\nBroccoli (feat. Lil Yachty)\nDRAM\nBig Baby DRAM\n11,845\n\n\nCloser\nThe Chainsmokers\nCloser\n11,656\n\n\nCongratulations\nPost Malone\nStoney\n11,310\n\n\n\n\n\nSubsequently, I determine the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data set.\n\n\nCode\n# What is the most popular track in the playlist data that does not have a corresponding entry in the \n# song characteristics data?\n\nfname &lt;- \"data/mp03/most_pop_not_songs.csv\"\nif (!file.exists(fname)){\n  most_pop_not_songs &lt;-\n    SPOTIFY_MILLIONS |&gt;\n      add_count(track_id, name = \"track_count\") |&gt;\n      anti_join(songs |&gt; select(id), join_by(track_id == id)) |&gt;\n      slice_max(track_count, n=1) |&gt;\n    distinct(track_id, .keep_all = TRUE)\n  write.csv(most_pop_not_songs, fname, row.names = FALSE)\n} else{\n  most_pop_not_songs &lt;- readr::read_csv(fname)\n}\nmost_pop_not_songs |&gt;\n  select(\n    track_name,\n    artist_name,\n    album_name,\n    track_count,\n  ) |&gt;\n  rename(\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Playlist Appearances\" = track_count,\n  ) |&gt;\n  kable(caption = \"Table 3: Most Popular Track in the Playlist Data Without a Corresponding Entry in the \nSong Characteristics Data\")\n\n\n\nTable 3: Most Popular Track in the Playlist Data Without a Corresponding Entry in the Song Characteristics Data\n\n\nTrack Name\nArtist Name\nAlbum Name\nPlaylist Appearances\n\n\n\n\nOne Dance\nDrake\nViews\n12179\n\n\n\n\n\nI now look at the most danceable track in the playlist data, and count the number of times it appears in a playlist.\n\n\nCode\n# According to the song characteristics data, what is the most “danceable” track?\n# How often does it appear in a playlist?\n\nfname &lt;- \"data/mp03/most_danceable.csv\"\nif (!file.exists(fname)){\n  # Get the most danceable track\n  most_danceable &lt;- \n    songs |&gt;\n    select(id, name, artist, danceability) |&gt;\n    slice_max(danceability, n = 1)\n\n  # Join with playlist data and summarize appearances\n  most_danceable &lt;-\n    SPOTIFY_MILLIONS |&gt;\n    filter(track_id == most_danceable$id) |&gt;\n    select(track_id, album_name) |&gt;\n    mutate(appearances = n()) |&gt;\n    distinct(track_id, album_name, .keep_all = TRUE) |&gt;\n    inner_join(most_danceable, by = c(\"track_id\" = \"id\")) |&gt;\n    select(name, artist, album_name, danceability, appearances)\n  write.csv(most_danceable, fname, row.names = FALSE)\n} else{\n  most_danceable &lt;- readr::read_csv(fname)\n}\n# Display as kable with caption\nmost_danceable |&gt;\n  rename(\n    `Track Name` = name,\n    `Artist Name` = artist,\n    `Album Name` = album_name,\n    `Danceability` = danceability,\n    `Playlist Appearances` = appearances\n    ) |&gt;\n  kable(caption = \"Table 4: Most Danceable Song and Its Playlist Appearances\")\n\n\n\nTable 4: Most Danceable Song and Its Playlist Appearances\n\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nAlbum Name\nDanceability\nPlaylist Appearances\n\n\n\n\nFunky Cold Medina\nTone-Loc\nLoc-ed After Dark\n0.988\n211\n\n\n\n\n\nThen, I locate the playlist with the longest average track length.\n\n\nCode\n# Which playlist has the longest average track length?\nfname &lt;- \"data/mp03/longest_avg_track_length.csv\"\n\nif (!file.exists(fname)) {\n  longest_avg_track_length &lt;-\n    SPOTIFY_MILLIONS |&gt; \n    select(playlist_id, playlist_name, duration) |&gt;\n    group_by(playlist_id) |&gt;\n    summarize(\n      `Longest Average Track Length (s)` = mean(duration / 1000, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt;\n    slice_max(`Longest Average Track Length (s)`, n = 1) |&gt;\n    left_join(\n      SPOTIFY_MILLIONS |&gt; select(playlist_id, playlist_name) |&gt; distinct(),\n      by = \"playlist_id\"\n    ) |&gt;\n    relocate(playlist_name, .after = playlist_id)\n  write.csv(longest_avg_track_length, fname, row.names = FALSE)\n} else {\n  longest_avg_track_length &lt;- readr::read_csv(fname)\n}\n\nlongest_avg_track_length |&gt;\n  mutate(`Longest Average Track Length (s)` = comma(`Longest Average Track Length (s)`)) |&gt;\n  rename(\n    `Playlist ID` = playlist_id,\n    `Playlist Name` = playlist_name\n    ) |&gt;\n  kable(caption = \"Table 5: Playlist with the Longest Average Track Length\")\n\n\n\nTable 5: Playlist with the Longest Average Track Length\n\n\nPlaylist ID\nPlaylist Name\nLongest Average Track Length (s)\n\n\n\n\n462471\nMixes\n3,869\n\n\n\n\n\nFinally, I identify the most popular playlist in the playlist data set.\n\n\nCode\n# What is the most popular playlist on Spotify?\nfname &lt;- \"data/mp03/most_pop_playlist.csv\"\nif (!file.exists(fname)){\n  most_pop_playlist &lt;-\n    SPOTIFY_MILLIONS |&gt;\n    select(playlist_id, playlist_name, playlist_followers) |&gt;\n    slice_max(playlist_followers, n=1) |&gt;\n    distinct(playlist_id, .keep_all = TRUE)\n  write.csv(most_pop_playlist, fname, row.names = FALSE)\n} else{\n    most_pop_playlist &lt;- readr::read_csv(fname)\n  }\n  \nmost_pop_playlist |&gt;\n  mutate(playlist_followers = comma(playlist_followers)) |&gt;\n  rename(\n    `Playlist ID` = playlist_id,\n    `Playlist Name` = playlist_name,\n    `Playlist Followers` = playlist_followers\n    ) |&gt;\n  kable(caption = \"Table 6: Most Popular Playlist\")\n\n\n\nTable 6: Most Popular Playlist\n\n\nPlaylist ID\nPlaylist Name\nPlaylist Followers\n\n\n\n\n746359\nBreaking Bad\n53,519\n\n\n\n\n\n\nIdentifying the Characteristics of Popular Songs\n\nJoining the Data Sets\nI joined the playlist and song characteristic data sets as in the code below. The Spotify playlist data and the song characteristics data was subsequently deleted to free up RAM.\n\n\nCode\nmerged_fpath &lt;- \"data/mp03/spotify_millions_songs_join.csv\"\nif (!file.exists(merged_fpath)){\n  spotify_millions_songs &lt;- \n    SPOTIFY_MILLIONS |&gt;\n    inner_join(songs, join_by(track_id == id, track_name == name, artist_name == artist, duration == duration_ms)) |&gt;\n    mutate(decade = paste0((year %/% 10) * 10, \"s\"))\n  write.csv(spotify_millions_songs, merged_fpath, row.names = FALSE)\n} else{\n  spotify_millions_songs &lt;- readr::read_csv(merged_fpath)\n}\nif (exists(SPOTIFY_MILLIONS) & exists(songs)){\n  rm(SPOTIFY_MILLIONS, songs)\n}\n\n\n\n\nFurther Data Exploration\nA brief, instructor-led analysis of the playlist data set is included below. To explore whether more popular songs tend to appear in more playlists, a linear regression was performed using popularity as the predictor and the log-transformed count of playlist appearances as the outcome. The figure below illustrates this relationship. The model shows a fairly strong positive association, with a regression equation of \\(y = -1.25 + 0.10x\\), a correlation of \\(r = 0.77\\), and a coefficient of determination \\(R^2 = 0.59\\). This suggests that popularity is a reasonably good predictor of how often a song shows up in playlists.\n\n\nCode\n# Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n# Prep the data\ndata &lt;- spotify_millions_songs |&gt;\n  add_count(track_id, name = \"playlist_appearances\") |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  mutate(log_appearances = log1p(playlist_appearances))\n\n# Fit the model\nmodel &lt;- lm(log_appearances ~ popularity, data = data)\n\n# Get model metrics\nmodel_eq &lt;- broom::tidy(model)\nr_squared &lt;- broom::glance(model)$r.squared\ncorrelation &lt;- cor(data$popularity, data$log_appearances, use = \"complete.obs\")\n\n# Build the equation text\neq_text &lt;- paste0(\n  \"y = \", round(model_eq$estimate[1], 2), \" + \",\n  round(model_eq$estimate[2], 3), \"x\\n\",\n  \"r = \", round(correlation, 2), \n  \" | R² = \", round(r_squared, 2)\n)\n\n# Plot\nggplot(data, aes(x = popularity, y = log_appearances)) +\n  geom_jitter(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_point(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  annotate(\"text\", x = 12.75, y = -1,\n           label = eq_text, hjust = 0, size = 4, fontface = \"italic\") +\n  labs(\n    title = \"Figure 1: Relationship Between Popularity and Playlist Appearances\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Popularity\",\n    y = \"Log(Playlist Appearances)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nMedian track popularity by year is presented in the figure below. The general trend is that track popularity is higher with release year. Notably, 2017 stands out as the year with the highest median song popularity.\n\n\nCode\n# In what year were the most popular songs released?\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  group_by(year) |&gt;\n  summarize(median_popularity = median(popularity, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = median_popularity)) +\n  geom_line(color = MINTY, linewidth = 1.2) +\n  geom_point(color = MINTY, size = 2) +\n  scale_x_continuous(\n    breaks = seq(min(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 max(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 by = 10)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Median Track Popularity\",\n    title = \"Median Track Popularity by Year\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhile median song popularity continues to keep up with the times, median track dance-ability peaked in 1929. The figure below presents the evolution of track dance ability over the past 90 years.\n\n\nCode\n# In what year did danceability peak?\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  group_by(year) |&gt;\n  summarize(median_danceability = median(danceability, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = median_danceability)) +\n  geom_line(color = MINTY, linewidth = 1.2) +\n  geom_point(color = MINTY, size = 2) +\n  scale_x_continuous(\n    breaks = seq(min(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 max(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 by = 10)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Median Track Danceability\",\n    title = \"Median Track Danceability by Year\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal() +\n  transition_reveal(year) +\n  ease_aes('linear') -&gt; animated_plot\n\nanimate(animated_plot)\n\n\n\n\n\n\n\n\n\nThe most represented decade on user playlists is the 2010s. The bar chart below presents the distribution of tracks belonging to a particular decade.\n\n\nCode\n# Which decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\n\nspotify_millions_songs |&gt;\n  mutate(decade = paste0((year %/% 10) * 10, \"s\")) |&gt;\n  group_by(decade) |&gt;\n  summarize(decade_count = n()/10000) |&gt;\n  ggplot(aes(x = decade_count, y = factor(decade))) +\n  geom_bar(stat = \"identity\", fill = MINTY) +\n  labs(\n    x = \"Number of Tracks (in tens of thousands)\",\n    y = \"Decade\",\n    title = \"Decade Representation in Playlists\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nThe most popular musical key is G, as can be seen in the figure of key distribution frequency below.\n\n\nCode\n# Create the key names\nkey_names &lt;- c(\n  \"C\", \"C♯/Db\", \"D\", \"D♯/Eb\", \"E\", \"F\",\n  \"F♯/Gb\", \"G\", \"G♯/Ab\", \"A\", \"A♯/Bb\", \"B\"\n)\n\n# Count frequency per key and label with musical key names\ndata &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  count(key, name = \"key_frequency\") |&gt;\n  mutate(key_label = factor(key_names[key + 1], levels = key_names))\n\n# Create label positions at fixed theta, variable radius\ny_values &lt;- seq(1000, max(data$key_frequency), by = 1000)\n\nradial_labels &lt;- tibble(\n  x = rep(\"D♯/Eb\", length(y_values)),  # Repeat \"C\" the same number of times as y values\n  y = y_values,\n  label = as.character(y_values)\n)\n\n# Plot with polar coordinates and annotations\nggplot(data, aes(x = key_label, y = key_frequency, fill = key_label)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(start = 0) +\n  geom_text(data = radial_labels, aes(x = x, y = y, label = label),\n            inherit.aes = FALSE, size = 3, color = \"gray20\", hjust = -0.2) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of Musical Keys\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\nAs shown in the figure below, there does not appear to be a strong preference for any particular track length, with short, medium, and long songs appearing at roughly equal frequencies. Length categories are defined by terciles: short songs are under 199.08 seconds (\\(33^{rd}\\) percentile), medium songs fall between 199.08 and 253.4 seconds (\\(33^{rd}\\)–\\(66^{th}\\) percentile), and long songs exceed 253.4 seconds.\n\n\nCode\n# What are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\n\nduration_percentiles &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  filter(!is.na(duration)) |&gt;\n  mutate(duration_sec = duration / 1000) |&gt;\n  summarize(\n    p33 = quantile(duration_sec, 0.33),\n    p66 = quantile(duration_sec, 0.66)\n  )\n\n# Pull out threshold values\np33 &lt;- duration_percentiles$p33\np66 &lt;- duration_percentiles$p66\n\n# Categorize and plot\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  filter(!is.na(duration)) |&gt;\n  mutate(\n    duration_sec = duration / 1000,\n    duration_category = case_when(\n      duration_sec &lt;= p33 ~ \"Short\",\n      duration_sec &lt;= p66 ~ \"Medium\",\n      TRUE ~ \"Long\"\n    )\n  ) |&gt;\n  count(duration_category) |&gt;\n  ggplot(aes(x = duration_category, y = n, fill = duration_category)) +\n  geom_col() +\n  labs(\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    title = \"Song Duration Categories\",\n    x = \"Duration Category\",\n    y = \"Number of Unique Tracks\"\n  ) +\n  scale_y_continuous(labels = comma_format()) +\n  scale_fill_manual(\n    name = \"Duration Category\",\n    values = c(\"Short\" = MINTY, \"Medium\" = \"#fdd835\", \"Long\" = \"#ef5350\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nA linear regression analysis reveals a very weak positive relationship between danceability and popularity, with a coefficient of determination (\\(R^2\\)) of just 0.03. This suggests that only 3% of the variability in popularity can be explained by danceability. The scatter plot below illustrates this weak association.\n\n\nCode\n# Does danceability correlate with popularity?\ndata &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE)\n# Fit the model\nmodel &lt;- lm(danceability ~ popularity, data = data)\n\n# Get model metrics\nmodel_eq &lt;- broom::tidy(model)\nr_squared &lt;- broom::glance(model)$r.squared\ncorrelation &lt;- cor(data$popularity, data$danceability, use = \"complete.obs\")\n\n# set up annotation\neq_text &lt;- paste0(\n  \"y = \", round(model_eq$estimate[1], 2), \" + \",\n  round(model_eq$estimate[2], 3), \"x\\n\",\n  \"r = \", round(correlation, 2), \n  \" | R² = \", round(r_squared, 2)\n)\n\n# Plot\nggplot(data, aes(x = popularity, y = danceability)) +\n  geom_jitter(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  annotate(\"text\", x = 5, y = 0,\n           label = eq_text, hjust = 0, size = 4, fontface = \"italic\") +\n  labs(\n    title = \"Relationship Between Popularity and Danceability\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Popularity\",\n    y = \"Log(Playlist Appearances)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTo understand which songs are most frequently chosen to open a playlist, I examined tracks appearing in the first position across playlists. The bar chart below displays the top 5 most common opening songs. Not surprisingly, many of these opening tracks are high-energy and/or popular.\n\n\nCode\n# What are the top 5 most common opening playlist songs?\nspotify_millions_songs |&gt;\n  filter(playlist_position == 0) |&gt; \n  add_count(track_id, name = \"song_frequency\") |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt; \n  arrange(desc(song_frequency)) |&gt;\n  slice_head(n = 5) |&gt;  \n  ggplot(aes(x = song_frequency, y = fct_reorder(paste0(track_name, \"\\n\", artist_name), song_frequency))) +\n  geom_bar(stat = \"identity\", fill = MINTY) +\n  labs(\n    title = \"Top 5 Most Popular Opening Playlist Songs\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Number of Playlists (Opening Position)\",\n    y = \"Track Name\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp03.html#analysis",
    "href": "mp03.html#analysis",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "Analysis",
    "text": "Analysis\nA brief analysis of the playlist data set is included below. To explore whether more popular songs tend to appear in more playlists, a linear regression was performed using popularity as the predictor and the log-transformed count of playlist appearances as the outcome. The figure below illustrates this relationship. The model shows a fairly strong positive association, with a regression equation of \\(y = -1.25 + 0.10x\\), a correlation of \\(r = 0.77\\), and a coefficient of determination \\(R^2 = 0.59\\). This suggests that popularity is a reasonably good predictor of how often a song shows up in playlists.\n\n\nCode\n# Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n# Prep the data\ndata &lt;- spotify_millions_songs |&gt;\n  add_count(track_id, name = \"playlist_appearances\") |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  mutate(log_appearances = log1p(playlist_appearances))\n\n# Fit the model\nmodel &lt;- lm(log_appearances ~ popularity, data = data)\n\n# Get model metrics\nmodel_eq &lt;- broom::tidy(model)\nr_squared &lt;- broom::glance(model)$r.squared\ncorrelation &lt;- cor(data$popularity, data$log_appearances, use = \"complete.obs\")\n\n# Build the equation text\neq_text &lt;- paste0(\n  \"y = \", round(model_eq$estimate[1], 2), \" + \",\n  round(model_eq$estimate[2], 3), \"x\\n\",\n  \"r = \", round(correlation, 2), \n  \" | R² = \", round(r_squared, 2)\n)\n\n# Plot\nggplot(data, aes(x = popularity, y = log_appearances)) +\n  geom_jitter(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_point(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  annotate(\"text\", x = 12.75, y = -1,\n           label = eq_text, hjust = 0, size = 4, fontface = \"italic\") +\n  labs(\n    title = \"Figure 1: Relationship Between Popularity and Playlist Appearances\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Popularity\",\n    y = \"Log(Playlist Appearances)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nMedian track popularity by year is presented in the figure below. The general trend is that track popularity is higher with release year. Notably, 2017 stands out as the year with the highest median song popularity.\n\n\nCode\n# In what year were the most popular songs released?\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  group_by(year) |&gt;\n  summarize(median_popularity = median(popularity, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = median_popularity)) +\n  geom_line(color = MINTY, linewidth = 1.2) +\n  geom_point(color = MINTY, size = 2) +\n  scale_x_continuous(\n    breaks = seq(min(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 max(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 by = 10)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Median Track Popularity\",\n    title = \"Median Track Popularity by Year\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhile median song popularity continues to keep up with the times, median track dance-ability peaked in 1929. The figure below presents the evolution of track dance ability over the past 90 years.\n\n\nCode\n# In what year did danceability peak?\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  group_by(year) |&gt;\n  summarize(median_danceability = median(danceability, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = median_danceability)) +\n  geom_line(color = MINTY, linewidth = 1.2) +\n  geom_point(color = MINTY, size = 2) +\n  scale_x_continuous(\n    breaks = seq(min(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 max(pull(spotify_millions_songs, year), na.rm = TRUE),\n                 by = 10)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Median Track Danceability\",\n    title = \"Median Track Danceability by Year\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe most represented decade on user playlists is the 2010s. The bar chart below presents the distribution of tracks belonging to a particular decade.\n\n\nCode\n# Which decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\n\nspotify_millions_songs |&gt;\n  mutate(decade = paste0((year %/% 10) * 10, \"s\")) |&gt;\n  group_by(decade) |&gt;\n  summarize(decade_count = n()/10000) |&gt;\n  ggplot(aes(x = decade_count, y = factor(decade))) +\n  geom_bar(stat = \"identity\", fill = MINTY) +\n  labs(\n    x = \"Number of Tracks (in tens of thousands)\",\n    y = \"Decade\",\n    title = \"Decade Representation in Playlists\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nThe most popular musical key is G, as can be seen in the figure of key distribution frequency below.\n\n\nCode\n# Create the key names\nkey_names &lt;- c(\n  \"C\", \"C♯/Db\", \"D\", \"D♯/Eb\", \"E\", \"F\",\n  \"F♯/Gb\", \"G\", \"G♯/Ab\", \"A\", \"A♯/Bb\", \"B\"\n)\n\n# Count frequency per key and label with musical key names\ndata &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  count(key, name = \"key_frequency\") |&gt;\n  mutate(key_label = factor(key_names[key + 1], levels = key_names))\n\n# Create label positions at fixed theta, variable radius\ny_values &lt;- seq(1000, max(data$key_frequency), by = 1000)\n\nradial_labels &lt;- tibble(\n  x = rep(\"D♯/Eb\", length(y_values)),  # Repeat \"C\" the same number of times as y values\n  y = y_values,\n  label = as.character(y_values)\n)\n\n# Plot with polar coordinates and annotations\nggplot(data, aes(x = key_label, y = key_frequency, fill = key_label)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(start = 0) +\n  geom_text(data = radial_labels, aes(x = x, y = y, label = label),\n            inherit.aes = FALSE, size = 3, color = \"gray20\", hjust = -0.2) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Distribution of Musical Keys\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\nAs shown in the figure below, there does not appear to be a strong preference for any particular track length, with short, medium, and long songs appearing at roughly equal frequencies. Length categories are defined by terciles: short songs are under 199.08 seconds (\\(33^{rd}\\) percentile), medium songs fall between 199.08 and 253.4 seconds (\\(33^{rd}\\)–\\(66^{th}\\) percentile), and long songs exceed 253.4 seconds.\n\n\nCode\n# What are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\n\nduration_percentiles &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  filter(!is.na(duration)) |&gt;\n  mutate(duration_sec = duration / 1000) |&gt;\n  summarize(\n    p33 = quantile(duration_sec, 0.33),\n    p66 = quantile(duration_sec, 0.66)\n  )\n\n# Pull out threshold values\np33 &lt;- duration_percentiles$p33\np66 &lt;- duration_percentiles$p66\n\n# Categorize and plot\nspotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt;\n  filter(!is.na(duration)) |&gt;\n  mutate(\n    duration_sec = duration / 1000,\n    duration_category = case_when(\n      duration_sec &lt;= p33 ~ \"Short\",\n      duration_sec &lt;= p66 ~ \"Medium\",\n      TRUE ~ \"Long\"\n    )\n  ) |&gt;\n  count(duration_category) |&gt;\n  ggplot(aes(x = duration_category, y = n, fill = duration_category)) +\n  geom_col() +\n  labs(\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    title = \"Song Duration Categories\",\n    x = \"Duration Category\",\n    y = \"Number of Unique Tracks\"\n  ) +\n  scale_y_continuous(labels = comma_format()) +\n  scale_fill_manual(\n    name = \"Duration Category\",\n    values = c(\"Short\" = MINTY, \"Medium\" = \"#fdd835\", \"Long\" = \"#ef5350\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nA linear regression analysis reveals a very weak positive relationship between danceability and popularity, with a coefficient of determination (\\(R^2\\)) of just 0.03. This suggests that only 3% of the variability in popularity can be explained by danceability. The scatter plot below illustrates this weak association.\n\n\nCode\n# Does danceability correlate with popularity?\ndata &lt;- spotify_millions_songs |&gt;\n  distinct(track_id, .keep_all = TRUE)\n# Fit the model\nmodel &lt;- lm(danceability ~ popularity, data = data)\n\n# Get model metrics\nmodel_eq &lt;- broom::tidy(model)\nr_squared &lt;- broom::glance(model)$r.squared\ncorrelation &lt;- cor(data$popularity, data$danceability, use = \"complete.obs\")\n\n# set up annotation\neq_text &lt;- paste0(\n  \"y = \", round(model_eq$estimate[1], 2), \" + \",\n  round(model_eq$estimate[2], 3), \"x\\n\",\n  \"r = \", round(correlation, 2), \n  \" | R² = \", round(r_squared, 2)\n)\n\n# Plot\nggplot(data, aes(x = popularity, y = danceability)) +\n  geom_jitter(alpha = 0.1, width = 0.3, height = 0.1, color = MINTY) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  annotate(\"text\", x = 5, y = 0,\n           label = eq_text, hjust = 0, size = 4, fontface = \"italic\") +\n  labs(\n    title = \"Relationship Between Popularity and Danceability\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Popularity\",\n    y = \"Log(Playlist Appearances)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTo understand which songs are most frequently chosen to open a playlist, I examined tracks appearing in the first position across playlists. The bar chart below displays the top 5 most common opening songs, revealing listener and curator preferences for kick-starting a playlist. Not surprisingly, many of these opening tracks are high-energy or popular.\n\n\nCode\n# What are the top 5 most common opening playlist songs?\nspotify_millions_songs |&gt;\n  filter(playlist_position == 0) |&gt; \n  add_count(track_id, name = \"song_frequency\") |&gt;\n  distinct(track_id, .keep_all = TRUE) |&gt; \n  arrange(desc(song_frequency)) |&gt;\n  slice_head(n = 5) |&gt;  \n  ggplot(aes(x = song_frequency, y = fct_reorder(paste0(track_name, \"\\n\", artist_name), song_frequency))) +\n  geom_bar(stat = \"identity\", fill = MINTY) +\n  labs(\n    title = \"Top 5 Most Popular Opening Playlist Songs\",\n    caption = \"Data sources: Spotify Millions Playlist data set and Spotify Song Characteristics data set.\",\n    x = \"Number of Playlists (Opening Position)\",\n    y = \"Track Name\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp03.html#playlist-construction",
    "href": "mp03.html#playlist-construction",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "Playlist Construction",
    "text": "Playlist Construction\n\nChoosing Anchor Songs\nSince I’ve been on an 80s kick recently, I picked the anchor songs shown in table 7.\n\n\nCode\n# base songs\nsong1 &lt;- spotify_millions_songs |&gt; filter(track_id == \"4S1VYqwfkLit9mKVY3MXoo\") # Forever Young by Alphaville\nsong2 &lt;- spotify_millions_songs |&gt; filter(track_id == \"22Ca1a5rQ5g2UFEZ3pp4tL\") # The Blue Sky by a-ha\n\nanchor_songs &lt;- list(song1, song2)\n\nbind_rows(song1,song2) |&gt; \n  select(track_name, artist_name, album_name, year) |&gt;\n  distinct(track_name, .keep_all = TRUE) |&gt;\n  rename(\n    \"Track Name\" = track_name,\n    \"Artist Name\" = artist_name,\n    \"Album Name\" = album_name,\n    \"Year\" = year\n  ) |&gt; \n  kable(caption = \"Table 7: Anchor Songs Chosen for the Ultimate Playlist\")\n\n\n\nTable 7: Anchor Songs Chosen for the Ultimate Playlist\n\n\nTrack Name\nArtist Name\nAlbum Name\nYear\n\n\n\n\nForever Young\nAlphaville\nForever Young\n1984\n\n\nThe Blue Sky\na-ha\nHunting High And Low\n1985\n\n\n\n\n\n\n\nCreating a Candidate Pool\nTo determine compatibility with the anchor songs, I applied the following heuristics:\n\nCo-occurrence: Songs that frequently appear on playlists alongside the anchor songs.\nMusical similarity: Songs in the same key and with a tempo within \\(\\pm5\\) BPM of the anchor.\nArtist match: Songs by the same artist(s) as the anchor tracks.\nAudio profile similarity: Songs released in the same year and exhibiting similar values for acousticness, dance-ability, energy, instrumentalness, liveness, loudness, speechiness, and valence. Similarity is measured using Euclidean distance: \\[d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\] where \\(x_i\\) and \\(y_i\\) are the characteristic values for the anchor and candidate songs, respectively.\nDecade match and structural similarity: Songs from the same decade and duration within \\(\\pm\\) 15 seconds of the anchor song.\n\nThe application of these heuristics is implemented in the code below.\n\n\nCode\nfname &lt;- \"data/mp03/peer_songs.csv\"\nif (!file.exists(fname)){\n# What other songs commonly appear on playlists along side these songs?\npeer_songs &lt;- data.frame()\nfor (song in anchor_songs) {\n  playlist_ids &lt;- song |&gt; pull(playlist_id) |&gt; unique() |&gt; as.vector()\n  for (pid in playlist_ids) {\n    peer_songs &lt;- bind_rows(\n      peer_songs,\n      spotify_millions_songs |&gt;\n        filter(playlist_id == pid) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }\n}\n  write.csv(peer_songs |&gt; distinct(track_id, .keep_all = TRUE), fname, row.names = FALSE)\n} else{\n  peer_songs &lt;- readr::read_csv(fname)\n}\n\n# What songs have a similar key and tempo to these songs\nfname &lt;- \"data/mp03/key_and_tempo.csv\"\nif (!file.exists(fname)){\n  key_and_tempo &lt;- data.frame()\n  for (song in anchor_songs){\n    song_key &lt;- unique(song |&gt; pull(key))\n    song_tempo &lt;- unique(song |&gt; pull(tempo))\n    key_and_tempo &lt;- \n      bind_rows(\n        key_and_tempo,\n        spotify_millions_songs |&gt;\n          mutate(\n            similar_tempo = case_when(\n              abs(tempo - unique(song |&gt; pull(tempo))) &lt;= 5 ~ TRUE,\n              TRUE ~ FALSE\n            )) |&gt;\n          filter(\n            key == song_key,\n            similar_tempo == TRUE) |&gt;\n          distinct(track_id, .keep_all = TRUE)\n      )\n  }\n  write.csv(key_and_tempo, fname, row.names = FALSE)\n} else{\n    key_and_tempo &lt;- readr::read_csv(fname)\n}\n\n# What songs have the same artist?\nfname &lt;- \"data/mp03/same_artist.csv\"\nif (!file.exists(fname)){\n  same_artist = data.frame()\n  for (song in anchor_songs){\n    song_artist_id &lt;- unique(song |&gt; pull(artist_id))\n    same_artist &lt;- bind_rows(\n      same_artist,\n      spotify_millions_songs |&gt;\n        filter(artist_id == song_artist_id) |&gt;\n        distinct(track_id, .keep_all = TRUE)\n    )\n  }\n  write.csv(same_artist, fname, row.names = FALSE)\n} else{\n    same_artist &lt;- readr::read_csv(fname)\n}\n\n# What other songs were released in the same year and have similar levels of acousticness, danceability, etc.\nfname &lt;- \"data/mp03/all_similar_songs.csv\"\nif (!file.exists(fname)){\n  all_similar_songs &lt;- data.frame()\n  \n  #' Calculate Similar Songs Based on Selected Metrics\n  #'\n  #' This function identifies songs similar to a given song by calculating the Euclidean distance between \n  #' the selected metrics (e.g., acousticness, danceability, energy, etc.) of the input song and all other songs\n  #' from the same year. Only songs with a distance smaller than a specified threshold (2.5) are considered similar.\n  #'\n  #' @param song A data frame representing a single song, containing columns for song characteristics (e.g., `track_id`, `year`, and metrics like `acousticness`, `danceability`, etc.).\n  #' @param std_songs A data frame representing the standardized set of songs to compare against, which must include the same metrics as in the `song` argument.\n  #' @param metrics A character vector containing the names of the metrics to be used in calculating the similarity (e.g., `c(\"acousticness\", \"danceability\", \"energy\", \"valence\")`).\n  #'\n  #' @return A data frame with songs that are similar to the input song, based on the Euclidean distance between their metric values. The songs are filtered by a distance threshold of 2.5, \n  #'         and they are ordered by their proximity to the input song.\n  #'\n  #' @details The function scales the metrics for all songs in the same year as the input song and calculates the Euclidean distance between the input song's metrics and each song's metrics.\n  #'          The distance is computed as the square root of the sum of squared differences between the selected metrics of the input song and each candidate song.\n  #'          Only songs with a distance smaller than 2.5 are considered similar.\n  #'\n  #' @examples\n  #' # Assuming `song` and `std_songs` are data frames with appropriate structure\n  #' similar_songs &lt;- calculate_similar_songs(song = song_data, std_songs = song_dataset, metrics = c(\"acousticness\", \"danceability\", \"energy\"))\n  #'\n  #' @export\n  calculate_similar_songs &lt;- function(song, std_songs, metrics) {\n    # Filter for songs from the same year and apply scaling to the metrics\n    std_songs_scaled &lt;- std_songs |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      filter(year == unique(song |&gt; pull(year))) |&gt;\n      mutate(across(all_of(metrics), scale))\n    \n    # Get the song vector (metrics for the current song)\n    song_vec &lt;- std_songs_scaled |&gt;\n      filter(track_id == unique(song |&gt; pull(track_id))) |&gt;\n      distinct(track_id, .keep_all = TRUE) |&gt;\n      select(all_of(metrics)) |&gt;\n      as.numeric()\n    \n    # Assign proper names to the song vector based on metrics\n    names(song_vec) &lt;- metrics\n    \n    # Calculate the distance for each song\n    similar_songs &lt;- std_songs_scaled |&gt;\n      mutate(distance = sqrt(\n        (acousticness - song_vec[\"acousticness\"])^2 +\n          (danceability - song_vec[\"danceability\"])^2 +\n          (energy - song_vec[\"energy\"])^2 +\n          (instrumentalness - song_vec[\"instrumentalness\"])^2 +\n          (liveness - song_vec[\"liveness\"])^2 +\n          (loudness - song_vec[\"loudness\"])^2 +\n          (speechiness - song_vec[\"speechiness\"])^2 +\n          (valence - song_vec[\"valence\"])^2\n      )) |&gt;\n      arrange(distance) |&gt;\n      filter(distance &lt; 2)\n    # Return the result\n    return(similar_songs)\n  }\n  \n  anchor_songs &lt;- list(song1, song2)\n  # For loop to calculate similar songs for each song and bind the rows\n  for (song in anchor_songs) {\n    similar_songs &lt;- calculate_similar_songs(song, spotify_millions_songs, metrics)\n    all_similar_songs &lt;- bind_rows(all_similar_songs, similar_songs)\n  }\n  write.csv(all_similar_songs, fname, row.names = FALSE)\n} else{\n    all_similar_songs &lt;- readr::read_csv(fname)\n}\n\n# What songs are in the same decade and have a similar duration?\n# add decade to song data.frames\nsong1 &lt;- song1 |&gt; mutate(decade = as.character((year %/% 10) * 10))\nsong2 &lt;- song2 |&gt; mutate(decade = as.character((year %/% 10) * 10))\nanchor_songs &lt;- list(song1, song2)\n\nfname &lt;- \"data/mp03/decade_and_song_length.csv\"\nif (!file.exists(fname)) {\n  decade_and_song_length &lt;- data.frame()\n\n  for (song in anchor_songs) {\n    song_decade &lt;- unique(song$decade)\n    song_length &lt;- unique(song$duration)\n\n    matched_songs &lt;- spotify_millions_songs |&gt;\n      mutate(\n        similar_duration = abs(duration - song_length) &lt;= 15 * 1000\n      ) |&gt; \n      filter(\n        decade == song_decade,\n        similar_duration\n      ) |&gt;\n      distinct(track_id, .keep_all = TRUE)\n\n    decade_and_song_length &lt;- bind_rows(decade_and_song_length, matched_songs)\n  }\n\n  write.csv(decade_and_song_length, fname, row.names = FALSE)\n\n} else {\n  decade_and_song_length &lt;- readr::read_csv(fname)\n}\n\n\nThe full list of candidate songs is shown below:\n\n\n\n\n\n\n\n\nSelecting Playlist Songs\nSince I wanted the playlist to have a deliberate “rise and fall,” in mood, I decided to methodically control the valence of the playlist. Specifically, I shaped the playlist so that the valence of the selected songs would follow the shape of a sine wave, and create an emotional roller-coaster for listeners to ride.\nTo implement this:\n\nI generated a cosine wave scaled between 0 and 1 to define the target valence levels for each of the 12 songs in the playlist.\nI added the anchor songs to the playlist first, placing each one in the position where its valence most closely matched the wave.\nRemaining slots were filled with songs from a filtered pool of candidates that met 2 or more compatibility heuristics (see definitions above). I also restricted the decade of the song to 1980.\nFor each remaining position, I selected a song with a valence close to the target value. I allowed for a small tolerance to ensure flexibility in the song choice and still maintain the shape of the wave.\n\nThe code for implementing this is shown below.\n\n\nCode\n# Set seed for reproducibility\nset.seed(7)\nplaylist_length &lt;- 12\n\n# Generate cosine wave to model valence progression\nvalence_wave &lt;- (cos(seq(0, 2 * pi, length.out = playlist_length)) + 1) / 2\n\n# Define valence similarity tolerance\ntolerance &lt;- 0.05\n\n# Initialize empty playlist\nplaylist &lt;- data.frame()\n\n# Combine anchor songs and ensure uniqueness\nbase_songs &lt;- bind_rows(anchor_songs) |&gt; distinct(track_id, .keep_all = TRUE)\n\n# Select high-confidence candidate songs\ncandidates &lt;- combined_heuristics |&gt; \n  filter(heuristic_count &gt;= 2, decade == '1980s') |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\n# Create a vector of available positions\navailable_slots &lt;- 1:playlist_length\n\n# Place anchor songs into playlist by matching valence to wave\nfor (i in seq_len(nrow(base_songs))) {\n  song &lt;- base_songs[i, ]\n  if (length(available_slots) == 0) break\n  \n  diffs &lt;- abs(valence_wave[available_slots] - song$valence)\n  best_slot &lt;- available_slots[which.min(diffs)]\n  \n  playlist &lt;- bind_rows(playlist, song)\n  available_slots &lt;- setdiff(available_slots, best_slot)\n  candidates &lt;- candidates |&gt; filter(track_id != song$track_id)\n}\n\n# Fill remaining slots with best valence matches from candidates\nfor (slot in available_slots) {\n  target_valence &lt;- valence_wave[slot]\n  \n  pool &lt;- candidates |&gt; filter(abs(valence - target_valence) &lt;= tolerance)\n  \n  # Expand tolerance if needed\n  if (nrow(pool) == 0) {\n    message(glue::glue(\"Widening tolerance at slot {slot}\"))\n    pool &lt;- candidates |&gt; filter(abs(valence - target_valence) &lt;= (tolerance * 2))\n  }\n  \n  # Select one matching song if available\n  if (nrow(pool) &gt; 0) {\n    selected &lt;- sample_n(pool, 1)\n    playlist &lt;- bind_rows(playlist, selected)\n    candidates &lt;- candidates |&gt; filter(track_id != selected$track_id)\n  } else {\n    message(glue::glue(\"No song could be placed at slot {slot}\"))\n  }\n}\n\n# Add song position index\nplaylist &lt;- playlist |&gt; mutate(position = row_number())\n\n# Reshape for plotting\nplaylist_long &lt;- playlist |&gt; pivot_longer(cols = all_of(metrics), names_to = \"metric\", values_to = \"value\")\n\n\nThe final result is an 80s playlist designed to, at some points, make you feel like you’re about to hit 88 mph in Doc’s Delorean, and at others, like you’re slow-dancing alone at prom under a disco ball."
  },
  {
    "objectID": "mp03.html#internets-best-playlist",
    "href": "mp03.html#internets-best-playlist",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "",
    "text": "I nominate this playlist for the internet’s best playlist because it was intentionally crafted to optimize an emotional flow by ordering the songs so that their valence (a measure of musical positivity) follows a cosine wave–rising and falling in a deliberate rhythm. This approach was algorithmically driven to create an emotional arc that feels both natural and dynamic.\nThe figure below visualizes how various song characteristics evolve across the playlist. There is a clear dynamism accross the different characteristics–guaranteeing a emotionally driven listening experience."
  },
  {
    "objectID": "mp03.html#tracklist",
    "href": "mp03.html#tracklist",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "",
    "text": "Position\nTrack Name\nArtist Name\nAlbum Name\nYear\n\n\n\n\n1\nForever Young\nAlphaville\nForever Young\n1984\n\n\n2\nThe Blue Sky\na-ha\nHunting High And Low\n1985\n\n\n3\nWhip It\nDEVO\nFreedom Of Choice\n1980\n\n\n4\nLove Plus One\nHaircut 100\nPelican West Plus\n1982\n\n\n5\nLonely Ol’ Night\nJohn Mellencamp\nScarecrow\n1985\n\n\n6\nWhat You Need\nINXS\nListen Like Thieves\n1985\n\n\n7\nI Drink Alone\nGeorge Thorogood & The Destroyers\nMaverick\n1985\n\n\n8\nI Need Someone Like Me\nGeorge Strait\nDoes Fort Worth Ever Cross Your Mind\n1984\n\n\n9\nMusic For A Found Harmonium - 2008 Digital Remaster\nPenguin Cafe Orchestra\nBroadcasting From Home\n1984\n\n\n10\nFlash - Single Version\nQueen\nFlash Gordon\n1980\n\n\n11\nI Love You Babe\nBabyface\nLovers\n1986\n\n\n12\nStrut\nSheena Easton\nA Private Heaven [Bonus Tracks Version]\n1984"
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis image was generated with Google Gemini (Google Gemini, 2025).↩︎"
  },
  {
    "objectID": "mp03.html#data-import",
    "href": "mp03.html#data-import",
    "title": "The Ultimate Playlist: Valence Synth Wave",
    "section": "Data Import",
    "text": "Data Import\n\nSong Characteristics Dataset\nThe song characteristics data set was downloaded from this URL, using the code below:\n\n\nCode\n# Imports\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(scales)\nlibrary(gganimate)\n# constants\nMINTY &lt;- \"#78c2ad\"\n\n#' Load Spotify song data\n#'\n#' This function checks for the existence of a local CSV file containing Spotify song data.\n#' If the file does not exist, it downloads it from a remote URL. The function then reads and returns the data as a tibble.\n#'\n#' @return A tibble containing Spotify song analytics data.\nload_songs &lt;- function() {\n  directory &lt;- \"data/mp03/\"\n  file_name &lt;- paste0(directory, \"spotify_song_analytics.csv\")\n  \n  # Create the data directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  # Download the data file if it's not already present\n  if (!file.exists(file_name)) {\n    download.file(\n      url = \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_name,\n      method = \"auto\"\n    )\n  }\n  \n  # Read the CSV into a tibble\n  songs &lt;- readr::read_csv(file_name)\n  return(songs)\n}\n\n# Load songs into a dataframe\nsongs &lt;- load_songs()\n\n\nA small amount of clean up was required for this data set. The code used to process the raw data set is presented below.\n\n\nCode\n# Instructor provided code for cleaning artist string\nclean_artist_string &lt;- function(x){\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\n# Split songs with multiple artists into separate rows and clean artist names\nsongs &lt;-\n  songs |&gt; \n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\n\n\n\nPlaylists Dataset\nThe playlists data set was downloaded from this URL, using the code outlined below:\n\n\nCode\n#' Load Spotify playlist data\n#'\n#' This function downloads and/or reads the Million Playlist Dataset slices from \n#' a GitHub repository. If `read_only = TRUE`, it skips downloading and only reads \n#' existing local files.\n#'\n#' @param read_only Logical. If TRUE, skips download and only loads local data. Default is FALSE.\n#' @return A list of parsed JSON objects representing playlist data.\nload_playlists &lt;- function(read_only = FALSE) {\n  source_root_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  directory &lt;- \"data/mp03/mpd/\"\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  if (!read_only) {\n    # Loop over all playlist slice file names\n    for (i in seq(0, 999000, by = 1000)) {\n      fname &lt;- paste0(\n        \"mpd.slice.\",\n        format(i, scientific = FALSE), \"-\",\n        format(i + 999, scientific = FALSE),\n        \".json\"\n      )\n      fpath &lt;- file.path(directory, fname)\n      \n      # Download file if it's not already present\n      if (!file.exists(fpath)) {\n        Sys.sleep(0.01)  # Avoid hitting rate limits\n        tryCatch({\n          invisible(download.file(\n            url = paste0(source_root_url, fname),\n            destfile = fpath,\n            method = \"auto\",\n            quiet = TRUE\n          ))\n        }, error = function(e) {\n          message(\"Download failed: \", fname)\n        })\n      }\n    }\n  }\n  \n  # Read all the JSON files in the directory\n  json_files &lt;- list.files(directory, pattern = \"\\\\.json$\", full.names = TRUE)\n  all_data &lt;- vector(\"list\", length(json_files))\n  \n  for (i in seq_along(json_files)) {\n    tryCatch({\n      all_data[[i]] &lt;- suppressWarnings(jsonlite::fromJSON(json_files[i]))\n    }, error = function(e) {\n      message(\"Failed to read: \", json_files[i])\n    })\n  }\n  \n  return(all_data)\n}\n\nSPOTIFY_MILLIONS &lt;- load_playlists()\n\n\nSince the playlist data set was hierarchical, it required “rectangling,” to be adapted for use in this analysis. This process was accomplished with the following code.\n\n\nCode\n#' Process a Spotify playlist slice\n#'\n#' This function processes a single playlist slice from the Million Playlist Dataset,\n#' extracting track information and saving it as a CSV file for faster future loading.\n#' It strips Spotify URI prefixes and appends playlist metadata.\n#'\n#' @param playlist A list object representing one slice of the Spotify playlist data (parsed JSON).\n#' @return A data frame (tibble) of processed track data with playlist-level metadata.\nprocess &lt;- function(playlist) {\n  \n  # Instructor provided helper function to strip Spotify URI prefix\n  strip_spotify_prefix &lt;- function(x) {\n    str_replace(x, \".*:.*:\", \"\")\n  }\n  \n  # Create the output directory if it doesn't exist\n  output_dir &lt;- \"data/mp03/processed\"\n  if (!dir.exists(output_dir)){\n    dir.create(output_dir, showWarnings = FALSE)\n  }\n  \n  # Generate filename for processed slice\n  slice_range &lt;- playlist$info$slice\n  csv_outfile &lt;- paste0(output_dir, \"/mpd.slice.\", slice_range, \".csv\")\n  \n  # If CSV already exists, load it instead of reprocessing\n  if (file.exists(csv_outfile)){\n    all_tracks &lt;- readr::read_csv(csv_outfile)\n  } else {\n    all_tracks &lt;- data.frame()  # Placeholder for all processed tracks\n    \n    for (i in seq_along(playlist$playlists$tracks)) {\n      tracks_df &lt;- as.data.frame(playlist$playlists$tracks[[i]])\n      \n      # Skip empty playlists\n      if (nrow(tracks_df) == 0) next\n      \n      # Add metadata and clean up Spotify URIs\n      tracks_df &lt;- tracks_df |&gt;\n        mutate(\n          playlist_name = playlist$playlists$name[i],\n          playlist_id = playlist$playlists$pid[i],\n          playlist_followers = playlist$playlists$num_followers[i],\n          track_uri = strip_spotify_prefix(track_uri),\n          artist_uri = strip_spotify_prefix(artist_uri),\n          album_uri = strip_spotify_prefix(album_uri)\n        )\n      \n      # Combine with all previously processed tracks\n      all_tracks &lt;- bind_rows(all_tracks, tracks_df)\n    }\n    \n    # Save the processed data for this slice\n    write.csv(all_tracks, csv_outfile, row.names = FALSE)\n    message(\"Saved: \", csv_outfile)\n  }\n  return(all_tracks)\n}\n\n#' Merge all processed playlist slices into a single dataset\n#'\n#' This function checks if a master CSV file (`spotify_millions.csv`) already exists.\n#' If not, it processes all playlist JSON slices provided and merges them into one\n#' unified dataframe, then saves it as a CSV for future use. If the file already exists,\n#' it loads the data directly.\n#'\n#' @param data A list of playlist slices (parsed JSON). Defaults to `master_json`.\n#' @return A dataframe containing the merged Spotify playlist data.\nmerge_processed_playlists &lt;- function(data = master_json) {\n  # Path to the master output CSV\n  spotify_millions_fpath &lt;- \"data/mp03/spotify_millions.csv\"\n  \n  if (!file.exists(spotify_millions_fpath)) {\n    # If no master file exists, process each playlist slice\n    playlists &lt;- data.frame()  # Initialize an empty dataframe\n    \n    for (i in seq_along(master_json)) {\n      playlist &lt;- master_json[[i]]\n      processed_df &lt;- process(playlist)  # Process individual slice\n      playlists &lt;- bind_rows(playlists, processed_df)  # Append to master dataframe\n    }\n\n    # Clean up and standardize column names\n    playlists &lt;- playlists |&gt; \n      rename(\n        playlist_position = `pos`,\n        track_id = `track_uri`,\n        artist_id = `artist_uri`,\n        album_id = `album_uri`,\n        duration = `duration_ms`\n      ) |&gt;\n      select(!`...1`)  # Drop the unnamed index column\n\n    # Save merged data to CSV\n    write.csv(playlists, spotify_millions_fpath, row.names = FALSE)\n  } else {\n    # If file exists, just read it\n    playlists &lt;- readr::read_csv(spotify_millions_fpath)\n  }\n\n  return(playlists)\n}\n\n# Load or process the Spotify Millions json dataset\nSPOTIFY_MILLIONS &lt;- merge_processed_playlists(SPOTIFY_MILLIONS)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Jocely Lopez Luna, a researcher with a B.A. in environmental science (Columbia University, ’20) and 5 years of work experience in seismology at Lamont-Doherty Earth Observatory. I’ve worked extensively with seismic data, programming, and quantitative research, and I’m currently expanding my expertise through an MS in Statistics at Baruch College.\nAt Lamont, I work on modernizing analog seismic data and investigating atmospheric and underground nuclear explosions. You can find my publications on ORCID.\nAs a Catholic, my faith guides my approach to learning, discovery, and service. I strive to bring truth, integrity, and purpose to everything I do.\nHere’s my resume. Explore my work, and let’s connect!\n\nLast updated: Wednesday May 07, 2025 at 10:15 AM"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Test Page",
    "section": "",
    "text": "Test Cards\n\n\n&lt;div class=\"col\"&gt;\n  &lt;div class=\"card h-100\"&gt;\n    &lt;div class=\"card-body\"&gt;\n      &lt;h5 class=\"card-title\"&gt;Card 1&lt;/h5&gt;\n      &lt;p class=\"card-text\"&gt;This is a test.&lt;/p&gt;\n      &lt;a href=\"#\" class=\"btn btn-primary\"&gt;Go&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div class=\"col\"&gt;\n  &lt;div class=\"card h-100\"&gt;\n    &lt;div class=\"card-body\"&gt;\n      &lt;h5 class=\"card-title\"&gt;Card 2&lt;/h5&gt;\n      &lt;p class=\"card-text\"&gt;Another one.&lt;/p&gt;\n      &lt;a href=\"#\" class=\"btn btn-primary\"&gt;Go&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "index.html#explore-my-mini-projects",
    "href": "index.html#explore-my-mini-projects",
    "title": "Welcome to STA9750 - Spring 2025",
    "section": "1 Explore My Mini Projects",
    "text": "1 Explore My Mini Projects\n\n\n\nMini Project 01\n\n\nWelcome to the Commission to Analyze Taxpayer Spending (CATS)\n\n\n\n\n🏙️ Mini Project 02\n\n\nNYC Real Estate: Pricing, location insights, and remote work.\n\n\n\n\n🎶 Mini Project 03\n\n\nSpotify Data: Curating the Echoes of Euphoria playlist."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "US Political Shifts",
    "section": "",
    "text": "This appendix briefly summarizes the steps taken to recreate the New York Times inspired political shift plot. The code chunk below lists the R libraries used in this analysis, as well as any constants defined in the analysis.\n\n\nCode\n# import libraries\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(httr)\nlibrary(rvest)\nlibrary(tigris)\nlibrary(plotly)\n\n# define constants\nARROW_ANGLE &lt;- pi / 6\nARROWHEAD_ANGLE &lt;- pi / 6\nSCALE_FACTOR &lt;- 10000\nLINEWIDTH &lt;- 0.3\n\n\n\n\n\n\nI imported the 1 : 20,000,000 (national) US county shapes from the US Census Bureau with the code below.\n\n\nCode\n#' Download and Process US County Shapefile\n#'\n#' Downloads, extracts, and processes the 2023 US county shapefile from the U.S. Census Bureau if not already present locally.\n#'\n#' This function checks whether the shapefile exists in the specified local directory (`data/mp04/`). If it does not exist,\n#' the function downloads the ZIP file containing the shapefile, extracts its contents, and deletes the ZIP file to save space.\n#' It then reads the shapefile using `read_sf()`, shifts geometries (typically to reposition Alaska and Hawaii), casts geometries\n#' to `MULTIPOLYGON`, and computes centroids for each county polygon.\n#'\n#' @return A `sf` object representing the US counties with geometry and centroid columns.\n#' @importFrom sf read_sf st_cast st_centroid\n#' @importFrom dplyr mutate\n#' @importFrom tigris shift_geometry\n#' @export\ndownload_shp &lt;- function(){\n  directory &lt;- \"data/mp04/\"\n  fname &lt;- \"cb_2023_us_county_20m\"\n  zip_fpath &lt;- paste0(directory, fname, \".zip\")\n  shp_fpath &lt;- paste0(directory, fname, \".shp\")\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n\n  files_matching_pattern &lt;- list.files(directory, pattern = \"cb_2023_us_county_20m\", full.names = TRUE)\n  if (length(files_matching_pattern) == 0){\n    source_root_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_20m.zip\"\n    download.file(\n      url = source_root_url,\n      destfile = zip_fpath,\n      method = \"auto\",\n      quiet = TRUE\n    )\n  }\n  # check if zip exists\n  if (file.exists(zip_fpath)){\n    # if true, unzip the shp file, then delete the zip to save a measly amount of memory.\n    unzip(zip_fpath, exdir=directory)  \n    file.remove(zip_fpath)\n  } \n  # check if shp exists\n  if (file.exists(shp_fpath)){\n    # if true, read in shp file\n    shpfile &lt;- read_sf(shp_fpath) |&gt;\n      shift_geometry(position = \"below\", preserve_area = FALSE) |&gt;\n      mutate(\n        geometry = st_cast(geometry, \"MULTIPOLYGON\"),\n        centroid = st_centroid(geometry)\n      )\n    return(shpfile)\n  }\n}\n\nus_counties &lt;- download_shp()\n\n\n\n\n\nUS presidential election results for each state were scraped from Wikipedia using the load_state_election_results function defined below.\n\n\nCode\n#' Load or Download County-Level U.S. Presidential Election Results from Wikipedia\n#'\n#' Retrieves, cleans, and processes county-level (or equivalent) U.S. presidential election results\n#' for a given state and election year (2020 or 2024). If the data is not available locally, it is\n#' scraped from the corresponding Wikipedia page and cached to disk for future use.\n#'\n#' Handles special cases such as Connecticut (town or planning region level),\n#' the District of Columbia (ward level), and Washington (naming convention on Wikipedia).\n#' Also maps Connecticut towns to planning regions for 2024.\n#'\n#' @param state A character string with the full state name (e.g., \"Connecticut\", \"Texas\").\n#' @param year An integer (2020 or 2024). Other years are not supported.\n#'\n#' @return A data frame containing county-level election results for the specified state and year.\n#'         The columns include vote counts and percentages for major parties and others, with standardized\n#'         county identifiers. Also includes a `county_type` and `state` column.\n#'\n#' @importFrom rvest read_html html_elements html_table\n#' @importFrom dplyr mutate rename_with slice inner_join group_by summarize across\n#' @importFrom stringr str_to_lower str_replace_all str_remove_all str_remove str_replace\n#' @importFrom utils download.file write.csv read.csv\n#' @importFrom stats na.omit\n#' @export\nload_state_election_results &lt;- function(state, year){\n  if (!(year %in% c(2020, 2024))) {\n    stop(\"Please enter year = 2020 or year = 2024\")\n  }\n  year &lt;- as.integer(year)\n  state_fname &lt;- paste0(str_to_lower(state),\".csv\")\n  directory &lt;- \"data/mp04/\"\n  subdirectory &lt;- paste0(directory,year,'/')\n  state_fpath &lt;- paste0(subdirectory,state_fname)\n  mapping_flag &lt;- FALSE\n  \n  if (!dir.exists(subdirectory)) {\n    dir.create(subdirectory, recursive = TRUE)\n  }\n  \n  if (!file.exists(state_fpath)){\n    \n    regex_str &lt;- \"^(County(/City)?|Parish|Ward|State\\\\sHouse\\\\sDistrict)(\\\\[[0-9]+\\\\])?$\"\n    \n    # no file found, therefore, i will download straight from wikipedia.\n    url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"#By_county\")\n    if (state == \"Washington\"){\n      url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"_(state)#By_county\")\n    }\n    if (state == \"District of Columbia\"){\n      url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_the_\",str_replace_all(state, \"\\\\s+\", \"_\"),\"#Results_by_ward\")\n    }\n    if (state == \"Connecticut\"){\n      if (year == 2020){\n        mapping_flag &lt;- TRUE\n        regex_str = \"^Town$\"\n        url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"#By_town\")\n      }\n      if (year == 2024){\n        regex_str = \"^Council\\\\sof\\\\sGovernment$\"\n        url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"#By_Council_of_Government\")\n      }\n    }\n    \n    \n    county_counts &lt;- tryCatch({\n      read_html(url) |&gt;\n        html_elements(\".wikitable\") |&gt;\n        html_table() |&gt;\n        Filter(\\(x)\n               any(grepl(regex_str, colnames(x))) &&\n                 any(grepl(\"Margin\", colnames(x))),\n               x = _\n        )\n    }, error = function(e) {\n      print(url)\n      warning(paste(\"Failed to load data for state:\", state))\n      return(NULL)\n    })\n    \n    if (length(county_counts) == 1){ county_counts &lt;- county_counts[[1]] }\n    if (length(county_counts) == 0){ warning(paste(\"No data for\", state)); return(NULL) }\n    if (is.list(county_counts) && length(county_counts) &gt; 1 && inherits(county_counts[[1]], \"data.frame\")) {\n      county_counts &lt;- county_counts[[1]]\n    }\n   \n    # first row hints at units--I want that info in the column names instead. \n    first_row &lt;- county_counts |&gt; head(n=1)\n    county_counts &lt;- county_counts |&gt;\n      rename_with(\n        ~ paste0(., \"_Count\"),\n        .cols = which(grepl(\"^#|Votes$\", first_row))\n      ) |&gt;\n      rename_with(\n        ~ paste0(., \"_Percentage\"), \n        .cols = which(first_row == \"%\")\n      )\n    \n    \n    # delete useless first row of values.\n    county_counts &lt;- county_counts |&gt;\n      slice(-1)\n    \n    #cleaning up column names for easier referencing\n    colnames(county_counts) &lt;- county_counts |&gt; \n      colnames() |&gt; \n      str_remove_all(\"Donald Trump|Kamala Harris|Various candidates|City|Joe Biden|Jo Jorgensen|Howie Hawkins\") |&gt; \n      str_remove_all(\"[^a-zA-Z_]\")\n  \n    \n    first_col &lt;- colnames(county_counts)[1]\n  \n    # saving cleaned up county type for later\n    county_counts &lt;- \n      county_counts |&gt;\n      mutate(county_type = first_col)\n    # converting data types\n    county_counts &lt;- county_counts |&gt;\n      mutate(across(!any_of(c(\"County\", \"county_type\", \"Town\", \"Parish\",\"Ward\",\"StateHouseDistrict\", \"CouncilofGovernment\")), ~ as.numeric(\n        str_remove_all(\n          str_replace_all(as.character(.x), \"\\u2212\", \"-\"),\n          \"[,%\\u00A0]\"\n        )\n      )))\n    \n    if (mapping_flag){\n      mapping_fdest &lt;- paste0(directory,\"ct-town-to-planning-region.csv\")\n      if (!file.exists(mapping_fdest)){\n        download.file(\n          url = \"https://raw.githubusercontent.com/CT-Data-Collaborative/ct-town-to-planning-region/refs/heads/main/ct-town-to-planning-region.csv\",\n          destfile = mapping_fdest,\n          method = \"auto\"\n        )\n      }\n      ct_mapping &lt;- as.data.frame(read.csv(mapping_fdest))\n    \n      county_counts &lt;-\n        county_counts |&gt; \n          inner_join(\n            ct_mapping |&gt;\n              select(town_name, ce_name_2022),\n            join_by(Town == town_name)) |&gt; \n          group_by(ce_name_2022) |&gt;\n          summarize(\n            Democratic_Count = sum(Democratic_Count),\n            Republican_Count = sum(Republican_Count),\n            Libertarian_Count = sum(Libertarian_Count),\n            Green_Count = sum(Green_Count),\n            Otherparties_Count = sum(Otherparties_Count),\n            ) |&gt;\n          mutate(\n            total_votes = rowSums(across(\n              .cols = c(Democratic_Count, Republican_Count, Libertarian_Count, Green_Count, Otherparties_Count),\n              .names = NULL\n            ), na.rm = TRUE),\n            Republican_Percentage = 100 * Republican_Count / total_votes,\n            Democratic_Percentage = 100 * Democratic_Count / total_votes,\n            Libertarian_Percentage = 100 * Libertarian_Count / total_votes,\n            Green_Percentage = 100 * Green_Count / total_votes,\n            Otherparties_Percentage = 100 * Otherparties_Count / total_votes,\n            county_type = \"Planning Region\",\n            ce_name_2022 = str_remove(ce_name_2022, \" Planning Region\")\n          ) |&gt;\n          rename(county = ce_name_2022)\n    }\n    \n    # finish cleaning up column names \n    colnames(county_counts) &lt;- \n      county_counts |&gt; \n      colnames() |&gt; \n      str_replace_all(\"Parish|Ward|StateHouseDistrict|CouncilofGovernment\", \"County\") |&gt;\n      str_to_lower()\n\n    if (state == \"Connecticut\" && year == 2024){\n      county_counts &lt;-\n        county_counts |&gt;\n        mutate(\n          county_type = \"Planning Region\",\n          county = str_remove(county, \" Planning Region\")\n          )\n    }\n    \n    # adding state name for easier merging later.\n    county_counts &lt;- county_counts |&gt;\n      mutate(state = state)\n    \n    write.csv(x = county_counts, file = state_fpath, row.names = FALSE)\n  } else{\n    county_counts &lt;- read.csv(state_fpath)\n  }\n  return (county_counts)\n}\n\n\nAfter assigning us_states a list of US states, us_states &lt;- unique(us_counties |&gt; select(STATE_NAME) |&gt; st_drop_geometry())[[1]], load_state_election_results was called from within the following function:\n\n\nCode\n#' Load or Compile U.S. Presidential Election Results for Multiple States\n#'\n#' Aggregates county-level (or equivalent) presidential election results for a list of U.S. states in a given election year.\n#' If a compiled CSV file already exists for the specified year, it is loaded. Otherwise, data is fetched (via\n#' \\code{\\link{load_state_election_results}}), cleaned, and saved for future use.\n#'\n#' Handles normalization of alternate party labels (e.g., Democratic–NPL, DFL), estimates missing values where necessary,\n#' and calculates total votes and percentages for Republican, Democratic, and other candidates.\n#'\n#' @param states A character vector of state names (e.g., \\code{c(\"Texas\", \"Ohio\", \"Connecticut\")}).\n#' @param year An integer representing the election year (only 2020 or 2024 are supported).\n#'\n#' @return A data frame with cleaned, harmonized election results by county, including vote counts and percentages for\n#' Republican, Democratic, and other parties, along with state and county identifiers.\n#'\n#' @importFrom dplyr bind_rows mutate case_when select any_of rowSums matches\n#' @importFrom tidyselect everything\n#' @importFrom stringr str_to_lower\n#' @importFrom utils read.csv write.csv\n#' @seealso \\code{\\link{load_state_election_results}}\n#' @export\nload_election_results &lt;- function(states, year){\n  directory &lt;- paste0(\"data/mp04/\",year,\"/\")\n  dest_fpath &lt;- paste0(directory, year,\"_election_results.csv\")\n  \n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  if (!file.exists(dest_fpath)){\n    election_results &lt;- data.frame()\n    for (state in states){\n      message(paste(\"Fetching State:\", state))\n      state_results &lt;- load_state_election_results(state,year)\n      if (!is.null(state_results)) {\n        election_results &lt;- bind_rows(election_results, state_results)\n      }\n      else{\n          warning(paste(\"No data for state:\", state))\n        }\n    }\n    \n    election_results &lt;- \n      election_results |&gt;\n      mutate(\n        democratic_count = case_when(\n          is.na(democratic_count) & !is.na(democraticnpl_count) ~ democraticnpl_count,\n          is.na(democratic_count) & !is.na(dfl_count) ~ dfl_count,\n          TRUE ~ democratic_count\n        ),\n        democratic_percentage = case_when(\n          is.na(democratic_percentage) & !is.na(democraticnpl_percentage) ~ democraticnpl_percentage,\n          is.na(democratic_percentage) & !is.na(dfl_percentage) ~ dfl_percentage,\n          TRUE ~ democratic_percentage\n        ),\n        otherparties_count = case_when(\n          is.na(otherparties_count) & !is.na(variouscandidatesotherparties_count) ~ variouscandidatesotherparties_count,\n          TRUE ~ otherparties_count\n        ),\n        otherparties_percentage = case_when(\n          is.na(otherparties_percentage) & !is.na(variouscandidatesotherparties_percentage) ~ variouscandidatesotherparties_percentage,\n          TRUE ~ otherparties_percentage\n        )\n      ) |&gt;\n      select(-any_of(c(\n        \"variouscandidatesotherparties_count\", \"variouscandidatesotherparties_percentage\",\n        \"dfl_count\", \"dfl_percentage\",\n        \"democraticnpl_count\", \"democraticnpl_percentage\"\n      )))\n    \n    if (year == 2020){\n      election_results &lt;- \n        election_results |&gt;\n        select(-c(totalvotescast, totalvotes, registeredvoters, voterturnout, total)) |&gt;\n        mutate(\n          other_count_components = rowSums(\n            pick(matches(\"_count$\") & \n                   !matches(\"republican_count\") & \n                   !matches(\"democratic_count\") &\n                   !matches(\"otherparties_count\")),\n            na.rm = TRUE\n          ),\n          otherparties_count = case_when(\n            is.na(otherparties_count) ~ other_count_components,\n            !is.na(otherparties_count) & other_count_components &gt; 0 ~ otherparties_count + other_count_components,\n            TRUE ~ otherparties_count\n          )\n        ) |&gt;\n        select(-other_count_components)\n    }\n    \n    election_results &lt;- \n      election_results |&gt; \n      mutate(\n        total_votes = rowSums(\n          pick(matches(\"_count$\") & !matches(\"margin_count\")),\n          na.rm = TRUE\n          ),\n        republican_percentage = 100 * republican_count / total_votes,\n        democratic_percentage = 100 * democratic_count / total_votes,\n        otherparties_percentage = 100 * otherparties_count / total_votes\n        ) |&gt;\n      select(\n        county, republican_count, republican_percentage, democratic_count,\n        democratic_percentage, otherparties_count, otherparties_percentage,\n        county_type, state, total_votes\n        )\n        \n    write.csv(election_results, file = dest_fpath, row.names = FALSE)\n    }\n  else{\n    election_results &lt;- read.csv(dest_fpath)\n  }\n  return(election_results)\n  }\n\n\nThese functions were designed to cleanly import the election results of all 50 states with just one line of code.\n\n\nCode\nelection_results_2020 &lt;- load_election_results(us_states, 2020)\nelection_results_2024 &lt;- load_election_results(us_states, 2024)\n\n\n\n\n\n\nI merged the census and election results data into one data.frame object called election_results.\n\n\nCode\nelection_results &lt;- left_join(\n  election_results_2020,\n  election_results_2024,\n  join_by(county == county, state == state, county_type == county_type),\n  suffix = c(\"_2020\", \"_2024\")\n  ) |&gt;\n  right_join(us_counties, join_by(county == NAME, state == STATE_NAME)) |&gt;\n  mutate(\n    democratic_count_change = democratic_count_2024 - democratic_count_2020,\n    democratic_percentage_change = democratic_percentage_2024 - democratic_percentage_2020,\n    republican_count_change = republican_count_2024 - republican_count_2020,\n    republican_percentage_change = republican_percentage_2024 - republican_percentage_2020,\n    otherparties_count_change = otherparties_count_2024 - otherparties_count_2020,\n    otherparties_percentage_change = otherparties_percentage_2024 - otherparties_percentage_2020,\n  )\n\n\nFirst, I identify the county with the most votes cast for Trump in 2024.\n\n\nCode\n# Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\nelection_results |&gt;\n  select(county, state, republican_count_2024) |&gt;\n  filter(county != \"Totals\") |&gt;\n  slice_max(republican_count_2024, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Votes\" = republican_count_2024) |&gt;\n  kable(caption = \"Table 1: County with the most votes cast for Trump in 2024.\")\n\n\nNext, I find the county that cast the highest percentage of votes for Biden in 2020.\n\n\nCode\n# Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\nelection_results |&gt;\n  select(county, state, democratic_percentage_2020) |&gt;\n  filter(county != \"Totals\") |&gt;\n  slice_max(democratic_percentage_2020, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Percent of Votes\" = democratic_percentage_2020) |&gt;\n  kable(caption = \"Table 2: County with the highest percentage of votes for Biden in 2020?\")\n\n\nThen, I determine the county with the largest shift in votes towards Trump in 2024.\n\n\nCode\n# Which county or counties had the largest shift towards Trump (in absolute terms) in 2024?\nelection_results |&gt;\n  select(county, state, republican_count_change) |&gt;\n  filter(county != \"Totals\") |&gt;\n  slice_max(republican_count_change, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Change in Votes\" = republican_count_change\n  ) |&gt;\n  kable(caption = \"Table 3: County with the largest shift in votes towards Trump in 2024?\")\n\n\nHere, I locate the state the smallest shift toward Trump in 2024.\n\n\nCode\n# Which state had the largest shift towards Harris (or smallest shift towards Trump) in 2024? (Note that the total votes for a state can be obtained by summing all counties in that state.)\nelection_results |&gt;\n  select(county, state, republican_count_change) |&gt;\n  filter(county == \"Totals\") |&gt;\n  slice_min(republican_count_change, n=1) |&gt;\n  select(!c(county)) |&gt;\n  rename(\n    \"State\" = state,\n    \"Change in Votes\" = republican_count_change\n  ) |&gt;\n  kable(caption = \"Table 4: State with the smallest shift towards Trump in 2024\")\n\n\nNext, I ascertain the county with the largest total area (i.e. land and water area).\n\n\nCode\n# What is the largest county, by area, in this data set?\nelection_results |&gt;\n  select(county, state, ALAND, AWATER) |&gt;\n  st_drop_geometry() |&gt;\n  mutate(`Total Area` = ALAND + AWATER) |&gt;\n  slice_max(`Total Area`, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Land Area\" = ALAND,\n    \"Water Area\" = AWATER,\n  ) |&gt;\n  kable(caption = \"Table 5: County with the largest area.\")\n\n\nThen, I discern which county boasted the highest voter density in 2020.\n\n\nCode\n# Which county has the highest voter density (voters per unit of area) in 2020?\nelection_results |&gt;\n  select(county, state, ALAND, AWATER, total_votes_2020) |&gt;\n  st_drop_geometry() |&gt;\n  mutate(\n    `Total Area` = ALAND + AWATER,\n    `Voter Density` = total_votes_2020 / `Total Area`\n         ) |&gt;\n  slice_max(`Voter Density`, n=1) |&gt;\n  select(county, state, `Voter Density`) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n  ) |&gt;\n  kable(caption = \"Table 6: County with the highest voter density.\")\n\n\nFinally, I deduce the county that had the largest increase in voter runout in the most recent election.\n\n\nCode\n# Which county had the largest increase in voter turnout in 2024?\nelection_results |&gt; \n  select(county, state, total_votes_2020, total_votes_2024) |&gt;\n  filter(county != \"Totals\") |&gt;\n  mutate(\n    change = total_votes_2024 - total_votes_2020\n      ) |&gt;\n  slice_max(change, n = 1) |&gt;\n  select(-c(total_votes_2020,total_votes_2024)) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Change in Voter Turnout\" = change\n  ) |&gt;\n  kable(caption = \"Table 7: County with the largest increase in voter turnout in 2024.\")"
  },
  {
    "objectID": "mp04.html#importing-data",
    "href": "mp04.html#importing-data",
    "title": "How Counties Changed: 2020 vs. 2024 Elections",
    "section": "Importing Data",
    "text": "Importing Data\n\nUS County Shapes\nTo visualize election results geographically, I imported U.S. county boundary shapefiles at a 1:20,000,000 scale from the US Census Bureau(U.S. Census Bureau, 2023). The code used to import and preprocess these shapefiles is shown below.\n\n\nCode\n#' Download and Process US County Shapefile\n#'\n#' Downloads, extracts, and processes the 2023 US county shapefile from the U.S. Census Bureau if not already present locally.\n#'\n#' This function checks whether the shapefile exists in the specified local directory (`data/mp04/`). If it does not exist,\n#' the function downloads the ZIP file containing the shapefile, extracts its contents, and deletes the ZIP file to save space.\n#' It then reads the shapefile using `read_sf()`, shifts geometries (typically to reposition Alaska and Hawaii), casts geometries\n#' to `MULTIPOLYGON`, and computes centroids for each county polygon.\n#'\n#' @return A `sf` object representing the US counties with geometry and centroid columns.\n#' @importFrom sf read_sf st_cast st_centroid\n#' @importFrom dplyr mutate\n#' @importFrom tigris shift_geometry\n#' @export\ndownload_shp &lt;- function(){\n  directory &lt;- \"data/mp04/\"\n  fname &lt;- \"cb_2023_us_county_20m\"\n  zip_fpath &lt;- paste0(directory, fname, \".zip\")\n  shp_fpath &lt;- paste0(directory, fname, \".shp\")\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n\n  files_matching_pattern &lt;- list.files(directory, pattern = \"cb_2023_us_county_20m\", full.names = TRUE)\n  if (length(files_matching_pattern) == 0){\n    source_root_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_20m.zip\"\n    download.file(\n      url = source_root_url,\n      destfile = zip_fpath,\n      method = \"auto\",\n      quiet = TRUE\n    )\n  }\n  # check if zip exists\n  if (file.exists(zip_fpath)){\n    # if true, unzip the shp file, then delete the zip to save a measly amount of memory.\n    unzip(zip_fpath, exdir=directory)  \n    file.remove(zip_fpath)\n  } \n  # check if shp exists\n  if (file.exists(shp_fpath)){\n    # if true, read in shp file\n    shpfile &lt;- read_sf(shp_fpath) |&gt;\n      shift_geometry(position = \"below\", preserve_area = FALSE) |&gt;\n      mutate(\n        geometry = st_cast(geometry, \"MULTIPOLYGON\"),\n        centroid = st_centroid(geometry)\n      )\n    return(shpfile)\n  }\n}\n\nus_counties &lt;- download_shp()\n\n\n\n\n2020 and 2024 US Presidential Election Results\nTo analyze county-level results in the U.S. presidential elections from 2020 to 2024, I scraped state-level election data from Wikipedia (contributors, 2024k, 2024ab, 2024av, 2024i, 2024q, 2024x, 2024y, 2024am, 2024ar, 2024z, 2024aq, 2024w, 2024f, 2024s, 2024o, 2024r, 2024ay, 2024al, 2024p, 2024aj, 2024a, 2024ah, 2024ap, 2024ax, 2024c, 2024n, 2024ak, 2024ag, 2024m, 2024ao, 2024ai, 2024d, 2024t, 2024j, 2024af, 2024v, 2024an, 2024aw, 2024ac, 2024e, 2024ae, 2024at, 2024as, 2024ad, 2024g, 2024u, 2024aa, 2024h, 2024au, 2024b, 2024l, 2024as) using a custom load_state_election_results function (defined below). However, scraping from Wikipedia was challenging, particularly due to inconsistencies in how political subdivisions are defined and reported across states.\nFor most states, election results are reported by county, with several exceptions. Louisiana, for example, reports results by parish rather than county. In other states, such as the District of Columbia, results are organized by wards. These nonstandard units complicated efforts to make a consistent cross-state comparison.\nThe most difficult case was Connecticut. In 2020, results were reported by county, but by 2024, the state had transitioned to using “planning regions”–a new administrative unit that does not align cleanly with previous county boundaries. To reconcile this, I downloaded 2020 election results at the town level, then mapped them to the appropriate planning regions based on this data set (Connecticut Data Collaborative, 2023). This workaround ensured comparability across years for the state.\n\n\nCode\n#' Load or Download County-Level U.S. Presidential Election Results from Wikipedia\n#'\n#' Retrieves, cleans, and processes county-level (or equivalent) U.S. presidential election results\n#' for a given state and election year (2020 or 2024). If the data is not available locally, it is\n#' scraped from the corresponding Wikipedia page and cached to disk for future use.\n#'\n#' Handles special cases such as Connecticut (town or planning region level),\n#' the District of Columbia (ward level), and Washington (naming convention on Wikipedia).\n#' Also maps Connecticut towns to planning regions for 2024.\n#'\n#' @param state A character string with the full state name (e.g., \"Connecticut\", \"Texas\").\n#' @param year An integer (2020 or 2024). Other years are not supported.\n#'\n#' @return A data frame containing county-level election results for the specified state and year.\n#'         The columns include vote counts and percentages for major parties and others, with standardized\n#'         county identifiers. Also includes a `county_type` and `state` column.\n#'\n#' @importFrom rvest read_html html_elements html_table\n#' @importFrom dplyr mutate rename_with slice inner_join group_by summarize across\n#' @importFrom stringr str_to_lower str_replace_all str_remove_all str_remove str_replace\n#' @importFrom utils download.file write.csv read.csv\n#' @importFrom stats na.omit\n#' @export\nload_state_election_results &lt;- function(state, year){\n  if (!(year %in% c(2020, 2024))) {\n    stop(\"Please enter year = 2020 or year = 2024\")\n  }\n  year &lt;- as.integer(year)\n  state_fname &lt;- paste0(str_to_lower(state),\".csv\")\n  directory &lt;- \"data/mp04/\"\n  subdirectory &lt;- paste0(directory,year,'/')\n  state_fpath &lt;- paste0(subdirectory,state_fname)\n  mapping_flag &lt;- FALSE\n  \n  if (!dir.exists(subdirectory)) {\n    dir.create(subdirectory, recursive = TRUE)\n  }\n  \n  if (!file.exists(state_fpath)){\n    \n    regex_str &lt;- \"^(County(/City)?|Parish|Ward|State\\\\sHouse\\\\sDistrict)(\\\\[[0-9]+\\\\])?$\"\n    \n    # no file found, therefore, i will download straight from wikipedia.\n    url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"#By_county\")\n    if (state == \"Washington\"){\n      # washington has an extra (state) in the url.\n      url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"_(state)#By_county\")\n    }\n    if (state == \"District of Columbia\"){\n      # District of columbia url also has a unique url\n      url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_the_\",str_replace_all(state, \"\\\\s+\", \"_\"),\"#Results_by_ward\")\n    }\n    if (state == \"Connecticut\"){\n      # connecticut changed from \"counties\" to \"planning regions\" in 2022. this code maps towns to the new planning regions\n      if (year == 2020){\n        mapping_flag &lt;- TRUE\n        regex_str = \"^Town$\"\n        url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"#By_town\")\n      }\n      if (year == 2024){\n        regex_str = \"^Council\\\\sof\\\\sGovernment$\"\n        url &lt;- paste0(\"https://en.wikipedia.org/wiki/\",year,\"_United_States_presidential_election_in_\", str_replace_all(state, \"\\\\s+\", \"_\"), \"#By_Council_of_Government\")\n      }\n    }\n    \n    county_counts &lt;- tryCatch({\n      read_html(url) |&gt;\n        html_elements(\".wikitable\") |&gt;\n        html_table() |&gt;\n        Filter(\\(x)\n               any(grepl(regex_str, colnames(x))) &&\n                 any(grepl(\"Margin\", colnames(x))),\n               x = _\n        )\n    }, error = function(e) {\n      print(url)\n      warning(paste(\"Failed to load data for state:\", state))\n      return(NULL)\n    })\n    \n    if (length(county_counts) == 1){ county_counts &lt;- county_counts[[1]] }\n    if (length(county_counts) == 0){ warning(paste(\"No data for\", state)); return(NULL) }\n    if (is.list(county_counts) && length(county_counts) &gt; 1 && inherits(county_counts[[1]], \"data.frame\")) {\n      county_counts &lt;- county_counts[[1]]\n    }\n   \n    # first row hints at units--I want that info in the column names instead. \n    first_row &lt;- county_counts |&gt; head(n=1)\n    county_counts &lt;- county_counts |&gt;\n      rename_with(\n        ~ paste0(., \"_Count\"),\n        .cols = which(grepl(\"^#|Votes$\", first_row))\n      ) |&gt;\n      rename_with(\n        ~ paste0(., \"_Percentage\"), \n        .cols = which(first_row == \"%\")\n      )\n    \n    \n    # delete useless first row of values.\n    county_counts &lt;- county_counts |&gt;\n      slice(-1)\n    \n    #cleaning up column names for easier referencing\n    colnames(county_counts) &lt;- county_counts |&gt; \n      colnames() |&gt; \n      str_remove_all(\"Donald Trump|Kamala Harris|Various candidates|City|Joe Biden|Jo Jorgensen|Howie Hawkins\") |&gt; \n      str_remove_all(\"[^a-zA-Z_]\")\n  \n    \n    first_col &lt;- colnames(county_counts)[1]\n  \n    # saving cleaned up county type for later\n    county_counts &lt;- \n      county_counts |&gt;\n      mutate(county_type = first_col)\n    # converting data types\n    county_counts &lt;- county_counts |&gt;\n      mutate(across(!any_of(c(\"County\", \"county_type\", \"Town\", \"Parish\",\"Ward\",\"StateHouseDistrict\", \"CouncilofGovernment\")), ~ as.numeric(\n        str_remove_all(\n          str_replace_all(as.character(.x), \"\\u2212\", \"-\"),\n          \"[,%\\u00A0]\"\n        )\n      )))\n    \n    if (mapping_flag){\n      mapping_fdest &lt;- paste0(directory,\"ct-town-to-planning-region.csv\")\n      if (!file.exists(mapping_fdest)){\n        download.file(\n          url = \"https://raw.githubusercontent.com/CT-Data-Collaborative/ct-town-to-planning-region/refs/heads/main/ct-town-to-planning-region.csv\",\n          destfile = mapping_fdest,\n          method = \"auto\"\n        )\n      }\n      ct_mapping &lt;- as.data.frame(read.csv(mapping_fdest))\n    \n      county_counts &lt;-\n        county_counts |&gt; \n          inner_join(\n            ct_mapping |&gt;\n              select(town_name, ce_name_2022),\n            join_by(Town == town_name)) |&gt; \n          group_by(ce_name_2022) |&gt;\n          summarize(\n            Democratic_Count = sum(Democratic_Count),\n            Republican_Count = sum(Republican_Count),\n            Libertarian_Count = sum(Libertarian_Count),\n            Green_Count = sum(Green_Count),\n            Otherparties_Count = sum(Otherparties_Count),\n            ) |&gt;\n          mutate(\n            total_votes = rowSums(across(\n              .cols = c(Democratic_Count, Republican_Count, Libertarian_Count, Green_Count, Otherparties_Count),\n              .names = NULL\n            ), na.rm = TRUE),\n            Republican_Percentage = 100 * Republican_Count / total_votes,\n            Democratic_Percentage = 100 * Democratic_Count / total_votes,\n            Libertarian_Percentage = 100 * Libertarian_Count / total_votes,\n            Green_Percentage = 100 * Green_Count / total_votes,\n            Otherparties_Percentage = 100 * Otherparties_Count / total_votes,\n            county_type = \"Planning Region\",\n            ce_name_2022 = str_remove(ce_name_2022, \" Planning Region\")\n          ) |&gt;\n          rename(county = ce_name_2022)\n    }\n    \n    # finish cleaning up column names \n    colnames(county_counts) &lt;- \n      county_counts |&gt; \n      colnames() |&gt; \n      str_replace_all(\"Parish|Ward|StateHouseDistrict|CouncilofGovernment\", \"County\") |&gt;\n      str_to_lower()\n\n    if (state == \"Connecticut\" && year == 2024){\n      county_counts &lt;-\n        county_counts |&gt;\n        mutate(\n          county_type = \"Planning Region\",\n          county = str_remove(county, \" Planning Region\")\n          )\n    }\n    \n    # adding state name for easier merging later.\n    county_counts &lt;- county_counts |&gt;\n      mutate(state = state)\n    \n    write.csv(x = county_counts, file = state_fpath, row.names = FALSE)\n  } else{\n    county_counts &lt;- read.csv(state_fpath)\n  }\n  return (county_counts)\n}\n\n\nAfter assigning us_states a list of US states, us_states &lt;- unique(us_counties |&gt; select(STATE_NAME) |&gt; st_drop_geometry())[[1]], load_state_election_results was called from within the following function:\n\n\nCode\n#' Load or Compile U.S. Presidential Election Results for Multiple States\n#'\n#' Aggregates county-level (or equivalent) presidential election results for a list of U.S. states in a given election year.\n#' If a compiled CSV file already exists for the specified year, it is loaded. Otherwise, data is fetched (via\n#' \\code{\\link{load_state_election_results}}), cleaned, and saved for future use.\n#'\n#' Handles normalization of alternate party labels (e.g., Democratic–NPL, DFL), estimates missing values where necessary,\n#' and calculates total votes and percentages for Republican, Democratic, and other candidates.\n#'\n#' @param states A character vector of state names (e.g., \\code{c(\"Texas\", \"Ohio\", \"Connecticut\")}).\n#' @param year An integer representing the election year (only 2020 or 2024 are supported).\n#'\n#' @return A data frame with cleaned, harmonized election results by county, including vote counts and percentages for\n#' Republican, Democratic, and other parties, along with state and county identifiers.\n#'\n#' @importFrom dplyr bind_rows mutate case_when select any_of rowSums matches\n#' @importFrom tidyselect everything\n#' @importFrom stringr str_to_lower\n#' @importFrom utils read.csv write.csv\n#' @seealso \\code{\\link{load_state_election_results}}\n#' @export\nload_election_results &lt;- function(states, year){\n  directory &lt;- paste0(\"data/mp04/\",year,\"/\")\n  dest_fpath &lt;- paste0(directory, year,\"_election_results.csv\")\n  \n  if (!dir.exists(directory)) {\n    dir.create(directory, recursive = TRUE)\n  }\n  \n  if (!file.exists(dest_fpath)){\n    # download data \n    election_results &lt;- data.frame()\n    for (state in states){\n      message(paste(\"Fetching State:\", state))\n      state_results &lt;- load_state_election_results(state,year)\n      if (!is.null(state_results)) {\n        election_results &lt;- bind_rows(election_results, state_results)\n      }\n      else{\n          warning(paste(\"No data for state:\", state))\n        }\n    }\n    \n    # clean up full election results...\n    election_results &lt;- \n      election_results |&gt;\n      mutate(\n        democratic_count = case_when(\n          is.na(democratic_count) & !is.na(democraticnpl_count) ~ democraticnpl_count,\n          is.na(democratic_count) & !is.na(dfl_count) ~ dfl_count,\n          TRUE ~ democratic_count\n        ),\n        democratic_percentage = case_when(\n          is.na(democratic_percentage) & !is.na(democraticnpl_percentage) ~ democraticnpl_percentage,\n          is.na(democratic_percentage) & !is.na(dfl_percentage) ~ dfl_percentage,\n          TRUE ~ democratic_percentage\n        ),\n        otherparties_count = case_when(\n          is.na(otherparties_count) & !is.na(variouscandidatesotherparties_count) ~ variouscandidatesotherparties_count,\n          TRUE ~ otherparties_count\n        ),\n        otherparties_percentage = case_when(\n          is.na(otherparties_percentage) & !is.na(variouscandidatesotherparties_percentage) ~ variouscandidatesotherparties_percentage,\n          TRUE ~ otherparties_percentage\n        )\n      ) |&gt;\n      select(-any_of(c(\n        \"variouscandidatesotherparties_count\", \"variouscandidatesotherparties_percentage\",\n        \"dfl_count\", \"dfl_percentage\",\n        \"democraticnpl_count\", \"democraticnpl_percentage\"\n      )))\n    \n    if (year == 2020){\n      # consolidate misc parties vote counts into otherparties\n      election_results &lt;- \n        election_results |&gt;\n        select(-c(totalvotescast, totalvotes, registeredvoters, voterturnout, total)) |&gt;\n        mutate(\n          other_count_components = rowSums(\n            pick(matches(\"_count$\") & \n                   !matches(\"republican_count\") & \n                   !matches(\"democratic_count\") &\n                   !matches(\"otherparties_count\")),\n            na.rm = TRUE\n          ),\n          otherparties_count = case_when(\n            is.na(otherparties_count) ~ other_count_components,\n            !is.na(otherparties_count) & other_count_components &gt; 0 ~ otherparties_count + other_count_components,\n            TRUE ~ otherparties_count\n          )\n        ) |&gt;\n        select(-other_count_components)\n    }\n    # recalculate percentages just in case... and drop all the extra columns. \n    election_results &lt;- \n      election_results |&gt; \n      mutate(\n        total_votes = rowSums(\n          pick(matches(\"_count$\") & !matches(\"margin_count\")),\n          na.rm = TRUE\n          ),\n        republican_percentage = 100 * republican_count / total_votes,\n        democratic_percentage = 100 * democratic_count / total_votes,\n        otherparties_percentage = 100 * otherparties_count / total_votes\n        ) |&gt;\n      select(\n        county, republican_count, republican_percentage, democratic_count,\n        democratic_percentage, otherparties_count, otherparties_percentage,\n        county_type, state, total_votes\n        )\n        \n    write.csv(election_results, file = dest_fpath, row.names = FALSE)\n    }\n  else{\n    election_results &lt;- read.csv(dest_fpath)\n  }\n  return(election_results)\n  }\n\n\nIn the function above, the quirks of scraping data from Wikipedia reappeared–particularly with the 2020 election tables. Many of these tables included an overwhelming number of columns, often listing minor party and independent candidates individually. To streamline the dataset, I consolidated all non-Democratic and non-Republican vote counts into a single “Other” category. This not only simplified the data structure but also made it easier to visualize and compare across states and years. Combined, these functions were designed to cleanly import the election results of all 50 states with just one line of code.\n\n\nCode\nelection_results_2020 &lt;- load_election_results(us_states, 2020)\nelection_results_2024 &lt;- load_election_results(us_states, 2024)"
  },
  {
    "objectID": "mp04.html#merging-and-initial-analysis",
    "href": "mp04.html#merging-and-initial-analysis",
    "title": "US Political Shifts",
    "section": "",
    "text": "I merged the census and election results data into one data.frame object called election_results.\n\n\nCode\nelection_results &lt;- left_join(\n  election_results_2020,\n  election_results_2024,\n  join_by(county == county, state == state, county_type == county_type),\n  suffix = c(\"_2020\", \"_2024\")\n  ) |&gt;\n  right_join(us_counties, join_by(county == NAME, state == STATE_NAME)) |&gt;\n  mutate(\n    democratic_count_change = democratic_count_2024 - democratic_count_2020,\n    democratic_percentage_change = democratic_percentage_2024 - democratic_percentage_2020,\n    republican_count_change = republican_count_2024 - republican_count_2020,\n    republican_percentage_change = republican_percentage_2024 - republican_percentage_2020,\n    otherparties_count_change = otherparties_count_2024 - otherparties_count_2020,\n    otherparties_percentage_change = otherparties_percentage_2024 - otherparties_percentage_2020,\n    \n  )"
  },
  {
    "objectID": "mp04.html#initial-analysis",
    "href": "mp04.html#initial-analysis",
    "title": "How Counties Changed: 2020 vs. 2024 Elections",
    "section": "Initial Analysis",
    "text": "Initial Analysis\nAs shown below, I merged the census and election results data into one data.frame object called election_results.\n\n\nCode\nelection_results &lt;- left_join(\n  election_results_2020,\n  election_results_2024,\n  join_by(county == county, state == state, county_type == county_type),\n  suffix = c(\"_2020\", \"_2024\")\n  ) |&gt;\n  right_join(us_counties, join_by(county == NAME, state == STATE_NAME)) |&gt;\n  mutate(\n    democratic_count_change = democratic_count_2024 - democratic_count_2020,\n    democratic_percentage_change = democratic_percentage_2024 - democratic_percentage_2020,\n    republican_count_change = republican_count_2024 - republican_count_2020,\n    republican_percentage_change = republican_percentage_2024 - republican_percentage_2020,\n    otherparties_count_change = otherparties_count_2024 - otherparties_count_2020,\n    otherparties_percentage_change = otherparties_percentage_2024 - otherparties_percentage_2020,\n  )\n\n\nFirst, I identify the county with the most votes cast for Trump in 2024.\n\n\nCode\n# Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\nelection_results |&gt;\n  select(county, state, republican_count_2024) |&gt;\n  filter(county != \"Totals\") |&gt;\n  slice_max(republican_count_2024, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Votes\" = republican_count_2024) |&gt;\n  kable(caption = \"Table 1: County with the most votes cast for Trump in 2024.\")\n\n\nNext, I find the county that cast the highest percentage of votes for Biden in 2020.\n\n\nCode\n# Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\nelection_results |&gt;\n  select(county, state, democratic_percentage_2020) |&gt;\n  filter(county != \"Totals\") |&gt;\n  slice_max(democratic_percentage_2020, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Percent of Votes\" = democratic_percentage_2020) |&gt;\n  kable(caption = \"Table 2: County with the highest percentage of votes for Biden in 2020?\")\n\n\nThen, I determine the county with the largest shift in votes towards Trump in 2024.\n\n\nCode\n# Which county or counties had the largest shift towards Trump (in absolute terms) in 2024?\nelection_results |&gt;\n  select(county, state, republican_count_change) |&gt;\n  filter(county != \"Totals\") |&gt;\n  slice_max(republican_count_change, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Change in Votes\" = republican_count_change\n  ) |&gt;\n  kable(caption = \"Table 3: County with the largest shift in votes towards Trump in 2024?\")\n\n\nHere, I locate the state the smallest shift toward Trump in 2024.\n\n\nCode\n# Which state had the largest shift towards Harris (or smallest shift towards Trump) in 2024? (Note that the total votes for a state can be obtained by summing all counties in that state.)\nelection_results |&gt;\n  select(county, state, republican_count_change) |&gt;\n  filter(county == \"Totals\") |&gt;\n  slice_min(republican_count_change, n=1) |&gt;\n  select(!c(county)) |&gt;\n  rename(\n    \"State\" = state,\n    \"Change in Votes\" = republican_count_change\n  ) |&gt;\n  kable(caption = \"Table 4: State with the smallest shift towards Trump in 2024\")\n\n\nNext, I ascertain the county with the largest total area (i.e. land and water area).\n\n\nCode\n# What is the largest county, by area, in this data set?\nelection_results |&gt;\n  select(county, state, ALAND, AWATER) |&gt;\n  st_drop_geometry() |&gt;\n  mutate(`Total Area` = ALAND + AWATER) |&gt;\n  slice_max(`Total Area`, n=1) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Land Area\" = ALAND,\n    \"Water Area\" = AWATER,\n  ) |&gt;\n  kable(caption = \"Table 5: County with the largest area.\")\n\n\nThen, I discern which county boasted the highest voter density in 2020.\n\n\nCode\n# Which county has the highest voter density (voters per unit of area) in 2020?\nelection_results |&gt;\n  select(county, state, ALAND, AWATER, total_votes_2020) |&gt;\n  st_drop_geometry() |&gt;\n  mutate(\n    `Total Area` = ALAND + AWATER,\n    `Voter Density` = total_votes_2020 / `Total Area`\n         ) |&gt;\n  slice_max(`Voter Density`, n=1) |&gt;\n  select(county, state, `Voter Density`) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n  ) |&gt;\n  kable(caption = \"Table 6: County with the highest voter density.\")\n\n\nFinally, I deduce the county that had the largest increase in voter runout in the most recent election.\n\n\nCode\n# Which county had the largest increase in voter turnout in 2024?\nelection_results |&gt; \n  select(county, state, total_votes_2020, total_votes_2024) |&gt;\n  filter(county != \"Totals\") |&gt;\n  mutate(\n    change = total_votes_2024 - total_votes_2020\n      ) |&gt;\n  slice_max(change, n = 1) |&gt;\n  select(-c(total_votes_2020,total_votes_2024)) |&gt;\n  rename(\n    \"County\" = county,\n    \"State\" = state,\n    \"Change in Voter Turnout\" = change\n  ) |&gt;\n  kable(caption = \"Table 7: County with the largest increase in voter turnout in 2024.\")"
  },
  {
    "objectID": "mp04.html#new-york-times-figure",
    "href": "mp04.html#new-york-times-figure",
    "title": "US Political Shifts",
    "section": "New York Times Figure",
    "text": "New York Times Figure\nI reproduced the image at (Weiland et al., 2024)"
  },
  {
    "objectID": "mp04.html#new-york-times-red-shift-figure-reproduction",
    "href": "mp04.html#new-york-times-red-shift-figure-reproduction",
    "title": "How Counties Changed: 2020 vs. 2024 Elections",
    "section": "New York Times “Red Shift” Figure Reproduction",
    "text": "New York Times “Red Shift” Figure Reproduction\nI reproduced the image at (Weiland et al., 2024) with the following code. I obtained geometry for the state boundaries by grouping the census data by STATE_NAME.\n\n\nCode\nstate_boundaries &lt;- us_counties |&gt;\n  group_by(STATE_NAME) |&gt;\n  summarise(geometry = st_union(geometry), .groups = \"drop\") |&gt;\n  mutate(geometry = st_cast(geometry, \"MULTIPOLYGON\"))\n\n\nI created coordinates for the arrow bodies corresponding to the magnitude and direction of the shift.\n\n\nCode\n# calculating centroids + arrow body coords\nplot_data &lt;- st_as_sf(election_results) |&gt;\n  mutate(\n    geometry = st_cast(geometry, \"MULTIPOLYGON\",\n    centroid = st_centroid(geometry),\n    coords = st_coordinates(centroid),\n    x_start = coords[, 1],\n    y_start = coords[, 2],\n    x_end = x_start + (republican_percentage_change * SCALE_FACTOR) * cos(ARROW_ANGLE),\n    y_end = y_start + (abs(republican_percentage_change) * SCALE_FACTOR) * sin(ARROW_ANGLE),\n    shift_direction = ifelse(republican_percentage_change &gt; 0, \"Republican\", \"Democrat\"),\n  )\n\n\nI wanted to make the plot interactive with plotly, but plotly::ggplotly() doesn’t natively support the arrow argument used in geom_segment()–which draws proper arrowheads in static ggplot2 plots. As a result, the arrowheads simply didn’t render in the interactive version.\nTo work around this, I manually constructed arrowheads by drawing two short line segments that converge at the endpoint of each arrow shaft. This required calculating offset angles from the direction of the main arrow to position the “wings” of the arrowhead.\n\n\nCode\n# Compute work around arrowhead segments\narrowheads &lt;- plot_data |&gt; \n  filter(!is.na(republican_percentage_change)) |&gt;\n  rowwise() |&gt; \n  mutate(\n    theta = ifelse(republican_percentage_change &gt;= 0, ARROW_ANGLE, -ARROW_ANGLE),\n    x_tip = x_end,\n    y_tip = y_end,\n    arrowhead_length = SCALE_FACTOR * 0.5 * republican_percentage_change,\n    x1 = x_tip - arrowhead_length * cos(theta + ARROWHEAD_ANGLE),\n    y1 = y_tip - arrowhead_length * sin(theta + ARROWHEAD_ANGLE),\n    x2 = x_tip - arrowhead_length * cos(theta - ARROWHEAD_ANGLE),\n    y2 = y_tip - arrowhead_length * sin(theta - ARROWHEAD_ANGLE)\n  )\n\n\nFinally, I generated the plot using the code below.\n\n\nCode\nplot &lt;- ggplot() +\n  geom_sf(\n    data = plot_data |&gt; distinct(geometry, .keep_all = TRUE),\n    fill = \"grey90\",\n    color = \"white\",\n    linewidth = LINEWIDTH - 0.1\n  ) +\n  geom_sf(\n    data = state_boundaries,\n    fill = NA,\n    color = \"gray40\",\n    linewidth = LINEWIDTH - 0.1\n  ) +\n  geom_segment(\n    data = plot_data |&gt; filter(!is.na(republican_percentage_change)),\n    aes(\n      x = x_start, y = y_start, xend = x_end, yend = y_end,\n      color = shift_direction,\n      text = paste0(county, \" \", county_type, \", \", state, \"\\nShift: \", round(abs(republican_percentage_change), digits = 2), \"% more \", shift_direction, \" in 2024\")\n    ),\n    arrow = arrow(length = unit(0.1, \"inches\")),\n    linewidth = LINEWIDTH\n  ) +\n  geom_segment(\n    data = arrowheads,\n    aes(x = x_tip, y = y_tip, xend = x1, yend = y1, color = shift_direction),\n    linewidth = LINEWIDTH\n  ) +\n  geom_segment(\n    data = arrowheads,\n    aes(x = x_tip, y = y_tip, xend = x2, yend = y2, color = shift_direction),\n    linewidth = LINEWIDTH\n  ) +\n  scale_color_manual(values = c(\"Democrat\" = DEM_BLUE, \"Republican\" = REP_RED), na.translate = FALSE) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    plot.margin = margin(0, 0, 0, 0),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"bottom\",\n  ) +\n  labs(color = \"Shift\")\n\n# Convert to plotly\nggplotly(plot, tooltip = \"text\") |&gt;\n  layout(\n  margin = list(l = 0, r = 0, t = 0, b = 1), \n  showlegend = TRUE,\n  legend = list(\n    orientation = \"h\",\n    x = 0.5,\n    xanchor = \"center\",\n    y = 0.2,\n    yanchor = \"top\"\n  )\n)"
  }
]